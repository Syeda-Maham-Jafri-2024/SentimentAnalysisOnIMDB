{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7618506,"sourceType":"datasetVersion","datasetId":4437305}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install afinn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-25T06:41:45.751414Z","iopub.execute_input":"2024-02-25T06:41:45.751836Z","iopub.status.idle":"2024-02-25T06:42:01.102970Z","shell.execute_reply.started":"2024-02-25T06:41:45.751802Z","shell.execute_reply":"2024-02-25T06:42:01.101716Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting afinn\n  Downloading afinn-0.1.tar.gz (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: afinn\n  Building wheel for afinn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=da3ae55b7842fcef490683633d0b4584d840f3fe926cc83645a969170aec1fb7\n  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\nSuccessfully built afinn\nInstalling collected packages: afinn\nSuccessfully installed afinn-0.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------ Afinn with stop word removal---------------------------------","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Remove stopwords\ndef remove_stopwords(review):\n    stop_words = set(stopwords.words('english'))\n    words = review.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = remove_stopwords(review)\n    sentiment_score = afinn.score(text_without_stopwords)\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# End timer and calculate time taken\nend_time = time.time()\n\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\nprint(\"Missing values in Predicted_Sentiment column:\", test_data['Predicted_Sentiment'].isnull().sum())\nprint(\"Missing values in sentiment column:\", test_data['sentiment'].isnull().sum())\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:07:27.741234Z","iopub.execute_input":"2024-02-25T08:07:27.741741Z","iopub.status.idle":"2024-02-25T08:09:01.485262Z","shell.execute_reply.started":"2024-02-25T08:07:27.741706Z","shell.execute_reply":"2024-02-25T08:09:01.483954Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nMissing values in Predicted_Sentiment column: 0\nMissing values in sentiment column: 0\n              precision    recall  f1-score   support\n\n           0       0.80      0.53      0.64      9935\n           1       0.65      0.87      0.75     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.73      0.70      0.69     20000\nweighted avg       0.73      0.70      0.69     20000\n\nTime taken: 93.36256527900696 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:09:22.239418Z","iopub.execute_input":"2024-02-25T08:09:22.239886Z","iopub.status.idle":"2024-02-25T08:09:22.252505Z","shell.execute_reply.started":"2024-02-25T08:09:22.239853Z","shell.execute_reply":"2024-02-25T08:09:22.250999Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Accuracy: 0.70195\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------ Afinn with stop word removal + tokenization + pos ---------------------------------\n","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nimport time\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download NLTK resources (if not already downloaded)\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Remove stopwords and perform POS tagging\ndef preprocess(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)\n    # Remove stopwords\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    # POS tagging\n    pos_tags = nltk.pos_tag(filtered_words)\n    return pos_tags\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    pos_tags = preprocess(review)\n    sentiment_score = 0\n    for word, pos in pos_tags:\n        if pos.startswith('JJ'):  # Consider only adjectives for sentiment scoring\n            sentiment_score += afinn.score(word)\n    if sentiment_score >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\n# Read test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# End timer and calculate time taken\nend_time = time.time()\n\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:10:06.497489Z","iopub.execute_input":"2024-02-25T08:10:06.497939Z","iopub.status.idle":"2024-02-25T08:14:50.515348Z","shell.execute_reply.started":"2024-02-25T08:10:06.497907Z","shell.execute_reply":"2024-02-25T08:14:50.513855Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nAccuracy: 0.70065\n              precision    recall  f1-score   support\n\n           0       0.83      0.50      0.63      9935\n           1       0.65      0.90      0.75     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.74      0.70      0.69     20000\nweighted avg       0.74      0.70      0.69     20000\n\nTime taken: 283.604065656662 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------ Afinn with stop word removal + tokeizatiom ---------------------------------","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Remove stopwords and tokenize\ndef remove_stopwords(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the input review\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = remove_stopwords(review)\n    sentiment_score = afinn.score(text_without_stopwords)\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# End timer and calculate time taken\nend_time = time.time()\n\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Check for missing values\nprint(\"Missing values in Predicted_Sentiment column:\", test_data['Predicted_Sentiment'].isnull().sum())\nprint(\"Missing values in sentiment column:\", test_data['sentiment'].isnull().sum())\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:15:28.571355Z","iopub.execute_input":"2024-02-25T08:15:28.571817Z","iopub.status.idle":"2024-02-25T08:17:40.073835Z","shell.execute_reply.started":"2024-02-25T08:15:28.571780Z","shell.execute_reply":"2024-02-25T08:17:40.072177Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nMissing values in Predicted_Sentiment column: 0\nMissing values in sentiment column: 0\nAccuracy: 0.70125\n              precision    recall  f1-score   support\n\n           0       0.80      0.53      0.64      9935\n           1       0.65      0.87      0.75     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.73      0.70      0.69     20000\nweighted avg       0.73      0.70      0.69     20000\n\nTime taken: 131.09869074821472 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------ Afinn with stop word removal + tokeizatiom + porter stemmer ---------------------------------","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nimport re\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Initialize Porter Stemmer\nporter = PorterStemmer()\n\n# Remove stopwords, punctuation, special characters, and numbers, apply Porter stemming, and tokenize\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(text)  # Tokenize the input text\n    filtered_words = [porter.stem(word.lower()) for word in words if word.lower() not in stop_words and word.isalnum()]  # Remove stopwords, non-alphanumeric characters, apply Porter stemming, and convert to lowercase\n    return filtered_words\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    processed_text = preprocess_text(review)\n    text_without_stopwords = ' '.join(processed_text)\n    sentiment_score = afinn.score(text_without_stopwords)\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# End timer and calculate time taken\nend_time = time.time()\n\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Check for missing values\nprint(\"Missing values in Predicted_Sentiment column:\", test_data['Predicted_Sentiment'].isnull().sum())\nprint(\"Missing values in sentiment column:\", test_data['sentiment'].isnull().sum())\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:18:30.635638Z","iopub.execute_input":"2024-02-25T08:18:30.636436Z","iopub.status.idle":"2024-02-25T08:21:38.329596Z","shell.execute_reply.started":"2024-02-25T08:18:30.636393Z","shell.execute_reply":"2024-02-25T08:21:38.328213Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nMissing values in Predicted_Sentiment column: 0\nMissing values in sentiment column: 0\nAccuracy: 0.6571\n              precision    recall  f1-score   support\n\n           0       0.75      0.46      0.57      9935\n           1       0.62      0.85      0.71     10065\n\n    accuracy                           0.66     20000\n   macro avg       0.68      0.66      0.64     20000\nweighted avg       0.68      0.66      0.64     20000\n\nTime taken: 187.31321787834167 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------ Afinn with stop word removal + tokeizatiom + Lancaster  stemmer ---------------------------------","metadata":{}},{"cell_type":"code","source":"from nltk.stem import LancasterStemmer\nfrom afinn import Afinn\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nimport re\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Initialize Porter Stemmer\nporter =  LancasterStemmer()\n\n# Remove stopwords, punctuation, special characters, and numbers, apply Porter stemming, and tokenize\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(text)  # Tokenize the input text\n    filtered_words = [porter.stem(word.lower()) for word in words if word.lower() not in stop_words and word.isalnum()]  # Remove stopwords, non-alphanumeric characters, apply Porter stemming, and convert to lowercase\n    return filtered_words\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    processed_text = preprocess_text(review)\n    text_without_stopwords = ' '.join(processed_text)\n    sentiment_score = afinn.score(text_without_stopwords)\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# End timer and calculate time taken\nend_time = time.time()\n\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Check for missing values\nprint(\"Missing values in Predicted_Sentiment column:\", test_data['Predicted_Sentiment'].isnull().sum())\nprint(\"Missing values in sentiment column:\", test_data['sentiment'].isnull().sum())\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:23:40.044395Z","iopub.execute_input":"2024-02-25T08:23:40.044922Z","iopub.status.idle":"2024-02-25T08:26:24.377298Z","shell.execute_reply.started":"2024-02-25T08:23:40.044886Z","shell.execute_reply":"2024-02-25T08:26:24.375669Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nMissing values in Predicted_Sentiment column: 0\nMissing values in sentiment column: 0\nAccuracy: 0.68435\n              precision    recall  f1-score   support\n\n           0       0.71      0.62      0.66      9935\n           1       0.67      0.75      0.70     10065\n\n    accuracy                           0.68     20000\n   macro avg       0.69      0.68      0.68     20000\nweighted avg       0.69      0.68      0.68     20000\n\nTime taken: 163.92923212051392 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------ Afinn with stop word removal + without tokeizatiom + Lancaster  stemmer ---------------------------------","metadata":{}},{"cell_type":"code","source":"from nltk.stem import LancasterStemmer\nfrom afinn import Afinn\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Initialize Lancaster Stemmer\nlancaster = LancasterStemmer()\n\n# Remove stopwords, punctuation, special characters, and numbers, apply Lancaster stemming\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    words = text.split()  # Split the input text\n    filtered_words = [lancaster.stem(word.lower()) for word in words if word.lower() not in stop_words and word.isalnum()]  # Remove stopwords, non-alphanumeric characters, apply Lancaster stemming, and convert to lowercase\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    processed_text = preprocess_text(review)\n    sentiment_score = afinn.score(processed_text)\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# End timer and calculate time taken\nend_time = time.time()\n\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Check for missing values\nprint(\"Missing values in Predicted_Sentiment column:\", test_data['Predicted_Sentiment'].isnull().sum())\nprint(\"Missing values in sentiment column:\", test_data['sentiment'].isnull().sum())\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:26:47.602318Z","iopub.execute_input":"2024-02-25T08:26:47.602739Z","iopub.status.idle":"2024-02-25T08:28:24.718738Z","shell.execute_reply.started":"2024-02-25T08:26:47.602709Z","shell.execute_reply":"2024-02-25T08:28:24.717397Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nMissing values in Predicted_Sentiment column: 0\nMissing values in sentiment column: 0\nAccuracy: 0.66795\n              precision    recall  f1-score   support\n\n           0       0.69      0.59      0.64      9935\n           1       0.65      0.74      0.69     10065\n\n    accuracy                           0.67     20000\n   macro avg       0.67      0.67      0.67     20000\nweighted avg       0.67      0.67      0.67     20000\n\nTime taken: 96.71896886825562 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------ Afinn with stop word removal + tokeizatiom + snowball  stemmer ---------------------------------","metadata":{}},{"cell_type":"code","source":"from nltk.stem import LancasterStemmer\nfrom afinn import Afinn\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nimport re\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Initialize Porter Stemmer\nporter = SnowballStemmer('english')\n\n# Remove stopwords, punctuation, special characters, and numbers, apply Porter stemming, and tokenize\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(text)  # Tokenize the input text\n    filtered_words = [porter.stem(word.lower()) for word in words if word.lower() not in stop_words and word.isalnum()]  # Remove stopwords, non-alphanumeric characters, apply Porter stemming, and convert to lowercase\n    return filtered_words\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    processed_text = preprocess_text(review)\n    text_without_stopwords = ' '.join(processed_text)\n    sentiment_score = afinn.score(text_without_stopwords)\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n# Start timer\nstart_time = time.time()\n\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# End timer and calculate time taken\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Check for missing values\nprint(\"Missing values in Predicted_Sentiment column:\", test_data['Predicted_Sentiment'].isnull().sum())\nprint(\"Missing values in sentiment column:\", test_data['sentiment'].isnull().sum())\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:30:04.357294Z","iopub.execute_input":"2024-02-25T08:30:04.357878Z","iopub.status.idle":"2024-02-25T08:32:45.704784Z","shell.execute_reply.started":"2024-02-25T08:30:04.357841Z","shell.execute_reply":"2024-02-25T08:32:45.703202Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nMissing values in Predicted_Sentiment column: 0\nMissing values in sentiment column: 0\nAccuracy: 0.66065\n              precision    recall  f1-score   support\n\n           0       0.76      0.46      0.58      9935\n           1       0.62      0.86      0.72     10065\n\n    accuracy                           0.66     20000\n   macro avg       0.69      0.66      0.65     20000\nweighted avg       0.69      0.66      0.65     20000\n\nTime taken: 160.94838523864746 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"with spacy","metadata":{}},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download en_core_web_sm\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T06:40:46.887268Z","iopub.execute_input":"2024-02-25T06:40:46.887707Z","iopub.status.idle":"2024-02-25T06:41:21.595218Z","shell.execute_reply.started":"2024-02-25T06:40:46.887676Z","shell.execute_reply":"2024-02-25T06:41:21.593207Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\nCollecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"from afinn import Afinn\nimport pandas as pd\nimport spacy\nfrom sklearn.metrics import accuracy_score\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport time\n\n# Download NLTK resources (if not already downloaded)\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Download spaCy model for English language\nspacy.cli.download(\"en_core_web_sm\")\n\n# Load the spaCy English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Instantiate the AFINN object\nafinn = Afinn()\n\n# Function to preprocess text\ndef preprocess(review):\n    # Tokenize the review\n    words = word_tokenize(review)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n    \n    # Lemmatize using spaCy\n    lemmatized_words = [token.lemma_ for token in nlp(\" \".join(filtered_words))]\n    \n    return lemmatized_words\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    words = preprocess(review)\n    sentiment_score = sum(afinn.score(word) for word in words)\n    if sentiment_score >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\n# Read test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\n\n# End timer and calculate time taken\nend_time = time.time()\nprint(\"Time taken:\", end_time - start_time, \"seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T06:42:09.697345Z","iopub.execute_input":"2024-02-25T06:42:09.698009Z","iopub.status.idle":"2024-02-25T06:54:45.530621Z","shell.execute_reply.started":"2024-02-25T06:42:09.697957Z","shell.execute_reply":"2024-02-25T06:54:45.529004Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nCollecting en-core-web-sm==3.7.1\n  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\nRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\nAccuracy: 0.70485\nTime taken: 738.4201760292053 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T06:56:04.539891Z","iopub.execute_input":"2024-02-25T06:56:04.540869Z","iopub.status.idle":"2024-02-25T06:56:04.597328Z","shell.execute_reply.started":"2024-02-25T06:56:04.540831Z","shell.execute_reply":"2024-02-25T06:56:04.595897Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.80      0.54      0.64      9935\n           1       0.66      0.87      0.75     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.73      0.70      0.70     20000\nweighted avg       0.73      0.70      0.70     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------------- Testblob with stop word removal --------------------------------","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport time\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\n\n# Remove stopwords\ndef remove_stopwords(review):\n    stop_words = set(stopwords.words('english'))\n    words = review.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = remove_stopwords(review)\n    blob = TextBlob(text_without_stopwords)\n    sentiment_score = blob.sentiment.polarity\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# End timer and calculate time taken\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\n\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:36:36.261300Z","iopub.execute_input":"2024-02-25T08:36:36.262739Z","iopub.status.idle":"2024-02-25T08:37:07.975859Z","shell.execute_reply.started":"2024-02-25T08:36:36.262681Z","shell.execute_reply":"2024-02-25T08:37:07.974690Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nAccuracy: 0.70535\n              precision    recall  f1-score   support\n\n           0       0.88      0.47      0.61      9935\n           1       0.64      0.94      0.76     10065\n\n    accuracy                           0.71     20000\n   macro avg       0.76      0.70      0.69     20000\nweighted avg       0.76      0.71      0.69     20000\n\nTime taken: 31.312926054000854 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------------- Testblob with stop word removal + tokeizatiom ---------------------------------","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Remove stopwords and tokenize\ndef remove_stopwords(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the input review\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = remove_stopwords(review)\n    blob = TextBlob(text_without_stopwords)\n    sentiment_score = blob.sentiment.polarity\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# End timer and calculate time taken\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:40:30.145617Z","iopub.execute_input":"2024-02-25T08:40:30.146100Z","iopub.status.idle":"2024-02-25T08:41:46.778839Z","shell.execute_reply.started":"2024-02-25T08:40:30.146067Z","shell.execute_reply":"2024-02-25T08:41:46.777604Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAccuracy: 0.7099\n              precision    recall  f1-score   support\n\n           0       0.89      0.48      0.62      9935\n           1       0.65      0.94      0.77     10065\n\n    accuracy                           0.71     20000\n   macro avg       0.77      0.71      0.69     20000\nweighted avg       0.77      0.71      0.69     20000\n\nTime taken: 75.81169414520264 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------------- Testblob with stop word removal + tokeizatiom + pos ---------------------------------","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nimport nltk\nfrom nltk.corpus import stopwords\nimport time\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords and punkt tokenizer if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n# Remove stopwords, tokenize, and perform POS tagging\ndef preprocess_text(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the input review\n    words_pos = nltk.pos_tag(words)  # Perform POS tagging\n    filtered_words = [word for word, pos in words_pos if word.lower() not in stop_words and pos.startswith('N')]  # Remove stopwords and keep only nouns\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = preprocess_text(review)\n    blob = TextBlob(text_without_stopwords)\n    sentiment_score = blob.sentiment.polarity\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# End timer and calculate time taken\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:42:15.712582Z","iopub.execute_input":"2024-02-25T08:42:15.713062Z","iopub.status.idle":"2024-02-25T08:48:30.634549Z","shell.execute_reply.started":"2024-02-25T08:42:15.713015Z","shell.execute_reply":"2024-02-25T08:48:30.632887Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nAccuracy: 0.60855\n              precision    recall  f1-score   support\n\n           0       0.65      0.46      0.54      9935\n           1       0.59      0.76      0.66     10065\n\n    accuracy                           0.61     20000\n   macro avg       0.62      0.61      0.60     20000\nweighted avg       0.62      0.61      0.60     20000\n\nTime taken: 374.55364751815796 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------------- Testblob with stop word removal + tokeizatiom + pos + porter stemmer ---------------------------------","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords and punkt tokenizer if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Initialize Porter Stemmer\nporter = PorterStemmer()\n\n# Remove stopwords, tokenize, and apply Porter stemming\ndef preprocess_text(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the input review\n    filtered_words = [porter.stem(word.lower()) for word in words if word.lower() not in stop_words]  # Remove stopwords and apply Porter stemming\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = preprocess_text(review)\n    blob = TextBlob(text_without_stopwords)\n    sentiment_score = blob.sentiment.polarity\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n# Start timer\nstart_time = time.time()\n\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# End timer and calculate time taken\nend_time = time.time()\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:48:42.220266Z","iopub.execute_input":"2024-02-25T08:48:42.220799Z","iopub.status.idle":"2024-02-25T08:51:09.432837Z","shell.execute_reply.started":"2024-02-25T08:48:42.220761Z","shell.execute_reply":"2024-02-25T08:51:09.431562Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAccuracy: 0.65375\n              precision    recall  f1-score   support\n\n           0       0.80      0.40      0.53      9935\n           1       0.60      0.90      0.72     10065\n\n    accuracy                           0.65     20000\n   macro avg       0.70      0.65      0.63     20000\nweighted avg       0.70      0.65      0.63     20000\n\nTime taken: 146.8269078731537 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------------- Testblob with stop word removal + tokeizatiom + pos +  LancasterStemmer ---------------------------------\n","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import LancasterStemmer\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords and punkt tokenizer if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n# Initialize Lancaster Stemmer\nlancaster = LancasterStemmer()\n\n# Remove stopwords, tokenize, perform POS tagging, and apply Lancaster stemming\ndef preprocess_text(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the input review\n    filtered_words = [lancaster.stem(word.lower()) for word in words if word.lower() not in stop_words]  # Remove stopwords and apply Porter stemming\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = preprocess_text(review)\n    blob = TextBlob(text_without_stopwords)\n    sentiment_score = blob.sentiment.polarity\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# End timer and calculate time taken\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:52:10.197673Z","iopub.execute_input":"2024-02-25T08:52:10.198191Z","iopub.status.idle":"2024-02-25T08:54:08.802492Z","shell.execute_reply.started":"2024-02-25T08:52:10.198152Z","shell.execute_reply":"2024-02-25T08:54:08.800888Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nAccuracy: 0.6676\n              precision    recall  f1-score   support\n\n           0       0.75      0.50      0.60      9935\n           1       0.63      0.83      0.72     10065\n\n    accuracy                           0.67     20000\n   macro avg       0.69      0.67      0.66     20000\nweighted avg       0.69      0.67      0.66     20000\n\nTime taken: 118.22693824768066 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------------- Testblob with stop word removal + tokeizatiom + pos + SnowballStemmer","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords and punkt tokenizer if not already downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n# Initialize Snowball Stemmer\nsnowball = SnowballStemmer(language='english')\n\n# Remove stopwords, tokenize, perform POS tagging, and apply Snowball stemming\ndef preprocess_text(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the input review\n    filtered_words = [snowball.stem(word.lower()) for word in words if word.lower() not in stop_words]  # Remove stopwords and apply Porter stemming\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = preprocess_text(review)\n    blob = TextBlob(text_without_stopwords)\n    sentiment_score = blob.sentiment.polarity\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Start timer\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# End timer and calculate time taken\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:54:33.526990Z","iopub.execute_input":"2024-02-25T08:54:33.527508Z","iopub.status.idle":"2024-02-25T08:56:30.097978Z","shell.execute_reply.started":"2024-02-25T08:54:33.527473Z","shell.execute_reply":"2024-02-25T08:56:30.096754Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nAccuracy: 0.65515\n              precision    recall  f1-score   support\n\n           0       0.81      0.40      0.53      9935\n           1       0.60      0.91      0.73     10065\n\n    accuracy                           0.66     20000\n   macro avg       0.71      0.65      0.63     20000\nweighted avg       0.71      0.66      0.63     20000\n\nTime taken: 116.20505619049072 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"spacy","metadata":{}},{"cell_type":"code","source":"import time\nimport pandas as pd\nimport spacy\nfrom textblob import TextBlob\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load English tokenizer, tagger, parser, NER, and word vectors\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Function to preprocess text using spaCy for tokenization and lemmatization\ndef preprocess_text(review):\n    # Tokenize the review\n    doc = nlp(review)\n    # Lemmatize and remove stopwords\n    processed_text = [token.lemma_ for token in doc if not token.is_stop]\n    return ' '.join(processed_text)\n\n# Function to predict sentiment\ndef predict_sentiment(review):\n    text_without_stopwords = preprocess_text(review)\n    blob = TextBlob(text_without_stopwords)\n    sentiment_score = blob.sentiment.polarity\n    if sentiment_score >= 0:\n        return \"positive\"\n    elif sentiment_score < 0:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")\n\n# Measure time taken\nstart_time = time.time()\n\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}  # Define mapping\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)  # Convert test_data['sentiment'] to numerical\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)  # Convert predicted sentiments to numerical\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\n\n# Print classification report\nclassification_rep = classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'], target_names=[\"negative\", \"positive\"])\nprint(\"Classification Report:\")\nprint(classification_rep)\n\nend_time = time.time()\nexecution_time = end_time - start_time\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Time taken:\", execution_time, \"seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:00:31.028921Z","iopub.execute_input":"2024-02-25T07:00:31.029485Z","iopub.status.idle":"2024-02-25T07:15:33.108713Z","shell.execute_reply.started":"2024-02-25T07:00:31.029449Z","shell.execute_reply":"2024-02-25T07:15:33.107396Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.86      0.53      0.66      9935\n    positive       0.67      0.91      0.77     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.76      0.72      0.71     20000\nweighted avg       0.76      0.73      0.71     20000\n\nAccuracy: 0.7253\nTime taken: 900.827437877655 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.metrics import accuracy_score\nimport time\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(review)\n    \n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Measure time taken\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# Record end time\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:00:38.671392Z","iopub.execute_input":"2024-02-25T08:00:38.671955Z","iopub.status.idle":"2024-02-25T08:01:44.199706Z","shell.execute_reply.started":"2024-02-25T08:00:38.671921Z","shell.execute_reply":"2024-02-25T08:01:44.198216Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nAccuracy: 0.6998\n              precision    recall  f1-score   support\n\n           0       0.79      0.53      0.64      9935\n           1       0.65      0.86      0.74     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.72      0.70      0.69     20000\nweighted avg       0.72      0.70      0.69     20000\n\nTime taken: 65.13692378997803 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------- vader+stopword removal--------------------------","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nimport time\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\n\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Remove stopwords\ndef remove_stopwords(review):\n    stop_words = set(stopwords.words('english'))\n    words = review.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Remove stopwords\n    review_without_stopwords = remove_stopwords(review)\n    \n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(review_without_stopwords)\n    \n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Measure time taken\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# Record end time\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:57:05.314777Z","iopub.execute_input":"2024-02-25T07:57:05.315286Z","iopub.status.idle":"2024-02-25T07:57:54.532424Z","shell.execute_reply.started":"2024-02-25T07:57:05.315252Z","shell.execute_reply":"2024-02-25T07:57:54.531315Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nAccuracy: 0.67825\n              precision    recall  f1-score   support\n\n           0       0.79      0.48      0.60      9935\n           1       0.63      0.88      0.73     10065\n\n    accuracy                           0.68     20000\n   macro avg       0.71      0.68      0.66     20000\nweighted avg       0.71      0.68      0.66     20000\n\nTime taken: 48.84004282951355 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------- vader+stopword removal + tokenization --------------------------","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport time\nfrom sklearn.metrics import accuracy_score\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\n\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Remove stopwords\ndef remove_stopwords(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the review text\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Remove stopwords and tokenize the review\n    review_without_stopwords = remove_stopwords(review)\n    \n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(review_without_stopwords)\n    \n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Measure time taken\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# Record end time\nend_time = time.time()\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:53:34.152149Z","iopub.execute_input":"2024-02-25T07:53:34.152586Z","iopub.status.idle":"2024-02-25T07:55:04.116795Z","shell.execute_reply.started":"2024-02-25T07:53:34.152557Z","shell.execute_reply":"2024-02-25T07:55:04.115530Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nAccuracy: 0.68105\n              precision    recall  f1-score   support\n\n           0       0.79      0.49      0.60      9935\n           1       0.63      0.87      0.73     10065\n\n    accuracy                           0.68     20000\n   macro avg       0.71      0.68      0.67     20000\nweighted avg       0.71      0.68      0.67     20000\n\nTime taken: 89.58695030212402 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re  # Import regular expression module\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\n\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Remove stopwords, punctuation, special characters, and numbers\ndef remove_noise(review):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(review)  # Tokenize the review text\n    cleaned_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]  # Keep only alphabetic words\n    return ' '.join(cleaned_words)\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Remove noise and tokenize the review\n    review_cleaned = remove_noise(review)\n    \n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(review_cleaned)\n    \n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Record start time\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# Record end time\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))\nprint(\"Time taken:\", end_time - start_time, \"seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:50:58.740971Z","iopub.execute_input":"2024-02-25T07:50:58.741478Z","iopub.status.idle":"2024-02-25T07:52:26.209273Z","shell.execute_reply.started":"2024-02-25T07:50:58.741445Z","shell.execute_reply":"2024-02-25T07:52:26.207737Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nAccuracy: 0.6763\n              precision    recall  f1-score   support\n\n           0       0.79      0.48      0.59      9935\n           1       0.63      0.87      0.73     10065\n\n    accuracy                           0.68     20000\n   macro avg       0.71      0.68      0.66     20000\nweighted avg       0.71      0.68      0.66     20000\n\nTime taken: 87.09370732307434 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Porter stemmer","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import PorterStemmer  # Import Porter stemmer\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Download Porter stemmer if not already downloaded\nnltk.download('punkt')\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Instantiate the PorterStemmer\nps = PorterStemmer()\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Tokenize the review text\n    words = nltk.word_tokenize(review)\n    \n    # Apply Porter stemming to each word\n    stemmed_words = [ps.stem(word) for word in words]\n    \n    # Join the stemmed words back into a single string\n    stemmed_review = ' '.join(stemmed_words)\n    \n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(stemmed_review)\n    \n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Record start time\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# Record end time\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:45:59.065417Z","iopub.execute_input":"2024-02-25T07:45:59.065914Z","iopub.status.idle":"2024-02-25T07:49:27.127034Z","shell.execute_reply.started":"2024-02-25T07:45:59.065878Z","shell.execute_reply":"2024-02-25T07:49:27.125783Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAccuracy: 0.67275\n              precision    recall  f1-score   support\n\n           0       0.76      0.50      0.61      9935\n           1       0.63      0.84      0.72     10065\n\n    accuracy                           0.67     20000\n   macro avg       0.69      0.67      0.66     20000\nweighted avg       0.69      0.67      0.66     20000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Time taken:\", end_time - start_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:50:32.374728Z","iopub.execute_input":"2024-02-25T07:50:32.375237Z","iopub.status.idle":"2024-02-25T07:50:32.382722Z","shell.execute_reply.started":"2024-02-25T07:50:32.375203Z","shell.execute_reply":"2024-02-25T07:50:32.381330Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Time taken: 207.6756112575531 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------------- Testblob with stop word removal + tokeizatiom + pos + Lancaster stemmer ---------------------","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import LancasterStemmer  # Import Lancaster stemmer\nfrom sklearn.metrics import accuracy_score\n\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Download Lancaster stemmer if not already downloaded\nnltk.download('punkt')\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Instantiate the LancasterStemmer\nls = LancasterStemmer()\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Tokenize the review text\n    words = nltk.word_tokenize(review)\n    \n    # Apply Lancaster stemming to each word\n    stemmed_words = [ls.stem(word) for word in words]\n    \n    # Join the stemmed words back into a single string\n    stemmed_review = ' '.join(stemmed_words)\n    \n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(stemmed_review)\n    \n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Record start time\nstart_time = time.time()\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n# Record end time\nend_time = time.time()\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:40:48.638143Z","iopub.execute_input":"2024-02-25T07:40:48.638632Z","iopub.status.idle":"2024-02-25T07:43:34.625281Z","shell.execute_reply.started":"2024-02-25T07:40:48.638596Z","shell.execute_reply":"2024-02-25T07:43:34.622052Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAccuracy: 0.69225\n              precision    recall  f1-score   support\n\n           0       0.72      0.63      0.67      9935\n           1       0.67      0.76      0.71     10065\n\n    accuracy                           0.69     20000\n   macro avg       0.70      0.69      0.69     20000\nweighted avg       0.70      0.69      0.69     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"SnowballStemmer","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import SnowballStemmer  # Import Snowball stemmer\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Download Snowball stemmer if not already downloaded\nnltk.download('punkt')\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Instantiate the SnowballStemmer\nss = SnowballStemmer('english')\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Tokenize the review text\n    words = nltk.word_tokenize(review)\n    \n    # Apply Snowball stemming to each word\n    stemmed_words = [ss.stem(word) for word in words]\n    \n    # Join the stemmed words back into a single string\n    stemmed_review = ' '.join(stemmed_words)\n    \n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(stemmed_review)\n    \n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Record start time\nstart_time = time.time()\n\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# Record end time\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\n\n# Print accuracy and time taken\nprint(\"Accuracy:\", accuracy)\nprint(\"Time taken:\", end_time - start_time, \"seconds\")\nfrom sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:37:36.460985Z","iopub.execute_input":"2024-02-25T07:37:36.462435Z","iopub.status.idle":"2024-02-25T07:40:19.919732Z","shell.execute_reply.started":"2024-02-25T07:37:36.462394Z","shell.execute_reply":"2024-02-25T07:40:19.918113Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAccuracy: 0.6807\nTime taken: 162.8048837184906 seconds\n              precision    recall  f1-score   support\n\n           0       0.75      0.53      0.62      9935\n           1       0.64      0.83      0.72     10065\n\n    accuracy                           0.68     20000\n   macro avg       0.70      0.68      0.67     20000\nweighted avg       0.70      0.68      0.67     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"spacy","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Download VADER lexicon if not already downloaded\nnltk.download('vader_lexicon')\n\n# Load English tokenizer, tagger, parser, NER, and word vectors\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Instantiate the VADER SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Function to preprocess text using spaCy for tokenization and lemmatization\ndef preprocess_text(review):\n    # Tokenize the review\n    doc = nlp(review)\n    # Lemmatize and remove stopwords\n    processed_text = [token.lemma_ for token in doc if not token.is_stop]\n    return ' '.join(processed_text)\n\n# Function to predict sentiment using VADER\ndef predict_sentiment(review):\n    # Preprocess the review text\n    processed_review = preprocess_text(review)\n    # Get the polarity scores for the review\n    scores = sid.polarity_scores(processed_review)\n    # Determine sentiment based on the compound score\n    if scores['compound'] >= 0:\n        return \"positive\"\n    else:\n        return \"negative\"\n\n# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/lexicondataset1/test.csv/test.csv\")  # Assuming the test data path is correct\n\n# Record start time\nstart_time = time.time()\n\n# Apply sentiment analysis on the test set\ntest_data['Predicted_Sentiment'] = test_data['review'].apply(predict_sentiment)\n\n# Record end time\nend_time = time.time()\n\n# Map string labels to numerical labels\nlabel_mapping = {\"positive\": 1, \"negative\": 0}\ntest_data['Sentiment_Num'] = test_data['sentiment'].map(label_mapping)\ntest_data['Predicted_Sentiment_Num'] = test_data['Predicted_Sentiment'].map(label_mapping)\n\n# Calculate accuracy\naccuracy = accuracy_score(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num'])\n\n# Print accuracy and time taken\nprint(\"Accuracy:\", accuracy)\nprint(\"Time taken:\", end_time - start_time, \"seconds\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:16:49.507519Z","iopub.execute_input":"2024-02-25T07:16:49.508038Z","iopub.status.idle":"2024-02-25T07:32:18.126919Z","shell.execute_reply.started":"2024-02-25T07:16:49.507985Z","shell.execute_reply":"2024-02-25T07:32:18.124999Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nAccuracy: 0.6762\nTime taken: 927.3920292854309 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Print classification report\nprint(classification_report(test_data['Sentiment_Num'], test_data['Predicted_Sentiment_Num']))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T07:36:25.395735Z","iopub.execute_input":"2024-02-25T07:36:25.397348Z","iopub.status.idle":"2024-02-25T07:36:25.448980Z","shell.execute_reply.started":"2024-02-25T07:36:25.397289Z","shell.execute_reply":"2024-02-25T07:36:25.447800Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.78      0.49      0.60      9935\n           1       0.63      0.86      0.73     10065\n\n    accuracy                           0.68     20000\n   macro avg       0.70      0.67      0.66     20000\nweighted avg       0.70      0.68      0.66     20000\n\n","output_type":"stream"}]}]}