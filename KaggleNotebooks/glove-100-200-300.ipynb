{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7648732,"sourceType":"datasetVersion","datasetId":4458685},{"sourceId":7680062,"sourceType":"datasetVersion","datasetId":4480657}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"glove","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 100  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.100d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:37:06.552168Z","iopub.execute_input":"2024-02-25T15:37:06.552509Z","iopub.status.idle":"2024-02-25T15:53:43.949602Z","shell.execute_reply.started":"2024-02-25T15:37:06.552479Z","shell.execute_reply":"2024-02-25T15:53:43.948606Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-25 15:37:09.997544: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 15:37:09.997674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 15:37:10.117544: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1708875476.157085     111 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"750/750 [==============================] - 190s 246ms/step - loss: 0.4559 - accuracy: 0.7809 - val_loss: 0.3434 - val_accuracy: 0.8502\nEpoch 2/5\n750/750 [==============================] - 183s 244ms/step - loss: 0.3137 - accuracy: 0.8662 - val_loss: 0.3127 - val_accuracy: 0.8678\nEpoch 3/5\n750/750 [==============================] - 184s 246ms/step - loss: 0.2607 - accuracy: 0.8920 - val_loss: 0.2990 - val_accuracy: 0.8743\nEpoch 4/5\n750/750 [==============================] - 183s 244ms/step - loss: 0.2167 - accuracy: 0.9153 - val_loss: 0.3503 - val_accuracy: 0.8443\nEpoch 5/5\n750/750 [==============================] - 183s 244ms/step - loss: 0.1760 - accuracy: 0.9340 - val_loss: 0.3137 - val_accuracy: 0.8717\n625/625 [==============================] - 29s 46ms/step - loss: 0.3171 - accuracy: 0.8644\nTest accuracy: 0.8644000291824341\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 200  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.200d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T16:20:05.897269Z","iopub.execute_input":"2024-02-25T16:20:05.897619Z","iopub.status.idle":"2024-02-25T16:35:27.773544Z","shell.execute_reply.started":"2024-02-25T16:20:05.897593Z","shell.execute_reply":"2024-02-25T16:35:27.772732Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 174s 228ms/step - loss: 0.4101 - accuracy: 0.8048 - val_loss: 0.3090 - val_accuracy: 0.8718\nEpoch 2/5\n750/750 [==============================] - 173s 230ms/step - loss: 0.2788 - accuracy: 0.8844 - val_loss: 0.2806 - val_accuracy: 0.8803\nEpoch 3/5\n750/750 [==============================] - 169s 226ms/step - loss: 0.2140 - accuracy: 0.9150 - val_loss: 0.2992 - val_accuracy: 0.8708\nEpoch 4/5\n750/750 [==============================] - 169s 226ms/step - loss: 0.1577 - accuracy: 0.9435 - val_loss: 0.2790 - val_accuracy: 0.8828\nEpoch 5/5\n750/750 [==============================] - 171s 227ms/step - loss: 0.1084 - accuracy: 0.9673 - val_loss: 0.2872 - val_accuracy: 0.8817\n625/625 [==============================] - 27s 43ms/step - loss: 0.2910 - accuracy: 0.8835\nTest accuracy: 0.8835499882698059\nTime required to fine-tune:  921.8578188419342\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T16:41:59.763789Z","iopub.execute_input":"2024-02-25T16:41:59.764708Z","iopub.status.idle":"2024-02-25T16:58:45.954806Z","shell.execute_reply.started":"2024-02-25T16:41:59.764673Z","shell.execute_reply":"2024-02-25T16:58:45.953862Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 179s 235ms/step - loss: 0.3902 - accuracy: 0.8217 - val_loss: 0.3019 - val_accuracy: 0.8753\nEpoch 2/5\n750/750 [==============================] - 173s 231ms/step - loss: 0.2528 - accuracy: 0.8959 - val_loss: 0.2744 - val_accuracy: 0.8883\nEpoch 3/5\n750/750 [==============================] - 174s 232ms/step - loss: 0.1846 - accuracy: 0.9302 - val_loss: 0.2635 - val_accuracy: 0.8925\nEpoch 4/5\n750/750 [==============================] - 173s 230ms/step - loss: 0.1206 - accuracy: 0.9601 - val_loss: 0.2942 - val_accuracy: 0.8870\nEpoch 5/5\n750/750 [==============================] - 173s 231ms/step - loss: 0.0702 - accuracy: 0.9825 - val_loss: 0.3101 - val_accuracy: 0.8828\n625/625 [==============================] - 27s 44ms/step - loss: 0.3178 - accuracy: 0.8789\nTest accuracy: 0.8789499998092651\nTime required to fine-tune:  1006.1718916893005\n","output_type":"stream"}]},{"cell_type":"markdown","source":"50d","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 50 # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.50d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T17:01:39.574852Z","iopub.execute_input":"2024-02-25T17:01:39.575497Z","iopub.status.idle":"2024-02-25T17:17:34.932990Z","shell.execute_reply.started":"2024-02-25T17:01:39.575466Z","shell.execute_reply":"2024-02-25T17:17:34.932041Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 183s 240ms/step - loss: 0.5043 - accuracy: 0.7503 - val_loss: 0.4002 - val_accuracy: 0.8260\nEpoch 2/5\n750/750 [==============================] - 180s 240ms/step - loss: 0.3671 - accuracy: 0.8388 - val_loss: 0.3576 - val_accuracy: 0.8438\nEpoch 3/5\n750/750 [==============================] - 175s 233ms/step - loss: 0.3198 - accuracy: 0.8652 - val_loss: 0.3614 - val_accuracy: 0.8397\nEpoch 4/5\n750/750 [==============================] - 175s 233ms/step - loss: 0.2858 - accuracy: 0.8797 - val_loss: 0.3447 - val_accuracy: 0.8517\nEpoch 5/5\n750/750 [==============================] - 178s 237ms/step - loss: 0.2565 - accuracy: 0.8972 - val_loss: 0.3349 - val_accuracy: 0.8507\n625/625 [==============================] - 27s 43ms/step - loss: 0.3350 - accuracy: 0.8525\nTest accuracy: 0.8524500131607056\nTime required to fine-tune:  955.3397836685181\n","output_type":"stream"}]},{"cell_type":"markdown","source":"10 epochs","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 10\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:26:00.293901Z","iopub.execute_input":"2024-02-22T18:26:00.294758Z","iopub.status.idle":"2024-02-22T18:57:25.908322Z","shell.execute_reply.started":"2024-02-22T18:26:00.294726Z","shell.execute_reply":"2024-02-22T18:57:25.907403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"20 epochs","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\nimport tensorflow as tf\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 20  # Increase the number of epochs\nearly_stopping_patience = 3  # Early stopping patience\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model with early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True\n)\n\n# Train the model with modified epochs and callbacks\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n          validation_split=0.2, callbacks=[early_stopping])\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T19:01:31.528819Z","iopub.execute_input":"2024-02-22T19:01:31.529182Z","iopub.status.idle":"2024-02-22T19:17:57.114827Z","shell.execute_reply.started":"2024-02-22T19:01:31.529155Z","shell.execute_reply":"2024-02-22T19:17:57.113990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"l2 regularizatiom","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import regularizers\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5  # Increase the number of epochs\nearly_stopping_patience = 3  # Early stopping patience\ndropout_rate = 0.2  # Dropout rate for regularization\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch with dropout\nlstm_branch = LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate,\n                   kernel_regularizer=regularizers.l2(0.001))(embedding_layer)\n\n# CNN branch with dropout\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\ncnn_branch = Dropout(rate=dropout_rate)(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model with early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True\n)\n\n# Train the model with modified epochs and callbacks\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n          validation_split=0.2, callbacks=[early_stopping])\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T19:20:25.209121Z","iopub.execute_input":"2024-02-22T19:20:25.209527Z","iopub.status.idle":"2024-02-22T19:37:05.275145Z","shell.execute_reply.started":"2024-02-22T19:20:25.209495Z","shell.execute_reply":"2024-02-22T19:37:05.274283Z"},"trusted":true},"execution_count":null,"outputs":[]}]}