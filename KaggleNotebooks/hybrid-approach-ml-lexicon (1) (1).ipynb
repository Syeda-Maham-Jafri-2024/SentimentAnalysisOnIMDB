{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7631479,"sourceType":"datasetVersion","datasetId":4446686}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install afinn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-17T10:58:48.647522Z","iopub.execute_input":"2024-02-17T10:58:48.648252Z","iopub.status.idle":"2024-02-17T10:59:09.038185Z","shell.execute_reply.started":"2024-02-17T10:58:48.648209Z","shell.execute_reply":"2024-02-17T10:59:09.036990Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting afinn\n  Downloading afinn-0.1.tar.gz (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: afinn\n  Building wheel for afinn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=3814c664a2a7b9545ed435a6bf6c2c9a57f2f5ab327a011dbf9445184c63cda7\n  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\nSuccessfully built afinn\nInstalling collected packages: afinn\nSuccessfully installed afinn-0.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Afinn without preprocessing with ML models with tf idf vectorizer ","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['review'])\ntest_tfidf = tfidf_vectorizer.transform(test['review'] )\n\n# Initialize classifiers\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Neural Network\": MLPClassifier(max_iter=1000),\n    \"KNN\": KNeighborsClassifier(),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T15:41:58.651774Z","iopub.execute_input":"2024-02-15T15:41:58.652565Z","iopub.status.idle":"2024-02-15T16:38:25.393699Z","shell.execute_reply.started":"2024-02-15T15:41:58.652532Z","shell.execute_reply":"2024-02-15T16:38:25.392371Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.89005\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.88      0.89      9935\n    positive       0.88      0.90      0.89     10065\n\n    accuracy                           0.89     20000\n   macro avg       0.89      0.89      0.89     20000\nweighted avg       0.89      0.89      0.89     20000\n\nDecision Tree Accuracy: 0.71155\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.71      0.71      0.71      9935\n    positive       0.72      0.71      0.71     10065\n\n    accuracy                           0.71     20000\n   macro avg       0.71      0.71      0.71     20000\nweighted avg       0.71      0.71      0.71     20000\n\nRandom Forest Accuracy: 0.84345\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.83      0.86      0.84      9935\n    positive       0.85      0.83      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\nGradient Boosting Accuracy: 0.81565\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.77      0.81      9935\n    positive       0.79      0.86      0.82     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.8643\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.86      9935\n    positive       0.86      0.87      0.87     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\nKNN Accuracy: 0.7169\nKNN Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.73      0.68      0.70      9935\n    positive       0.70      0.75      0.73     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Stacking\u001b[39;00m\n\u001b[1;32m     48\u001b[0m estimators \u001b[38;5;241m=\u001b[39m [(name, model) \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m---> 49\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m \u001b[43mStackingClassifier\u001b[49m(estimators\u001b[38;5;241m=\u001b[39mestimators, final_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression())\n\u001b[1;32m     50\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(train_tfidf, train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     51\u001b[0m stacking_predictions \u001b[38;5;241m=\u001b[39m stacking_model\u001b[38;5;241m.\u001b[39mpredict(test_tfidf)\n","\u001b[0;31mNameError\u001b[0m: name 'StackingClassifier' is not defined"],"ename":"NameError","evalue":"name 'StackingClassifier' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Afinn with stopword removal and tokenization with ML models with Count vectorizer","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Preprocessing\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Tokenization\n    tokens = word_tokenize(text)\n    # Removing stop words\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n    return \" \".join(filtered_tokens)\n\ntrain['review'] = train['review'].apply(preprocess_text)\ntest['review'] = test['review'].apply(preprocess_text)\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=5000)\ntrain_count = count_vectorizer.fit_transform(train['review'])\ntest_count = count_vectorizer.transform(test['review'])\n\n# Initialize classifiers\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Neural Network\": MLPClassifier(max_iter=1000),\n    \"KNN\": KNeighborsClassifier(),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_count, train['sentiment'])\n    predictions = model.predict(test_count)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T17:15:43.639308Z","iopub.execute_input":"2024-02-15T17:15:43.639629Z","iopub.status.idle":"2024-02-15T17:22:30.909962Z","shell.execute_reply.started":"2024-02-15T17:15:43.639606Z","shell.execute_reply":"2024-02-15T17:22:30.909027Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.8696\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.87      0.87      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\nDecision Tree Accuracy: 0.72205\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.72      0.73      0.72      9935\n    positive       0.73      0.71      0.72     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\nRandom Forest Accuracy: 0.8479\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.86      0.85      9935\n    positive       0.86      0.84      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\nGradient Boosting Accuracy: 0.81235\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.76      0.80      9935\n    positive       0.79      0.86      0.82     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.82      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\nNeural Network Accuracy: 0.8696\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.87      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\nKNN Accuracy: 0.6178\nKNN Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.60      0.69      0.64      9935\n    positive       0.64      0.54      0.59     10065\n\n    accuracy                           0.62     20000\n   macro avg       0.62      0.62      0.62     20000\nweighted avg       0.62      0.62      0.62     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Afinn with stopword removal, pos and tokenization with ML models with tf idf vectorizer\n","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Preprocessing\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Tokenization\n    tokens = word_tokenize(text)\n    # POS tagging\n    pos_tags = pos_tag(tokens)\n    # Removing stop words and non-alphabetic words, and keeping only nouns, verbs, adjectives, and adverbs\n    filtered_tokens = [word.lower() for word, tag in pos_tags if word.lower() not in stop_words and word.isalpha() and (tag.startswith('N') or tag.startswith('V') or tag.startswith('J') or tag.startswith('R'))]\n    return \" \".join(filtered_tokens)\n\ntrain['review'] = train['review'].apply(preprocess_text)\ntest['review'] = test['review'].apply(preprocess_text)\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=5000)\ntrain_count = count_vectorizer.fit_transform(train['review'])\ntest_count = count_vectorizer.transform(test['review'])\n\n# Initialize classifiers\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Neural Network\": MLPClassifier(max_iter=1000),\n    \"KNN\": KNeighborsClassifier(),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_count, train['sentiment'])\n    predictions = model.predict(test_count)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T17:28:47.389138Z","iopub.execute_input":"2024-02-15T17:28:47.389476Z","iopub.status.idle":"2024-02-15T17:44:02.718401Z","shell.execute_reply.started":"2024-02-15T17:28:47.389453Z","shell.execute_reply":"2024-02-15T17:44:02.717558Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.86515\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.86      9935\n    positive       0.86      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\nDecision Tree Accuracy: 0.7174\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.71      0.72      0.72      9935\n    positive       0.72      0.71      0.72     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\nRandom Forest Accuracy: 0.84245\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.83      0.85      0.84      9935\n    positive       0.85      0.83      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\nGradient Boosting Accuracy: 0.805\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.75      0.79      9935\n    positive       0.78      0.86      0.82     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.80      0.80     20000\nweighted avg       0.81      0.81      0.80     20000\n\nNeural Network Accuracy: 0.86515\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.86      9935\n    positive       0.86      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\nKNN Accuracy: 0.60745\nKNN Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.59      0.71      0.64      9935\n    positive       0.64      0.51      0.57     10065\n\n    accuracy                           0.61     20000\n   macro avg       0.61      0.61      0.60     20000\nweighted avg       0.61      0.61      0.60     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"afinn + no preprocessing + tf idf vectorizer + improved ml models","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Vectorize the text data using TF-IDF with improved parameters\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['review'])\ntest_tfidf = tfidf_vectorizer.transform(test['review'] )\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T17:48:04.943385Z","iopub.execute_input":"2024-02-15T17:48:04.943675Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.89705\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.89      0.90      9935\n    positive       0.89      0.91      0.90     10065\n\n    accuracy                           0.90     20000\n   macro avg       0.90      0.90      0.90     20000\nweighted avg       0.90      0.90      0.90     20000\n\nDecision Tree Accuracy: 0.73175\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.80      0.62      0.70      9935\n    positive       0.69      0.84      0.76     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.74      0.73      0.73     20000\nweighted avg       0.74      0.73      0.73     20000\n\nRandom Forest Accuracy: 0.8258\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.79      0.82      9935\n    positive       0.80      0.86      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\nGradient Boosting Accuracy: 0.81655\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.77      0.81      9935\n    positive       0.79      0.86      0.82     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.8657\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.86      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"afinn + no preprocessing + count vectorizer + improved ml models","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB  # Added Naive Bayes\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_count = count_vectorizer.fit_transform(train['review'])\ntest_count = count_vectorizer.transform(test['review'] )\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"Naive Bayes\": MultinomialNB()  # Added Naive Bayes\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_count, train['sentiment'])\n    predictions = model.predict(test_count)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:59:24.079682Z","iopub.execute_input":"2024-02-17T10:59:24.080147Z","iopub.status.idle":"2024-02-17T11:12:48.283596Z","shell.execute_reply.started":"2024-02-17T10:59:24.080106Z","shell.execute_reply":"2024-02-17T11:12:48.282329Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.8762\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.88      0.87      0.88      9935\n    positive       0.88      0.88      0.88     10065\n\n    accuracy                           0.88     20000\n   macro avg       0.88      0.88      0.88     20000\nweighted avg       0.88      0.88      0.88     20000\n\nDecision Tree Accuracy: 0.73015\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.78      0.63      0.70      9935\n    positive       0.69      0.83      0.76     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.74      0.73      0.73     20000\nweighted avg       0.74      0.73      0.73     20000\n\nRandom Forest Accuracy: 0.82395\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.86      0.77      0.81      9935\n    positive       0.79      0.88      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.83      0.82      0.82     20000\nweighted avg       0.83      0.82      0.82     20000\n\nGradient Boosting Accuracy: 0.81795\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.77      0.81      9935\n    positive       0.79      0.86      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.892\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.88      0.89      9935\n    positive       0.89      0.90      0.89     10065\n\n    accuracy                           0.89     20000\n   macro avg       0.89      0.89      0.89     20000\nweighted avg       0.89      0.89      0.89     20000\n\nNaive Bayes Accuracy: 0.85135\nNaive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.85      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Initialize stemmers\nporter_stemmer = PorterStemmer()\nlancaster_stemmer = LancasterStemmer()\nsnowball_stemmer = SnowballStemmer(\"english\")\n\n# Apply stemming using Porter Stemmer\ntrain['porter_stemmed'] = train['review'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in x.split()]))\ntest['porter_stemmed'] = test['review'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in x.split()]))\n\n\n# Vectorize the text data using TF-IDF with improved parameters\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\n\n# Train TF-IDF vectorizer on stemmed text\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['porter_stemmed'])\ntest_tfidf = tfidf_vectorizer.transform(test['porter_stemmed'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T14:09:12.881619Z","iopub.execute_input":"2024-02-16T14:09:12.882026Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.8947\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.89      0.89      9935\n    positive       0.89      0.90      0.90     10065\n\n    accuracy                           0.89     20000\n   macro avg       0.89      0.89      0.89     20000\nweighted avg       0.89      0.89      0.89     20000\n\nDecision Tree Accuracy: 0.7223\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.80      0.59      0.68      9935\n    positive       0.68      0.85      0.76     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.74      0.72      0.72     20000\nweighted avg       0.74      0.72      0.72     20000\n\nRandom Forest Accuracy: 0.8253\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.78      0.82      9935\n    positive       0.80      0.87      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.82     20000\nweighted avg       0.83      0.83      0.82     20000\n\nGradient Boosting Accuracy: 0.81215\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.77      0.80      9935\n    positive       0.79      0.85      0.82     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\nNeural Network Accuracy: 0.8685\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.87      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Initialize stemmers\nporter_stemmer = PorterStemmer()\nlancaster_stemmer = LancasterStemmer()\nsnowball_stemmer = SnowballStemmer(\"english\")\n\n# Apply stemming using Lancaster Stemmer\ntrain['lancaster_stemmed'] = train['review'].apply(lambda x: ' '.join([lancaster_stemmer.stem(word) for word in x.split()]))\ntest['lancaster_stemmed'] = test['review'].apply(lambda x: ' '.join([lancaster_stemmer.stem(word) for word in x.split()]))\n\n# Vectorize the text data using TF-IDF with improved parameters\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\n\n# Train TF-IDF vectorizer on stemmed text\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['lancaster_stemmed'])\ntest_tfidf = tfidf_vectorizer.transform(test['lancaster_stemmed'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T17:18:07.456984Z","iopub.execute_input":"2024-02-16T17:18:07.457382Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.8932\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.88      0.89      9935\n    positive       0.89      0.90      0.89     10065\n\n    accuracy                           0.89     20000\n   macro avg       0.89      0.89      0.89     20000\nweighted avg       0.89      0.89      0.89     20000\n\nDecision Tree Accuracy: 0.7242\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.80      0.59      0.68      9935\n    positive       0.68      0.85      0.76     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.74      0.72      0.72     20000\nweighted avg       0.74      0.72      0.72     20000\n\nRandom Forest Accuracy: 0.8192\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.77      0.81      9935\n    positive       0.79      0.86      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nGradient Boosting Accuracy: 0.81015\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.77      0.80      9935\n    positive       0.79      0.85      0.82     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\nNeural Network Accuracy: 0.85525\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.86      0.86      9935\n    positive       0.86      0.85      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Initialize stemmers\nporter_stemmer = PorterStemmer()\nlancaster_stemmer = LancasterStemmer()\nsnowball_stemmer = SnowballStemmer(\"english\")\n\n# Apply stemming using Snowball Stemmer\ntrain['snowball_stemmed'] = train['review'].apply(lambda x: ' '.join([snowball_stemmer.stem(word) for word in x.split()]))\ntest['snowball_stemmed'] = test['review'].apply(lambda x: ' '.join([snowball_stemmer.stem(word) for word in x.split()]))\n\n# Vectorize the text data using TF-IDF with improved parameters\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\n\n# Train TF-IDF vectorizer on stemmed text\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['snowball_stemmed'])\ntest_tfidf = tfidf_vectorizer.transform(test['snowball_stemmed'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download en_core_web_sm","metadata":{"execution":{"iopub.status.busy":"2024-02-17T07:58:31.456322Z","iopub.execute_input":"2024-02-17T07:58:31.456758Z","iopub.status.idle":"2024-02-17T07:59:12.219589Z","shell.execute_reply.started":"2024-02-17T07:58:31.456723Z","shell.execute_reply":"2024-02-17T07:59:12.218006Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\nCollecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"from afinn import Afinn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\nimport spacy\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Initialize Spacy's English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Define function to lemmatize text using Spacy\ndef lemmatize_text(text):\n    doc = nlp(text)\n    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n    return lemmatized_text\n\n# Apply lemmatization to the review text\ntrain['lemmatized_review'] = train['review'].apply(lemmatize_text)\ntest['lemmatized_review'] = test['review'].apply(lemmatize_text)\n\n# Vectorize the lemmatized text data using TF-IDF with improved parameters\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['lemmatized_review'])\ntest_tfidf = tfidf_vectorizer.transform(test['lemmatized_review'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T08:00:21.610232Z","iopub.execute_input":"2024-02-17T08:00:21.610713Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.89655\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.89      0.90      9935\n    positive       0.89      0.91      0.90     10065\n\n    accuracy                           0.90     20000\n   macro avg       0.90      0.90      0.90     20000\nweighted avg       0.90      0.90      0.90     20000\n\nDecision Tree Accuracy: 0.73585\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.76      0.68      0.72      9935\n    positive       0.71      0.79      0.75     10065\n\n    accuracy                           0.74     20000\n   macro avg       0.74      0.74      0.73     20000\nweighted avg       0.74      0.74      0.73     20000\n\nRandom Forest Accuracy: 0.8263\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.78      0.82      9935\n    positive       0.80      0.87      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\nGradient Boosting Accuracy: 0.818\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.78      0.81      9935\n    positive       0.80      0.86      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.86365\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.87      0.86      9935\n    positive       0.87      0.85      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--------------------------------- TEXTBLOB -------------------------------","metadata":{}},{"cell_type":"markdown","source":"Textblob + no preprocessing + count vectorizer + improved ml models","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom textblob import TextBlob  # Import TextBlob\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Perform sentiment analysis using TextBlob and add scores to dataset\ntrain['textblob_score'] = train['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\ntest['textblob_score'] = test['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_count = count_vectorizer.fit_transform(train['review'])\ntest_count = count_vectorizer.transform(test['review'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"Naive Bayes\": MultinomialNB()\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_count, train['sentiment'])\n    predictions = model.predict(test_count)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T11:17:52.956532Z","iopub.execute_input":"2024-02-17T11:17:52.957824Z","iopub.status.idle":"2024-02-17T11:27:10.321831Z","shell.execute_reply.started":"2024-02-17T11:17:52.957780Z","shell.execute_reply":"2024-02-17T11:27:10.320497Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.8762\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.88      0.87      0.88      9935\n    positive       0.88      0.88      0.88     10065\n\n    accuracy                           0.88     20000\n   macro avg       0.88      0.88      0.88     20000\nweighted avg       0.88      0.88      0.88     20000\n\nDecision Tree Accuracy: 0.7301\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.79      0.63      0.70      9935\n    positive       0.69      0.83      0.76     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.74      0.73      0.73     20000\nweighted avg       0.74      0.73      0.73     20000\n\nRandom Forest Accuracy: 0.82745\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.79      0.82      9935\n    positive       0.81      0.87      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\nGradient Boosting Accuracy: 0.81795\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.77      0.81      9935\n    positive       0.79      0.86      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.8901\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.89      0.88      0.89      9935\n    positive       0.89      0.90      0.89     10065\n\n    accuracy                           0.89     20000\n   macro avg       0.89      0.89      0.89     20000\nweighted avg       0.89      0.89      0.89     20000\n\nNaive Bayes Accuracy: 0.85135\nNaive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.85      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Textblob + no preprocessing + tf-idf + improved ml models","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom textblob import TextBlob  # Import TextBlob\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Perform sentiment analysis using TextBlob and add scores to dataset\ntrain['textblob_score'] = train['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\ntest['textblob_score'] = test['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['review'])\ntest_tfidf = tfidf_vectorizer.transform(test['review'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"Naive Bayes\": MultinomialNB()\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T11:28:29.241699Z","iopub.execute_input":"2024-02-17T11:28:29.242120Z","iopub.status.idle":"2024-02-17T11:42:30.708218Z","shell.execute_reply.started":"2024-02-17T11:28:29.242091Z","shell.execute_reply":"2024-02-17T11:42:30.706859Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.89705\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.89      0.90      9935\n    positive       0.89      0.91      0.90     10065\n\n    accuracy                           0.90     20000\n   macro avg       0.90      0.90      0.90     20000\nweighted avg       0.90      0.90      0.90     20000\n\nDecision Tree Accuracy: 0.7317\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.80      0.62      0.70      9935\n    positive       0.69      0.84      0.76     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.74      0.73      0.73     20000\nweighted avg       0.74      0.73      0.73     20000\n\nRandom Forest Accuracy: 0.82445\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.86      0.78      0.81      9935\n    positive       0.80      0.87      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.83      0.82      0.82     20000\nweighted avg       0.83      0.82      0.82     20000\n\nGradient Boosting Accuracy: 0.8166\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.77      0.81      9935\n    positive       0.79      0.86      0.82     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.86825\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.87      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\nNaive Bayes Accuracy: 0.86895\nNaive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.87      9935\n    positive       0.86      0.88      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"-------------------------- vader ----------------------","metadata":{}},{"cell_type":"markdown","source":"vader + no preprocessing + count vectorizer + improved ml models","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer  # Import VADER\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize VADER sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Perform sentiment analysis using VADER and add scores to dataset\ntrain['vader_score'] = train['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\ntest['vader_score'] = test['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_count = count_vectorizer.fit_transform(train['review'])\ntest_count = count_vectorizer.transform(test['review'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"Naive Bayes\": MultinomialNB()\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_count, train['sentiment'])\n    predictions = model.predict(test_count)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T11:50:32.768306Z","iopub.execute_input":"2024-02-17T11:50:32.768741Z","iopub.status.idle":"2024-02-17T12:00:51.999006Z","shell.execute_reply.started":"2024-02-17T11:50:32.768711Z","shell.execute_reply":"2024-02-17T12:00:51.997741Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.8762\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.88      0.87      0.88      9935\n    positive       0.88      0.88      0.88     10065\n\n    accuracy                           0.88     20000\n   macro avg       0.88      0.88      0.88     20000\nweighted avg       0.88      0.88      0.88     20000\n\nDecision Tree Accuracy: 0.72985\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.79      0.63      0.70      9935\n    positive       0.69      0.83      0.76     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.74      0.73      0.73     20000\nweighted avg       0.74      0.73      0.73     20000\n\nRandom Forest Accuracy: 0.8256\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.86      0.78      0.82      9935\n    positive       0.80      0.87      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\nGradient Boosting Accuracy: 0.81795\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.77      0.81      9935\n    positive       0.79      0.86      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.8874\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.89      0.89      0.89      9935\n    positive       0.89      0.89      0.89     10065\n\n    accuracy                           0.89     20000\n   macro avg       0.89      0.89      0.89     20000\nweighted avg       0.89      0.89      0.89     20000\n\nNaive Bayes Accuracy: 0.85135\nNaive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.85      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"vader + no preprocessing + tf idf vectorizer + improved ml models\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer  # Import TF-IDF Vectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer  # Import VADER\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize VADER sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Perform sentiment analysis using VADER and add scores to dataset\ntrain['vader_score'] = train['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\ntest['vader_score'] = test['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\n\n# Vectorize the text data using TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['review'])\ntest_tfidf = tfidf_vectorizer.transform(test['review'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"Naive Bayes\": MultinomialNB()\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T12:02:05.549269Z","iopub.execute_input":"2024-02-17T12:02:05.550490Z","iopub.status.idle":"2024-02-17T12:18:28.480770Z","shell.execute_reply.started":"2024-02-17T12:02:05.550430Z","shell.execute_reply":"2024-02-17T12:18:28.478807Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.89705\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.89      0.90      9935\n    positive       0.89      0.91      0.90     10065\n\n    accuracy                           0.90     20000\n   macro avg       0.90      0.90      0.90     20000\nweighted avg       0.90      0.90      0.90     20000\n\nDecision Tree Accuracy: 0.73165\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.80      0.62      0.70      9935\n    positive       0.69      0.84      0.76     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.74      0.73      0.73     20000\nweighted avg       0.74      0.73      0.73     20000\n\nRandom Forest Accuracy: 0.82395\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.78      0.82      9935\n    positive       0.80      0.86      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.83      0.82      0.82     20000\nweighted avg       0.83      0.82      0.82     20000\n\nGradient Boosting Accuracy: 0.8166\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.84      0.77      0.81      9935\n    positive       0.79      0.86      0.82     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.8596\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.88      0.83      0.85      9935\n    positive       0.84      0.89      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\nNaive Bayes Accuracy: 0.86895\nNaive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.87      9935\n    positive       0.86      0.88      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"vader + tokenization + tf idf vectorizer + improved ml models\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer  # Import TF-IDF Vectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer  # Import VADER\nfrom nltk.tokenize import word_tokenize  # Import NLTK's word_tokenize\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize VADER sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Perform sentiment analysis using VADER and add scores to dataset\ntrain['vader_score'] = train['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\ntest['vader_score'] = test['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\n\n# Tokenize the text data using NLTK's word_tokenize\ntrain['review'] = train['review'].apply(word_tokenize)\ntest['review'] = test['review'].apply(word_tokenize)\n\n# Convert the tokenized text back to strings\ntrain['review'] = train['review'].apply(lambda x: ' '.join(x))\ntest['review'] = test['review'].apply(lambda x: ' '.join(x))\n\n# Vectorize the text data using TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['review'])\ntest_tfidf = tfidf_vectorizer.transform(test['review'])\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"Naive Bayes\": MultinomialNB()\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_tfidf, train['sentiment'])\n    predictions = model.predict(test_tfidf)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:45:11.642222Z","iopub.execute_input":"2024-02-17T14:45:11.645614Z","iopub.status.idle":"2024-02-17T15:03:36.748905Z","shell.execute_reply.started":"2024-02-17T14:45:11.645551Z","shell.execute_reply":"2024-02-17T15:03:36.747903Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Logistic Regression Accuracy: 0.897\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.90      0.89      0.90      9935\n    positive       0.89      0.90      0.90     10065\n\n    accuracy                           0.90     20000\n   macro avg       0.90      0.90      0.90     20000\nweighted avg       0.90      0.90      0.90     20000\n\nDecision Tree Accuracy: 0.7329\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.80      0.62      0.70      9935\n    positive       0.69      0.85      0.76     10065\n\n    accuracy                           0.73     20000\n   macro avg       0.75      0.73      0.73     20000\nweighted avg       0.74      0.73      0.73     20000\n\nRandom Forest Accuracy: 0.82395\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.86      0.77      0.81      9935\n    positive       0.80      0.87      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.83      0.82      0.82     20000\nweighted avg       0.83      0.82      0.82     20000\n\nGradient Boosting Accuracy: 0.8182\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.77      0.81      9935\n    positive       0.79      0.86      0.83     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\nNeural Network Accuracy: 0.86785\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.87      0.87      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\nNaive Bayes Accuracy: 0.8696\nNaive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.88      0.86      0.87      9935\n    positive       0.86      0.88      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nimport pandas as pd\nimport scipy.sparse as sp\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize VADER sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Perform sentiment analysis using VADER and add scores to dataset\ntrain['vader_score'] = train['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\ntest['vader_score'] = test['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\n\n# Perform sentiment analysis using TextBlob and add scores to dataset\ntrain['textblob_score'] = train['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\ntest['textblob_score'] = test['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Perform sentiment analysis using AFINN and add scores to dataset\ntrain['afinn_score'] = train['review'].apply(lambda x: afinn.score(x))\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\n\n# Make sentiment scores non-negative by shifting\ntrain[['vader_score', 'textblob_score', 'afinn_score']] += 1  # Add 1 to shift scores\ntest[['vader_score', 'textblob_score', 'afinn_score']] += 1  # Add 1 to shift scores\n\n# Vectorize the text data using TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train['review'])\ntest_tfidf = tfidf_vectorizer.transform(test['review'])\n\n# Convert sparse matrix to DataFrame\ntrain_tfidf_df = pd.DataFrame.sparse.from_spmatrix(train_tfidf, columns=tfidf_vectorizer.get_feature_names_out())\ntest_tfidf_df = pd.DataFrame.sparse.from_spmatrix(test_tfidf, columns=tfidf_vectorizer.get_feature_names_out())\n\n# Combine sentiment scores with TF-IDF features\ntrain_features = pd.concat([train_tfidf_df, train[['vader_score', 'textblob_score', 'afinn_score']]], axis=1)\ntest_features = pd.concat([test_tfidf_df, test[['vader_score', 'textblob_score', 'afinn_score']]], axis=1)\n\n# Initialize classifiers with optimized parameters\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, C=1.0),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n    \"Neural Network\": MLPClassifier(max_iter=1000, hidden_layer_sizes=(100,), alpha=0.0001),\n    \"Naive Bayes\": MultinomialNB()\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(train_features, train['sentiment'])\n    predictions = model.predict(test_features)\n    accuracy = accuracy_score(test['sentiment'], predictions)\n    print(f\"{name} Accuracy: {accuracy}\")\n    print(f\"{name} Classification Report:\")\n    print(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T13:42:59.717158Z","iopub.execute_input":"2024-02-17T13:42:59.719076Z","iopub.status.idle":"2024-02-17T14:22:22.903748Z","shell.execute_reply.started":"2024-02-17T13:42:59.719016Z","shell.execute_reply":"2024-02-17T14:22:22.901887Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.8919\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.89      0.89      0.89      9935\n    positive       0.89      0.90      0.89     10065\n\n    accuracy                           0.89     20000\n   macro avg       0.89      0.89      0.89     20000\nweighted avg       0.89      0.89      0.89     20000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Decision Tree Accuracy: 0.7758\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.79      0.74      0.77      9935\n    positive       0.76      0.81      0.78     10065\n\n    accuracy                           0.78     20000\n   macro avg       0.78      0.78      0.78     20000\nweighted avg       0.78      0.78      0.78     20000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Random Forest Accuracy: 0.83625\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.85      0.82      0.83      9935\n    positive       0.83      0.85      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Gradient Boosting Accuracy: 0.82955\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.83      0.83      0.83      9935\n    positive       0.83      0.83      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Neural Network Accuracy: 0.8751\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.87      0.87      0.87      9935\n    positive       0.88      0.88      0.88     10065\n\n    accuracy                           0.88     20000\n   macro avg       0.88      0.88      0.88     20000\nweighted avg       0.88      0.88      0.88     20000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Train and evaluate each model\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_features)\n\u001b[1;32m     67\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], predictions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:776\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    774\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:898\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 898\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1418\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1415\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n","\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"],"ename":"ValueError","evalue":"Negative values in data passed to MultinomialNB (input X)","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from afinn import Afinn\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nimport pandas as pd\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Initialize AFINN sentiment analyzer\nafinn = Afinn()\n\n# Initialize VADER sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Calculate sentiment scores using AFINN, VADER, and TextBlob\ntest['afinn_score'] = test['review'].apply(lambda x: afinn.score(x))\ntest['vader_score'] = test['review'].apply(lambda x: sid.polarity_scores(x)['compound'])\ntest['textblob_score'] = test['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n# Take the average of the sentiment scores\ntest['average_score'] = (test['afinn_score'] + test['vader_score'] + test['textblob_score']) / 3\n\n# Predict sentiment based on the average score\ntest['predicted_sentiment'] = test['average_score'].apply(lambda x: 'positive' if x >= 0 else 'negative')\n\n# Evaluate accuracy\naccuracy = (test['predicted_sentiment'] == test['sentiment']).mean()\nprint(f\"Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T13:20:56.354250Z","iopub.execute_input":"2024-02-17T13:20:56.354762Z","iopub.status.idle":"2024-02-17T13:25:51.958414Z","shell.execute_reply.started":"2024-02-17T13:20:56.354726Z","shell.execute_reply":"2024-02-17T13:25:51.957087Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.71565\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers\n!pip install torch","metadata":{"execution":{"iopub.status.busy":"2024-02-17T18:37:47.951975Z","iopub.execute_input":"2024-02-17T18:37:47.952991Z","iopub.status.idle":"2024-02-17T18:38:18.379569Z","shell.execute_reply.started":"2024-02-17T18:37:47.952951Z","shell.execute_reply":"2024-02-17T18:38:18.378335Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import XLNetTokenizer, XLNetForSequenceClassification\nimport torch\n\n# Sample dataset (replace with your own dataset)\ntrain = pd.read_csv('/kaggle/input/hybridapproach/train.csv/train.csv')\ntest = pd.read_csv('/kaggle/input/hybridapproach/test.csv/test.csv')\n\n# Tokenize the text data using XLNet tokenizer\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\ntrain_encodings = tokenizer(list(train['review']), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test['review']), truncation=True, padding=True)\n\n# Convert the tokenized inputs to PyTorch tensors\ntrain_inputs = torch.tensor(train_encodings['input_ids'])\nsentiment_mapping = {'positive': 1, 'negative': 0}  # Mapping for sentiment labels\ntrain_labels = torch.tensor(train['sentiment'].map(sentiment_mapping))\ntest_inputs = torch.tensor(test_encodings['input_ids'])\ntest_labels = torch.tensor(test['sentiment'].map(sentiment_mapping))\n\n# Load XLNetForSequenceClassification model\nmodel = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n\n# Train the model\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs, train_labels)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n\nmodel.train()\nfor epoch in range(3):  # You can adjust the number of epochs as needed\n    total_loss = 0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}\")\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(test_inputs, labels=test_labels)\n    predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n\n# Evaluate the model\naccuracy = accuracy_score(test['sentiment'], predictions)\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\")\nprint(classification_report(test['sentiment'], predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T19:18:01.090853Z","iopub.execute_input":"2024-02-17T19:18:01.091339Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]}]}