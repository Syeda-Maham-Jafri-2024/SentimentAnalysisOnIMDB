{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7568763,"sourceType":"datasetVersion","datasetId":4406363}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"--------------------------------  porter stemmer with pos + stopwords + tokenize _ tf-idf vectorizer -------------------------------","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use PorterStemmer for stemming\n    stemmer = PorterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    # Perform POS tagging\n    pos_tags = pos_tag(tokens)\n    tokens = [word + '_' + pos for word, pos in pos_tags]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'] )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-06T08:07:37.193001Z","iopub.execute_input":"2024-02-06T08:07:37.193449Z","iopub.status.idle":"2024-02-06T08:27:01.854447Z","shell.execute_reply.started":"2024-02-06T08:07:37.193416Z","shell.execute_reply":"2024-02-06T08:27:01.852823Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(nb_predictions)\nprint(\"/n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(rf_predictions)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T08:34:16.581551Z","iopub.execute_input":"2024-02-06T08:34:16.582971Z","iopub.status.idle":"2024-02-06T08:35:49.710239Z","shell.execute_reply.started":"2024-02-06T08:34:16.582917Z","shell.execute_reply":"2024-02-06T08:35:49.708687Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.8469\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.85      0.84      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n/n\nRandom Forest Accuracy: 0.83375\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.84      0.83      9935\n    positive       0.84      0.82      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n","output_type":"stream"}]},{"cell_type":"code","source":"# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"/n\")\n\n# Decision Tree\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"/n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-02-06T08:35:58.443658Z","iopub.execute_input":"2024-02-06T08:35:58.444073Z","iopub.status.idle":"2024-02-06T09:09:31.056318Z","shell.execute_reply.started":"2024-02-06T08:35:58.444043Z","shell.execute_reply":"2024-02-06T09:09:31.054523Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"k-NN Accuracy: 0.66965\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.65      0.74      0.69      9935\n    positive       0.70      0.60      0.64     10065\n\n    accuracy                           0.67     20000\n   macro avg       0.67      0.67      0.67     20000\nweighted avg       0.67      0.67      0.67     20000\n\n/n\nDecision Tree Accuracy: 0.69575\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.69      0.70      0.70      9935\n    positive       0.70      0.69      0.70     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.70      0.70      0.70     20000\nweighted avg       0.70      0.70      0.70     20000\n\n/n\nGradient Boosting Accuracy: 0.79635\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.75      0.78      9935\n    positive       0.77      0.84      0.81     10065\n\n    accuracy                           0.80     20000\n   macro avg       0.80      0.80      0.80     20000\nweighted avg       0.80      0.80      0.80     20000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"/n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:10:11.440816Z","iopub.execute_input":"2024-02-06T09:10:11.441354Z","iopub.status.idle":"2024-02-06T11:05:23.119913Z","shell.execute_reply.started":"2024-02-06T09:10:11.441318Z","shell.execute_reply":"2024-02-06T11:05:23.118031Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Stacking Accuracy: 0.8613\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.86      0.86      9935\n    positive       0.86      0.87      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n/n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Bagging\u001b[39;00m\n\u001b[1;32m     14\u001b[0m bagging_model \u001b[38;5;241m=\u001b[39m BaggingClassifier(base_estimator\u001b[38;5;241m=\u001b[39mDecisionTreeClassifier(), n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mbagging_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sentiments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m bagging_predictions \u001b[38;5;241m=\u001b[39m bagging_model\u001b[38;5;241m.\u001b[39mpredict(test_tfidf)\n\u001b[1;32m     17\u001b[0m bagging_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(test_sentiments[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], bagging_predictions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_bagging.py:337\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    330\u001b[0m     X,\n\u001b[1;32m    331\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_bagging.py:472\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    469\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[0;32m--> 472\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    491\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[1;32m    492\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_bagging.py:141\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[0;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[1;32m    138\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    140\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mestimator_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[indices][:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X[indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"\n# Neural Network\nnn_model = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:06:02.899173Z","iopub.execute_input":"2024-02-06T11:06:02.900492Z","iopub.status.idle":"2024-02-06T11:09:47.835229Z","shell.execute_reply.started":"2024-02-06T11:06:02.900452Z","shell.execute_reply":"2024-02-06T11:09:47.833762Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Neural Network Accuracy: 0.8495\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.85      0.85      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Bagging\nbagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"/n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:10:29.601526Z","iopub.execute_input":"2024-02-06T11:10:29.602039Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Bagging Accuracy: 0.7925\nClassification Report for Bagging:\n               precision    recall  f1-score   support\n\n    negative       0.79      0.79      0.79      9935\n    positive       0.79      0.79      0.79     10065\n\n    accuracy                           0.79     20000\n   macro avg       0.79      0.79      0.79     20000\nweighted avg       0.79      0.79      0.79     20000\n\n/n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------  porter stemmer with pos with count vectorizer -----------------------------","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use PorterStemmer for stemming\n    stemmer = PorterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    # Perform POS tagging\n    pos_tags = pos_tag(tokens)\n    tokens = [word + '_' + pos for word, pos in pos_tags]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=5000)\ntrain_tfidf = count_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = count_vectorizer.transform(test_sentiments['processed_reviews'])","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:54:49.297854Z","iopub.execute_input":"2024-02-07T17:54:49.298301Z","iopub.status.idle":"2024-02-07T18:13:46.374277Z","shell.execute_reply.started":"2024-02-07T17:54:49.298261Z","shell.execute_reply":"2024-02-07T18:13:46.373139Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(nb_predictions)\nprint(\"/n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(rf_predictions)\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"/n\")\n\n# Decision Tree\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"/n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\n\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"/n\")\n\n# Bagging\nbagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"/n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))\n\n\n# Neural Network\nnn_model = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T18:13:46.376055Z","iopub.execute_input":"2024-02-07T18:13:46.376396Z","iopub.status.idle":"2024-02-07T19:27:53.658587Z","shell.execute_reply.started":"2024-02-07T18:13:46.376369Z","shell.execute_reply":"2024-02-07T19:27:53.657433Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.83615\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.84      0.84      9935\n    positive       0.84      0.83      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n/n\nRandom Forest Accuracy: 0.8348\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.84      0.83      9935\n    positive       0.84      0.83      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\nk-NN Accuracy: 0.58895\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.57      0.66      0.62      9935\n    positive       0.61      0.52      0.56     10065\n\n    accuracy                           0.59     20000\n   macro avg       0.59      0.59      0.59     20000\nweighted avg       0.59      0.59      0.59     20000\n\n/n\nDecision Tree Accuracy: 0.69715\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.70      0.69      0.69      9935\n    positive       0.70      0.70      0.70     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.70      0.70      0.70     20000\nweighted avg       0.70      0.70      0.70     20000\n\n/n\nGradient Boosting Accuracy: 0.79505\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.82      0.75      0.78      9935\n    positive       0.77      0.84      0.81     10065\n\n    accuracy                           0.80     20000\n   macro avg       0.80      0.79      0.79     20000\nweighted avg       0.80      0.80      0.79     20000\n\nStacking Accuracy: 0.8534\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.85      0.85      9935\n    positive       0.85      0.86      0.86     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n/n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bagging Accuracy: 0.7902\nClassification Report for Bagging:\n               precision    recall  f1-score   support\n\n    negative       0.79      0.78      0.79      9935\n    positive       0.79      0.80      0.79     10065\n\n    accuracy                           0.79     20000\n   macro avg       0.79      0.79      0.79     20000\nweighted avg       0.79      0.79      0.79     20000\n\n/n\nVoting Classifier Accuracy: 0.82985\nClassification Report for Voting Classifier:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.82      0.83      9935\n    positive       0.83      0.84      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\nNeural Network Accuracy: 0.85705\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.85      0.86      9935\n    positive       0.85      0.86      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"----------------- PORTER STEMMER WITHOUT POS _ COUNT VECTORIZER WITH ML MODELS","metadata":{}},{"cell_type":"markdown","source":"------------------ LANCASTER STEMMER WITH POS _ TF-IDF VECTORIZATION WITH ML MODELS -------------------------------","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer \nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use LancasterStemmer for stemming\n    stemmer = LancasterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    # Perform POS tagging\n    pos_tags = pos_tag(tokens)\n    tokens = [word + '_' + pos for word, pos in pos_tags]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'] )","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:02:03.793337Z","iopub.execute_input":"2024-02-06T13:02:03.793679Z","iopub.status.idle":"2024-02-06T13:13:21.004112Z","shell.execute_reply.started":"2024-02-06T13:02:03.793656Z","shell.execute_reply":"2024-02-06T13:13:21.002800Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(nb_predictions)\nprint(\"/n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(rf_predictions)\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"/n\")\n\n# Decision Tree\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"/n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\n\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"/n\")\n\n# Bagging\nbagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"/n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))\n\n\n# Neural Network\nnn_model = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:16:16.771388Z","iopub.execute_input":"2024-02-06T13:16:16.771740Z","iopub.status.idle":"2024-02-06T16:18:19.541840Z","shell.execute_reply.started":"2024-02-06T13:16:16.771713Z","shell.execute_reply":"2024-02-06T16:18:19.540675Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.845\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.84      0.84      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n/n\nRandom Forest Accuracy: 0.83445\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.84      0.83      9935\n    positive       0.84      0.83      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n['negative' 'positive' 'negative' ... 'negative' 'positive' 'negative']\nk-NN Accuracy: 0.67225\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.64      0.77      0.70      9935\n    positive       0.72      0.57      0.64     10065\n\n    accuracy                           0.67     20000\n   macro avg       0.68      0.67      0.67     20000\nweighted avg       0.68      0.67      0.67     20000\n\n/n\nDecision Tree Accuracy: 0.6931\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.69      0.70      0.69      9935\n    positive       0.70      0.69      0.69     10065\n\n    accuracy                           0.69     20000\n   macro avg       0.69      0.69      0.69     20000\nweighted avg       0.69      0.69      0.69     20000\n\n/n\nGradient Boosting Accuracy: 0.79635\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.82      0.75      0.79      9935\n    positive       0.77      0.84      0.81     10065\n\n    accuracy                           0.80     20000\n   macro avg       0.80      0.80      0.80     20000\nweighted avg       0.80      0.80      0.80     20000\n\nStacking Accuracy: 0.8602\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.87      0.85      0.86      9935\n    positive       0.86      0.87      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n/n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bagging Accuracy: 0.79065\nClassification Report for Bagging:\n               precision    recall  f1-score   support\n\n    negative       0.79      0.78      0.79      9935\n    positive       0.79      0.80      0.79     10065\n\n    accuracy                           0.79     20000\n   macro avg       0.79      0.79      0.79     20000\nweighted avg       0.79      0.79      0.79     20000\n\n/n\nVoting Classifier Accuracy: 0.8408\nClassification Report for Voting Classifier:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.84      0.84      9935\n    positive       0.84      0.84      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\nNeural Network Accuracy: 0.84925\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.85      0.84      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---------------------------------- LANCASTER STEMMER WITHOUT POS _ COUNT VECTORIZER ---------------------------------","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer \nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use LancasterStemmer for stemming\n    stemmer = LancasterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=5000)\ntrain_tfidf = count_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = count_vectorizer.transform(test_sentiments['processed_reviews'])\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:10:50.457962Z","iopub.execute_input":"2024-02-07T14:10:50.458468Z","iopub.status.idle":"2024-02-07T14:14:44.105664Z","shell.execute_reply.started":"2024-02-07T14:10:50.458432Z","shell.execute_reply":"2024-02-07T14:14:44.104657Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(nb_predictions)\nprint(\"/n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(rf_predictions)\n\n\n# Neural Network\nnn_model = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\n\n# Decision Tree\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"/n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"/n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"/n\")\n\n# Bagging\nbagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"/n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:14:44.107560Z","iopub.execute_input":"2024-02-07T14:14:44.107895Z","iopub.status.idle":"2024-02-07T15:29:26.880665Z","shell.execute_reply.started":"2024-02-07T14:14:44.107867Z","shell.execute_reply":"2024-02-07T15:29:26.877454Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.8427\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.85      0.84      9935\n    positive       0.85      0.83      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n/n\nRandom Forest Accuracy: 0.8427\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.85      0.84      9935\n    positive       0.85      0.84      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n['negative' 'positive' 'negative' ... 'negative' 'positive' 'negative']\nNeural Network Accuracy: 0.86675\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.86      9935\n    positive       0.86      0.88      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\nDecision Tree Accuracy: 0.723\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.72      0.72      0.72      9935\n    positive       0.73      0.72      0.72     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\n/n\nGradient Boosting Accuracy: 0.8084\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.76      0.80      9935\n    positive       0.79      0.85      0.82     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\nk-NN Accuracy: 0.62795\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.61      0.71      0.65      9935\n    positive       0.66      0.55      0.60     10065\n\n    accuracy                           0.63     20000\n   macro avg       0.63      0.63      0.63     20000\nweighted avg       0.63      0.63      0.63     20000\n\n/n\nStacking Accuracy: 0.8634\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.86      0.86      9935\n    positive       0.87      0.86      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n/n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bagging Accuracy: 0.80275\nClassification Report for Bagging:\n               precision    recall  f1-score   support\n\n    negative       0.81      0.79      0.80      9935\n    positive       0.80      0.82      0.81     10065\n\n    accuracy                           0.80     20000\n   macro avg       0.80      0.80      0.80     20000\nweighted avg       0.80      0.80      0.80     20000\n\n/n\nVoting Classifier Accuracy: 0.84365\nClassification Report for Voting Classifier:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.84      0.84      9935\n    positive       0.84      0.85      0.85     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---------------------------------- LANCASTER STEMMER WITHOUT POS _ COUNT VECTORIZER with modified parameters of ml models ---------------------------------\n","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer \nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use LancasterStemmer for stemming\n    stemmer = LancasterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=5000)\ntrain_tfidf = count_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = count_vectorizer.transform(test_sentiments['processed_reviews'])\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:46:47.996436Z","iopub.execute_input":"2024-02-11T06:46:47.996792Z","iopub.status.idle":"2024-02-11T06:50:04.406654Z","shell.execute_reply.started":"2024-02-11T06:46:47.996762Z","shell.execute_reply":"2024-02-11T06:50:04.405220Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n\n# Naive Bayes\nnb_model = MultinomialNB(alpha=0.1) \nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(\"\\n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42) \nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(\"\\n\")\n\n# Neural Network\nnn_model =  MLPClassifier(hidden_layer_sizes=(128,), max_iter=200, alpha=0.0001, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network MLP CLASSIFIER Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\nprint(\"\\n\")\n\n# Decision Tree\ndt_model =  DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"\\n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\nprint(\"\\n\")\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"\\n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"\\n\")\n\n# Bagging\nbagging_model = BaggingClassifier(estimator= ExtraTreesClassifier(), n_estimators=200, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"\\n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model), ('st', stacking_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-02-11T06:50:04.411498Z","iopub.execute_input":"2024-02-11T06:50:04.411925Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-02-11 06:50:06.656994: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-11 06:50:06.657186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-11 06:50:06.854562: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Naive Bayes Accuracy: 0.84295\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.85      0.84      9935\n    positive       0.85      0.83      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n\n\nRandom Forest Accuracy: 0.8277\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.78      0.82      9935\n    positive       0.80      0.87      0.84     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n\n\nNeural Network MLP CLASSIFIER Accuracy: 0.8717\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.88      0.86      0.87      9935\n    positive       0.87      0.88      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n\n\nDecision Tree Accuracy: 0.736\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.78      0.65      0.71      9935\n    positive       0.70      0.82      0.76     10065\n\n    accuracy                           0.74     20000\n   macro avg       0.74      0.74      0.73     20000\nweighted avg       0.74      0.74      0.73     20000\n\n\n\nGradient Boosting Accuracy: 0.8446\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.83      0.84      9935\n    positive       0.83      0.86      0.85     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.85      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n\n\nk-NN Accuracy: 0.6297\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.59      0.80      0.68      9935\n    positive       0.70      0.46      0.55     10065\n\n    accuracy                           0.63     20000\n   macro avg       0.65      0.63      0.62     20000\nweighted avg       0.65      0.63      0.62     20000\n\n\n\nStacking Accuracy: 0.869\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.87      0.87      0.87      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"LANCASTER STEMMER WITH POS WITH COUNT VECTORIZER","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer \nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use LancasterStemmer for stemming\n    stemmer = LancasterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    # Perform POS tagging\n    pos_tags = pos_tag(tokens)\n    tokens = [word + '_' + pos for word, pos in pos_tags]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=5000)\ntrain_tfidf = count_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = count_vectorizer.transform(test_sentiments['processed_reviews'])\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T15:33:14.309514Z","iopub.execute_input":"2024-02-07T15:33:14.310472Z","iopub.status.idle":"2024-02-07T15:46:38.292695Z","shell.execute_reply.started":"2024-02-07T15:33:14.310412Z","shell.execute_reply":"2024-02-07T15:46:38.291236Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(nb_predictions)\nprint(\"/n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(rf_predictions)\n\n\n# Neural Network\nnn_model = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\n\n# Decision Tree\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"/n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"/n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"/n\")\n\n# Bagging\nbagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"/n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T15:46:38.295162Z","iopub.execute_input":"2024-02-07T15:46:38.295769Z","iopub.status.idle":"2024-02-07T17:04:29.037385Z","shell.execute_reply.started":"2024-02-07T15:46:38.295724Z","shell.execute_reply":"2024-02-07T17:04:29.034790Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.8342\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.84      0.83      9935\n    positive       0.84      0.83      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n/n\nRandom Forest Accuracy: 0.8292\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.83      0.83      9935\n    positive       0.83      0.83      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n['negative' 'positive' 'negative' ... 'negative' 'positive' 'negative']\nNeural Network Accuracy: 0.85775\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.85      0.86      9935\n    positive       0.85      0.87      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\nDecision Tree Accuracy: 0.6954\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.69      0.70      0.69      9935\n    positive       0.70      0.69      0.70     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.70      0.70      0.70     20000\nweighted avg       0.70      0.70      0.70     20000\n\n/n\nGradient Boosting Accuracy: 0.79555\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.82      0.75      0.78      9935\n    positive       0.77      0.84      0.81     10065\n\n    accuracy                           0.80     20000\n   macro avg       0.80      0.80      0.79     20000\nweighted avg       0.80      0.80      0.80     20000\n\nk-NN Accuracy: 0.5851\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.57      0.66      0.61      9935\n    positive       0.60      0.51      0.55     10065\n\n    accuracy                           0.59     20000\n   macro avg       0.59      0.59      0.58     20000\nweighted avg       0.59      0.59      0.58     20000\n\n/n\nStacking Accuracy: 0.8516\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.84      0.85      9935\n    positive       0.85      0.86      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n/n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bagging Accuracy: 0.79035\nClassification Report for Bagging:\n               precision    recall  f1-score   support\n\n    negative       0.79      0.78      0.79      9935\n    positive       0.79      0.80      0.79     10065\n\n    accuracy                           0.79     20000\n   macro avg       0.79      0.79      0.79     20000\nweighted avg       0.79      0.79      0.79     20000\n\n/n\nVoting Classifier Accuracy: 0.82985\nClassification Report for Voting Classifier:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.82      0.83      9935\n    positive       0.83      0.84      0.83     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------ SNOWBALL STEMMER WITHOUT POS - TF-IDF VECTORIZATION WITH ML MODELS -------------------------------","metadata":{}},{"cell_type":"code","source":"from nltk.stem import SnowballStemmer \ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_sreview(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use SnowballStemmer for stemming\n    stemmer = SnowballStemmer(\"english\")\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_sreview)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_sreview)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'] )","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:59:43.542355Z","iopub.execute_input":"2024-02-06T16:59:43.542750Z","iopub.status.idle":"2024-02-06T17:02:47.084421Z","shell.execute_reply.started":"2024-02-06T16:59:43.542719Z","shell.execute_reply":"2024-02-06T17:02:47.083042Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(nb_predictions)\nprint(\"/n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(rf_predictions)\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"/n\")\n\n# Decision Tree\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"/n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:02:47.086563Z","iopub.execute_input":"2024-02-06T17:02:47.086922Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.85145\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.85      0.85      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n/n\nRandom Forest Accuracy: 0.8484\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.86      0.85      9935\n    positive       0.86      0.84      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n['negative' 'positive' 'negative' ... 'negative' 'positive' 'negative']\nk-NN Accuracy: 0.74325\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.76      0.71      0.73      9935\n    positive       0.73      0.77      0.75     10065\n\n    accuracy                           0.74     20000\n   macro avg       0.74      0.74      0.74     20000\nweighted avg       0.74      0.74      0.74     20000\n\n/n\nDecision Tree Accuracy: 0.72035\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.72      0.72      0.72      9935\n    positive       0.72      0.72      0.72     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\n/n\nGradient Boosting Accuracy: 0.81575\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.77      0.81      9935\n    positive       0.79      0.86      0.82     10065\n\n    accuracy                           0.82     20000\n   macro avg       0.82      0.82      0.82     20000\nweighted avg       0.82      0.82      0.82     20000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.stem import SnowballStemmer \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer \nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_sreview(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use SnowballStemmer for stemming\n    stemmer = SnowballStemmer(\"english\")\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_sreview)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_sreview)\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'] )\n\n# Neural Network\nnn_model = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\n\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\n# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"/n\")\n\n# Bagging\nbagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"/n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T10:50:41.122086Z","iopub.execute_input":"2024-02-07T10:50:41.122483Z","iopub.status.idle":"2024-02-07T14:04:31.501307Z","shell.execute_reply.started":"2024-02-07T10:50:41.122452Z","shell.execute_reply":"2024-02-07T14:04:31.499575Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nNeural Network Accuracy: 0.8579\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.85      0.86      9935\n    positive       0.86      0.86      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\nStacking Accuracy: 0.87435\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.87      0.87      0.87      9935\n    positive       0.88      0.88      0.88     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n/n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bagging Accuracy: 0.8092\nClassification Report for Bagging:\n               precision    recall  f1-score   support\n\n    negative       0.81      0.81      0.81      9935\n    positive       0.81      0.81      0.81     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\n/n\nVoting Classifier Accuracy: 0.8566\nClassification Report for Voting Classifier:\n               precision    recall  f1-score   support\n\n    negative       0.87      0.84      0.85      9935\n    positive       0.85      0.87      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---------------------------------------- SPACY LEMMATIZER without POS _ TF-IDF VECTORIZER WITH ML MODELS --------------------------","metadata":{}},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download en_core_web_sm","metadata":{"execution":{"iopub.status.busy":"2024-02-08T09:29:17.759199Z","iopub.execute_input":"2024-02-08T09:29:17.760127Z","iopub.status.idle":"2024-02-08T09:29:57.422073Z","shell.execute_reply.started":"2024-02-08T09:29:17.760085Z","shell.execute_reply":"2024-02-08T09:29:57.420844Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\nCollecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nimport pandas as pd\nimport nltk\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the English language model in SpaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Function for lemmatization\ndef preprocess_lemreview(review):\n    # Tokenize the review\n    tokens = word_tokenize(review)\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Lemmatize using SpaCy\n    doc = nlp(\" \".join(tokens))\n    tokens = [token.lemma_ for token in doc]\n\n    return ' '.join(tokens)\n\n# Read the CSV files\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Apply preprocessing to the reviews\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_lemreview)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_lemreview)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'])","metadata":{"execution":{"iopub.status.busy":"2024-02-08T09:31:26.971608Z","iopub.execute_input":"2024-02-08T09:31:26.972118Z","iopub.status.idle":"2024-02-08T10:05:23.718289Z","shell.execute_reply.started":"2024-02-08T09:31:26.972079Z","shell.execute_reply":"2024-02-08T10:05:23.717081Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(\"\\n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(\"\\n\")\n\n# Neural Network\nnn_model = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network MLP CLASSIFIER Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\nprint(\"\\n\")\n\n# Decision Tree\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"\\n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\nprint(\"\\n\")\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"\\n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"\\n\")\n\n# Bagging\nbagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"\\n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-02-08T10:05:23.720299Z","iopub.execute_input":"2024-02-08T10:05:23.720726Z","iopub.status.idle":"2024-02-08T13:40:20.638968Z","shell.execute_reply.started":"2024-02-08T10:05:23.720687Z","shell.execute_reply":"2024-02-08T13:40:20.637401Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-02-08 10:05:25.838282: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-08 10:05:25.838569: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-08 10:05:25.987635: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Naive Bayes Accuracy: 0.85375\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.85      0.85      0.85      9935\n    positive       0.85      0.85      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n\n\nRandom Forest Accuracy: 0.8441\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.83      0.86      0.85      9935\n    positive       0.85      0.83      0.84     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.84      0.84      0.84     20000\nweighted avg       0.84      0.84      0.84     20000\n\n\n\nNeural Network MLP CLASSIFIER Accuracy: 0.858\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.86      0.86      9935\n    positive       0.86      0.86      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n\n\nDecision Tree Accuracy: 0.717\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.71      0.72      0.72      9935\n    positive       0.72      0.71      0.72     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\n\n\nGradient Boosting Accuracy: 0.81005\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.84      0.76      0.80      9935\n    positive       0.79      0.86      0.82     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\n\n\nk-NN Accuracy: 0.73815\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.75      0.70      0.73      9935\n    positive       0.72      0.77      0.75     10065\n\n    accuracy                           0.74     20000\n   macro avg       0.74      0.74      0.74     20000\nweighted avg       0.74      0.74      0.74     20000\n\n\n\nStacking Accuracy: 0.87125\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.87      0.87      0.87      9935\n    positive       0.87      0.87      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n\n\nBagging Accuracy: 0.80865\nClassification Report for Bagging:\n               precision    recall  f1-score   support\n\n    negative       0.81      0.81      0.81      9935\n    positive       0.81      0.81      0.81     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\n\n\nVoting Classifier Accuracy: 0.8546\nClassification Report for Voting Classifier:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.84      0.85      9935\n    positive       0.85      0.87      0.86     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"spacy lemmatizer without pos _ tf -idf vectorizer with modified parameters of ML modela","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nimport pandas as pd\nimport nltk\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the English language model in SpaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Function for lemmatization\ndef preprocess_lemreview(review):\n    # Tokenize the review\n    tokens = word_tokenize(review)\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Lemmatize using SpaCy\n    doc = nlp(\" \".join(tokens))\n    tokens = [token.lemma_ for token in doc]\n\n    return ' '.join(tokens)\n\n# Read the CSV files\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Apply preprocessing to the reviews\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_lemreview)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_lemreview)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'])","metadata":{"execution":{"iopub.status.busy":"2024-02-08T16:19:23.742150Z","iopub.execute_input":"2024-02-08T16:19:23.742608Z","iopub.status.idle":"2024-02-08T16:47:04.505172Z","shell.execute_reply.started":"2024-02-08T16:19:23.742567Z","shell.execute_reply":"2024-02-08T16:47:04.503819Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n\n# Naive Bayes\nnb_model = MultinomialNB(alpha=0.1) \nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(\"\\n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42) \nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(\"\\n\")\n\n# Neural Network\nnn_model =  MLPClassifier(hidden_layer_sizes=(128,), max_iter=200, alpha=0.0001, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network MLP CLASSIFIER Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\nprint(\"\\n\")\n\n# Decision Tree\ndt_model =  DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"\\n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\nprint(\"\\n\")\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"\\n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"\\n\")\n\n# Bagging\nbagging_model = BaggingClassifier(estimator= ExtraTreesClassifier(), n_estimators=200, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"\\n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model), ('st', stacking_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-02-08T16:55:20.158699Z","iopub.execute_input":"2024-02-08T16:55:20.159184Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.8535\nClassification Report for Naive Bayes:\n               precision    recall  f1-score   support\n\n    negative       0.85      0.85      0.85      9935\n    positive       0.85      0.86      0.85     10065\n\n    accuracy                           0.85     20000\n   macro avg       0.85      0.85      0.85     20000\nweighted avg       0.85      0.85      0.85     20000\n\n\n\nRandom Forest Accuracy: 0.82955\nClassification Report for Random Forest:\n               precision    recall  f1-score   support\n\n    negative       0.85      0.80      0.82      9935\n    positive       0.81      0.86      0.84     10065\n\n    accuracy                           0.83     20000\n   macro avg       0.83      0.83      0.83     20000\nweighted avg       0.83      0.83      0.83     20000\n\n\n\nNeural Network MLP CLASSIFIER Accuracy: 0.8603\nClassification Report for Neural Network:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.86      0.86      9935\n    positive       0.86      0.86      0.86     10065\n\n    accuracy                           0.86     20000\n   macro avg       0.86      0.86      0.86     20000\nweighted avg       0.86      0.86      0.86     20000\n\n\n\nDecision Tree Accuracy: 0.7237\nClassification Report for Decision Tree:\n               precision    recall  f1-score   support\n\n    negative       0.72      0.73      0.72      9935\n    positive       0.73      0.72      0.72     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\n\n\nGradient Boosting Accuracy: 0.84485\nClassification Report for Gradient Boosting:\n               precision    recall  f1-score   support\n\n    negative       0.86      0.82      0.84      9935\n    positive       0.83      0.87      0.85     10065\n\n    accuracy                           0.84     20000\n   macro avg       0.85      0.84      0.84     20000\nweighted avg       0.85      0.84      0.84     20000\n\n\n\nk-NN Accuracy: 0.7522\nClassification Report for k-NN:\n               precision    recall  f1-score   support\n\n    negative       0.73      0.80      0.76      9935\n    positive       0.78      0.71      0.74     10065\n\n    accuracy                           0.75     20000\n   macro avg       0.75      0.75      0.75     20000\nweighted avg       0.75      0.75      0.75     20000\n\n\n\nStacking Accuracy: 0.8773\nClassification Report for Stacking:\n               precision    recall  f1-score   support\n\n    negative       0.88      0.88      0.88      9935\n    positive       0.88      0.88      0.88     10065\n\n    accuracy                           0.88     20000\n   macro avg       0.88      0.88      0.88     20000\nweighted avg       0.88      0.88      0.88     20000\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"porter stemmer with POS, tf-idf vectorizer, normalization (Min Max), improved parameters for ML model","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use PorterStemmer for stemming\n    stemmer = PorterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    # Perform POS tagging\n    pos_tags = pos_tag(tokens)\n    tokens = [word + '_' + pos for word, pos in pos_tags]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'] )\n\n# Perform Min-Max scaling for normalization\nscaler = MinMaxScaler()\ntrain_tfidf = scaler.fit_transform(train_tfidf.toarray())\ntest_tfidf = scaler.transform(test_tfidf.toarray())\n\n# You can use train_tfidf_normalized and test_tfidf_normalized for further processing\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:14:10.929423Z","iopub.status.idle":"2024-02-11T11:14:10.930148Z","shell.execute_reply.started":"2024-02-11T11:14:10.929974Z","shell.execute_reply":"2024-02-11T11:14:10.929990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n\n# Naive Bayes\nnb_model = MultinomialNB(alpha=0.1) \nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(\"\\n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42) \nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(\"\\n\")\n\n# Neural Network\nnn_model =  MLPClassifier(hidden_layer_sizes=(128,), max_iter=200, alpha=0.0001, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network MLP CLASSIFIER Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\nprint(\"\\n\")\n\n# Decision Tree\ndt_model =  DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"\\n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\nprint(\"\\n\")\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"\\n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"\\n\")\n\n# Bagging\nbagging_model = BaggingClassifier(estimator= ExtraTreesClassifier(), n_estimators=200, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"\\n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model), ('st', stacking_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:14:10.930864Z","iopub.status.idle":"2024-02-11T11:14:10.931370Z","shell.execute_reply.started":"2024-02-11T11:14:10.931164Z","shell.execute_reply":"2024-02-11T11:14:10.931178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"porter stemmer with POS, tf-idf vectorizer, normalization (Standard), improved parameters for ML model","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use PorterStemmer for stemming\n    stemmer = PorterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    # Perform POS tagging\n    pos_tags = pos_tag(tokens)\n    tokens = [word + '_' + pos for word, pos in pos_tags]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'] )\n\n# Perform standardization for normalization\nscaler = StandardScaler()\ntrain_tfidf= scaler.fit_transform(train_tfidf.toarray())\ntest_tfidf = scaler.transform(test_tfidf.toarray())\n\n# You can use train_tfidf_standardized and test_tfidf_standardized for further processing\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n\n# Naive Bayes\nnb_model = MultinomialNB(alpha=0.1) \nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(\"\\n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42) \nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(\"\\n\")\n\n# Neural Network\nnn_model =  MLPClassifier(hidden_layer_sizes=(128,), max_iter=200, alpha=0.0001, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network MLP CLASSIFIER Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\nprint(\"\\n\")\n\n# Decision Tree\ndt_model =  DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"\\n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\nprint(\"\\n\")\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"\\n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"\\n\")\n\n# Bagging\nbagging_model = BaggingClassifier(estimator= ExtraTreesClassifier(), n_estimators=200, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"\\n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model), ('st', stacking_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"porter stemmer with POS, tf-idf vectorizer, normalization (robust), improved parameters for ML model","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import RobustScaler\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\ntrain_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/train/train.csv\")\ntest_sentiments = pd.read_csv(\"/kaggle/input/sent-analysis/test/test.csv\")\n\n# Text preprocessing function with stemming and POS tagging\ndef preprocess_review(review):\n    tokens = word_tokenize(review)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Use PorterStemmer for stemming\n    stemmer = PorterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    # Perform POS tagging\n    pos_tags = pos_tag(tokens)\n    tokens = [word + '_' + pos for word, pos in pos_tags]\n\n    return ' '.join(tokens)\n\ntrain_sentiments['processed_reviews'] = train_sentiments['review'].apply(preprocess_review)\ntest_sentiments['processed_reviews'] = test_sentiments['review'].apply(preprocess_review)\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_sentiments['processed_reviews'])\ntest_tfidf = tfidf_vectorizer.transform(test_sentiments['processed_reviews'] )\n\n# Perform robust scaling for normalization\nscaler = RobustScaler()\ntrain_tfidf = scaler.fit_transform(train_tfidf.toarray())\ntest_tfidf = scaler.transform(test_tfidf.toarray())\n\n# You can use train_tfidf_scaled and test_tfidf_scaled for further processing\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n\n# Naive Bayes\nnb_model = MultinomialNB(alpha=0.1) \nnb_model.fit(train_tfidf, train_sentiments['sentiment'])\nnb_predictions = nb_model.predict(test_tfidf)\nnb_accuracy = accuracy_score(test_sentiments['sentiment'], nb_predictions)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\nprint(\"Classification Report for Naive Bayes:\\n\", classification_report(test_sentiments['sentiment'], nb_predictions))\nprint(\"\\n\")\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42) \nrf_model.fit(train_tfidf, train_sentiments['sentiment'])\nrf_predictions = rf_model.predict(test_tfidf)\nrf_accuracy = accuracy_score(test_sentiments['sentiment'], rf_predictions)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"Classification Report for Random Forest:\\n\", classification_report(test_sentiments['sentiment'], rf_predictions))\nprint(\"\\n\")\n\n# Neural Network\nnn_model =  MLPClassifier(hidden_layer_sizes=(128,), max_iter=200, alpha=0.0001, random_state=42)\nnn_model.fit(train_tfidf, train_sentiments['sentiment'])\nnn_predictions = nn_model.predict(test_tfidf)\nnn_accuracy = accuracy_score(test_sentiments['sentiment'], nn_predictions)\nprint(\"Neural Network MLP CLASSIFIER Accuracy:\", nn_accuracy)\nprint(\"Classification Report for Neural Network:\\n\", classification_report(test_sentiments['sentiment'], nn_predictions))\nprint(\"\\n\")\n\n# Decision Tree\ndt_model =  DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)\ndt_model.fit(train_tfidf, train_sentiments['sentiment'])\ndt_predictions = dt_model.predict(test_tfidf)\ndt_accuracy = accuracy_score(test_sentiments['sentiment'], dt_predictions)\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Classification Report for Decision Tree:\\n\", classification_report(test_sentiments['sentiment'], dt_predictions))\nprint(\"\\n\")\n\n# Gradient Boosting\ngb_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\ngb_model.fit(train_tfidf, train_sentiments['sentiment'])\ngb_predictions = gb_model.predict(test_tfidf)\ngb_accuracy = accuracy_score(test_sentiments['sentiment'], gb_predictions)\nprint(\"Gradient Boosting Accuracy:\", gb_accuracy)\nprint(\"Classification Report for Gradient Boosting:\\n\", classification_report(test_sentiments['sentiment'], gb_predictions))\nprint(\"\\n\")\n\n# k-NN\nknn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(train_tfidf, train_sentiments['sentiment'])\nknn_predictions = knn_model.predict(test_tfidf)\nknn_accuracy = accuracy_score(test_sentiments['sentiment'], knn_predictions)\nprint(\"k-NN Accuracy:\", knn_accuracy)\nprint(\"Classification Report for k-NN:\\n\", classification_report(test_sentiments['sentiment'], knn_predictions))\nprint(\"\\n\")\n\n# Stacking\nstacking_model = StackingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model),('gb', gb_model)],\n    final_estimator=GradientBoostingClassifier(n_estimators=100, random_state=42)\n)\nstacking_model.fit(train_tfidf, train_sentiments['sentiment'])\nstacking_predictions = stacking_model.predict(test_tfidf)\nstacking_accuracy = accuracy_score(test_sentiments['sentiment'], stacking_predictions)\nprint(\"Stacking Accuracy:\", stacking_accuracy)\nprint(\"Classification Report for Stacking:\\n\", classification_report(test_sentiments['sentiment'], stacking_predictions))\nprint(\"\\n\")\n\n# Bagging\nbagging_model = BaggingClassifier(estimator= ExtraTreesClassifier(), n_estimators=200, random_state=42)\nbagging_model.fit(train_tfidf, train_sentiments['sentiment'])\nbagging_predictions = bagging_model.predict(test_tfidf)\nbagging_accuracy = accuracy_score(test_sentiments['sentiment'], bagging_predictions)\nprint(\"Bagging Accuracy:\", bagging_accuracy)\nprint(\"Classification Report for Bagging:\\n\", classification_report(test_sentiments['sentiment'], bagging_predictions))\nprint(\"\\n\")\n\n# Voting Classifier\nvoting_model = VotingClassifier(\n    estimators=[('nb', nb_model), ('rf', rf_model), ('knn', knn_model), ('dt', dt_model), ('gb', gb_model), ('st', stacking_model)],\n    voting='hard'\n)\nvoting_model.fit(train_tfidf, train_sentiments['sentiment'])\nvoting_predictions = voting_model.predict(test_tfidf)\nvoting_accuracy = accuracy_score(test_sentiments['sentiment'], voting_predictions)\nprint(\"Voting Classifier Accuracy:\", voting_accuracy)\nprint(\"Classification Report for Voting Classifier:\\n\", classification_report(test_sentiments['sentiment'], voting_predictions))","metadata":{},"execution_count":null,"outputs":[]}]}