{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7648732,"sourceType":"datasetVersion","datasetId":4458685},{"sourceId":7680062,"sourceType":"datasetVersion","datasetId":4480657}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:44:43.006484Z","iopub.execute_input":"2024-02-18T11:44:43.007035Z","iopub.status.idle":"2024-02-18T11:44:56.369674Z","shell.execute_reply.started":"2024-02-18T11:44:43.007005Z","shell.execute_reply":"2024-02-18T11:44:56.368325Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, RobertaTokenizer, RobertaForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\n\n# Load training dataset\ntrain_df = pd.read_csv(\"/kaggle/input/plmdataset/train.csv/train.csv\")  # Replace \"train_dataset.csv\" with the actual path to your training dataset file\n\n# Load testing dataset\ntest_df = pd.read_csv(\"/kaggle/input/plmdataset/test.csv/test.csv\")  # Replace \"test_dataset.csv\" with the actual path to your testing dataset file\n\n# Extract texts and labels from training dataset\ntrain_texts = train_df['review']\ntrain_labels = train_df['sentiment']\n\n# Extract texts and labels from testing dataset\ntest_texts = test_df['review']\ntest_labels = test_df['sentiment']\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to tokenize and encode the text\ndef tokenize_and_encode(tokenizer, texts, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n                            text,\n                            add_special_tokens = True,\n                            max_length = max_length,\n                            padding = 'max_length',\n                            truncation = True,\n                            return_attention_mask = True,\n                            return_tensors = 'pt'\n                       )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\n# Function to train the model\ndef train_model(model, train_dataloader, optimizer, criterion, num_epochs=3):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for batch in train_dataloader:\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_dataloader)}\")\n\n# Function to evaluate the model\ndef evaluate_model(model, test_dataloader):\n    model.eval()\n    predictions = []\n    true_labels = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].numpy()\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n            logits = outputs.logits\n            _, predicted = torch.max(logits, dim=1)\n\n            predictions.extend(predicted.cpu().numpy())\n            true_labels.extend(labels)\n\n    accuracy = accuracy_score(true_labels, predictions)\n    print(f\"Accuracy: {accuracy}\")\n\n# Tokenize and encode the texts\nmax_length = 128\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(distilbert_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(distilbert_tokenizer, test_texts, max_length)\n\n# Assuming the labels are strings ('positive' and 'negative'), you can convert them to integers\nlabel_map = {'positive': 1, 'negative': 0}\ntrain_labels = train_labels.map(label_map)\ntest_labels = test_labels.map(label_map)\n\n# Convert labels to tensor\ntrain_labels = torch.tensor(train_labels.values, dtype=torch.long)\ntest_labels = torch.tensor(test_labels.values, dtype=torch.long)\n\n# Create dataloaders\nbatch_size = 16\ntrain_data = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntrain_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\ntest_data = torch.utils.data.TensorDataset(test_input_ids, test_attention_masks, test_labels)\ntest_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n\n# Fine-tune DistilBERT\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\ndistilbert_model.to(device)\noptimizer = AdamW(distilbert_model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\ntrain_model(distilbert_model, train_dataloader, optimizer, criterion)\nevaluate_model(distilbert_model, test_dataloader)\n\n# Repeat the same process for RoBERTa and GPT-2\n# For RoBERTa\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(roberta_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(roberta_tokenizer, test_texts, max_length)\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\nroberta_model.to(device)\noptimizer = AdamW(roberta_model.parameters(), lr=2e-5)\ntrain_model(roberta_model, train_dataloader, optimizer, criterion)\nevaluate_model(roberta_model, test_dataloader)\n\n# For GPT-2\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(gpt2_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(gpt2_tokenizer, test_texts, max_length)\ngpt2_model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\ngpt2_model.to(device)\noptimizer = AdamW(gpt2_model.parameters(), lr=2e-5)\ntrain_model(gpt2_model, train_dataloader, optimizer, criterion)\nevaluate_model(gpt2_model, test_dataloader)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-18T12:06:16.378857Z","iopub.execute_input":"2024-02-18T12:06:16.379602Z","iopub.status.idle":"2024-02-18T13:08:30.087235Z","shell.execute_reply.started":"2024-02-18T12:06:16.379557Z","shell.execute_reply":"2024-02-18T13:08:30.085788Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f79fd81a442b4afb84c47b3028de17bb"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.3393317736208439\nEpoch 2/3 - Loss: 0.20182555637632807\nEpoch 3/3 - Loss: 0.11509952055377265\nAccuracy: 0.85695\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c967f6ae054422805c30ff48e148a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"930403e57b2a48beac32e3c8f08c92ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e55c87f2fb48f6b680a0ed3264b8c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1665b360d88d437d8d80d28cd65e9fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3413453c88844779b71277a3dc9f7ad5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.6953605392773946\nEpoch 2/3 - Loss: 0.6947482631047567\nEpoch 3/3 - Loss: 0.6946594745000203\nAccuracy: 0.49675\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a08796202d644038901debc92d3db6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf392708c81142919b89e8626e73becc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a78ef98bbe35438997e9e9180f0f0fca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6d0e118db0461aa9505306c1a03cde"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# For GPT-2\u001b[39;00m\n\u001b[1;32m    133\u001b[0m gpt2_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m train_input_ids, train_attention_masks \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt2_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m test_input_ids, test_attention_masks \u001b[38;5;241m=\u001b[39m tokenize_and_encode(gpt2_tokenizer, test_texts, max_length)\n\u001b[1;32m    136\u001b[0m gpt2_model \u001b[38;5;241m=\u001b[39m GPT2ForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n","Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mtokenize_and_encode\u001b[0;34m(tokenizer, texts, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m---> 31\u001b[0m     encoded_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(encoded_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     41\u001b[0m     attention_masks\u001b[38;5;241m.\u001b[39mappend(encoded_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2973\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   2954\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2969\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 2973\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   2983\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2984\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3000\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3001\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2708\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2706\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2708\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2710\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2711\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2712\u001b[0m     )\n\u001b[1;32m   2714\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2716\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2717\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2720\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2721\u001b[0m ):\n","\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."],"ename":"ValueError","evalue":"Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, RobertaTokenizer, RobertaForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Load training dataset\ntrain_df = pd.read_csv(\"/kaggle/input/plmdataset/train.csv/train.csv\")  # Replace \"train_dataset.csv\" with the actual path to your training dataset file\n\n# Load testing dataset\ntest_df = pd.read_csv(\"/kaggle/input/plmdataset/test.csv/test.csv\")  # Replace \"test_dataset.csv\" with the actual path to your testing dataset file\n\n# Extract texts and labels from training dataset\ntrain_texts = train_df['review']\ntrain_labels = train_df['sentiment']\n\n# Extract texts and labels from testing dataset\ntest_texts = test_df['review']\ntest_labels = test_df['sentiment']\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to tokenize and encode the text\ndef tokenize_and_encode(tokenizer, texts, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n                            text,\n                            add_special_tokens=True,\n                            max_length=max_length,\n                            padding='max_length',\n                            truncation=True,\n                            return_attention_mask=True,\n                            return_tensors='pt'\n                       )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\n# Function to train the model with mixed precision\ndef train_model_with_mixed_precision(model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, batch in enumerate(train_dataloader):\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            optimizer.zero_grad()\n\n            with autocast():  # Use mixed precision\n                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n                loss = outputs.loss\n\n            scaler.scale(loss).backward()  # Scale loss to prevent overflow\n            scaler.unscale_(optimizer)  # Unscales the gradients of optimizer's assigned params in-place\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Clip gradients to prevent explosion\n            scaler.step(optimizer)  # Take a step using the optimizer\n            scaler.update()  # Updates the scale for next iteration\n\n            scheduler.step()  # Update learning rate scheduler\n\n            running_loss += loss.item()\n\n            if i % 100 == 99:  # Print every 100 mini-batches\n                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {running_loss/100}\")\n                running_loss = 0.0\n\n# Tokenize and encode the texts\nmax_length = 128\n\n# DistilBERT\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(distilbert_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(distilbert_tokenizer, test_texts, max_length)\n\n# Assuming the labels are strings ('positive' and 'negative'), you can convert them to integers\nlabel_map = {'positive': 1, 'negative': 0}\ntrain_labels = train_labels.map(label_map)\ntest_labels = test_labels.map(label_map)\n\n# Convert labels to tensor\ntrain_labels = torch.tensor(train_labels.values, dtype=torch.long)\ntest_labels = torch.tensor(test_labels.values, dtype=torch.long)\n\n# Create dataloaders\nbatch_size = 16\ntrain_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\ntest_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n# Define hyperparameters\nnum_epochs = 3\nmax_grad_norm = 1.0\nwarmup_steps = 0.1 * len(train_dataloader)  # 10% of total training steps\n\n# Create scheduler\noptimizer = AdamW(distilbert_model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\n\n# Define scaler for mixed precision training\nscaler = GradScaler()\n\n# Fine-tune DistilBERT with mixed precision\ntrain_model_with_mixed_precision(distilbert_model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs)\nevaluate_model(distilbert_model, test_dataloader)\n\n# Repeat the same process for RoBERTa and GPT-2\n# For RoBERTa\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(roberta_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(roberta_tokenizer, test_texts, max_length)\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\nroberta_model.to(device)\noptimizer = AdamW(roberta_model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\ntrain_model_with_mixed_precision(roberta_model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs)\nevaluate_model(roberta_model, test_dataloader)\n\n# # For GPT-2\n# gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n# train_input_ids, train_attention_masks = tokenize_and_encode(gpt2_tokenizer, train_texts, max_length)\n# test_input_ids, test_attention_masks = tokenize_and_encode(gpt2_tokenizer, test_texts, max_length)\n# gpt2_model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n# gpt2_model.to(device)\n# optimizer = AdamW(gpt2_model.parameters(), lr=2e-5)\n# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\n# train_model_with_mixed_precision(gpt2_model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs)\n# evaluate_model(gpt2_model, test_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T13:57:50.225653Z","iopub.execute_input":"2024-02-18T13:57:50.226035Z","iopub.status.idle":"2024-02-18T14:31:15.614269Z","shell.execute_reply.started":"2024-02-18T13:57:50.226004Z","shell.execute_reply":"2024-02-18T14:31:15.613284Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.10172989226877689\nEpoch 1/3, Batch 200/1875, Loss: 0.0676270891726017\nEpoch 1/3, Batch 300/1875, Loss: 0.06075821131467819\nEpoch 1/3, Batch 400/1875, Loss: 0.21413344100117684\nEpoch 1/3, Batch 500/1875, Loss: 0.07729954890906811\nEpoch 1/3, Batch 600/1875, Loss: 0.07085053972899914\nEpoch 1/3, Batch 700/1875, Loss: 0.0842372076958418\nEpoch 1/3, Batch 800/1875, Loss: 0.06937024764716625\nEpoch 1/3, Batch 900/1875, Loss: 0.12046432785689831\nEpoch 1/3, Batch 1000/1875, Loss: 0.06073136806488037\nEpoch 1/3, Batch 1100/1875, Loss: 0.06664066761732101\nEpoch 1/3, Batch 1200/1875, Loss: 0.07201315812766552\nEpoch 1/3, Batch 1300/1875, Loss: 0.08882725395262242\nEpoch 1/3, Batch 1400/1875, Loss: 0.11559346415102482\nEpoch 1/3, Batch 1500/1875, Loss: 0.07386761747300624\nEpoch 1/3, Batch 1600/1875, Loss: 0.07036987073719501\nEpoch 1/3, Batch 1700/1875, Loss: 0.08533724144101143\nEpoch 1/3, Batch 1800/1875, Loss: 0.06804419979453087\nEpoch 2/3, Batch 100/1875, Loss: 0.08687880970537662\nEpoch 2/3, Batch 200/1875, Loss: 0.05804656870663166\nEpoch 2/3, Batch 300/1875, Loss: 0.050513043999671936\nEpoch 2/3, Batch 400/1875, Loss: 0.05912723183631897\nEpoch 2/3, Batch 500/1875, Loss: 0.029623319283127784\nEpoch 2/3, Batch 600/1875, Loss: 0.05026861391961575\nEpoch 2/3, Batch 700/1875, Loss: 0.05506345801055432\nEpoch 2/3, Batch 800/1875, Loss: 0.04726280592381954\nEpoch 2/3, Batch 900/1875, Loss: 0.04690815605223179\nEpoch 2/3, Batch 1000/1875, Loss: 0.024645171239972116\nEpoch 2/3, Batch 1100/1875, Loss: 0.035323310494422916\nEpoch 2/3, Batch 1200/1875, Loss: 0.028357860296964646\nEpoch 2/3, Batch 1300/1875, Loss: 0.035835225731134415\nEpoch 2/3, Batch 1400/1875, Loss: 0.04125104203820228\nEpoch 2/3, Batch 1500/1875, Loss: 0.03690122202038765\nEpoch 2/3, Batch 1600/1875, Loss: 0.02771525226533413\nEpoch 2/3, Batch 1700/1875, Loss: 0.01586309790611267\nEpoch 2/3, Batch 1800/1875, Loss: 0.034487926065921784\nEpoch 3/3, Batch 100/1875, Loss: 0.03584137693047523\nEpoch 3/3, Batch 200/1875, Loss: 0.008388096615672112\nEpoch 3/3, Batch 300/1875, Loss: 0.03233360193669796\nEpoch 3/3, Batch 400/1875, Loss: 0.014073731899261475\nEpoch 3/3, Batch 500/1875, Loss: 0.016193446293473242\nEpoch 3/3, Batch 600/1875, Loss: 0.01802444279193878\nEpoch 3/3, Batch 700/1875, Loss: 0.026710401698946954\nEpoch 3/3, Batch 800/1875, Loss: 0.02113825984299183\nEpoch 3/3, Batch 900/1875, Loss: 0.014393161982297897\nEpoch 3/3, Batch 1000/1875, Loss: 0.013431413099169731\nEpoch 3/3, Batch 1100/1875, Loss: 0.013786088451743126\nEpoch 3/3, Batch 1200/1875, Loss: 0.018559847176074982\nEpoch 3/3, Batch 1300/1875, Loss: 0.009650771319866181\nEpoch 3/3, Batch 1400/1875, Loss: 0.03376125022768974\nEpoch 3/3, Batch 1500/1875, Loss: 0.01339066356420517\nEpoch 3/3, Batch 1600/1875, Loss: 0.009927525743842125\nEpoch 3/3, Batch 1700/1875, Loss: 0.020796019285917282\nEpoch 3/3, Batch 1800/1875, Loss: 0.01604520469903946\nAccuracy: 0.8848\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.6939346313476562\nEpoch 1/3, Batch 200/1875, Loss: 0.7001318359375\nEpoch 1/3, Batch 300/1875, Loss: 0.6976708984375\nEpoch 1/3, Batch 400/1875, Loss: 0.6888579559326172\nEpoch 1/3, Batch 500/1875, Loss: 0.6961116790771484\nEpoch 1/3, Batch 600/1875, Loss: 0.6953240966796875\nEpoch 1/3, Batch 700/1875, Loss: 0.6958424377441407\nEpoch 1/3, Batch 800/1875, Loss: 0.6959896850585937\nEpoch 1/3, Batch 900/1875, Loss: 0.6935813903808594\nEpoch 1/3, Batch 1000/1875, Loss: 0.6943620300292969\nEpoch 1/3, Batch 1100/1875, Loss: 0.6938188171386719\nEpoch 1/3, Batch 1200/1875, Loss: 0.6974050903320312\nEpoch 1/3, Batch 1300/1875, Loss: 0.6944967651367188\nEpoch 1/3, Batch 1400/1875, Loss: 0.6878280639648438\nEpoch 1/3, Batch 1500/1875, Loss: 0.6747649765014648\nEpoch 1/3, Batch 1600/1875, Loss: 0.6593856048583985\nEpoch 1/3, Batch 1700/1875, Loss: 0.6286680984497071\nEpoch 1/3, Batch 1800/1875, Loss: 0.5979949188232422\nEpoch 2/3, Batch 100/1875, Loss: 0.5938976287841797\nEpoch 2/3, Batch 200/1875, Loss: 0.5936203193664551\nEpoch 2/3, Batch 300/1875, Loss: 0.5665141296386719\nEpoch 2/3, Batch 400/1875, Loss: 0.5586728858947754\nEpoch 2/3, Batch 500/1875, Loss: 0.5395234870910645\nEpoch 2/3, Batch 600/1875, Loss: 0.5487950325012207\nEpoch 2/3, Batch 700/1875, Loss: 0.5115027236938476\nEpoch 2/3, Batch 800/1875, Loss: 0.5339586687088013\nEpoch 2/3, Batch 900/1875, Loss: 0.5137767028808594\nEpoch 2/3, Batch 1000/1875, Loss: 0.5034670782089233\nEpoch 2/3, Batch 1100/1875, Loss: 0.483096604347229\nEpoch 2/3, Batch 1200/1875, Loss: 0.5047172737121582\nEpoch 2/3, Batch 1300/1875, Loss: 0.47947437286376954\nEpoch 2/3, Batch 1400/1875, Loss: 0.4923399066925049\nEpoch 2/3, Batch 1500/1875, Loss: 0.46708165168762206\nEpoch 2/3, Batch 1600/1875, Loss: 0.4586261510848999\nEpoch 2/3, Batch 1700/1875, Loss: 0.4527506160736084\nEpoch 2/3, Batch 1800/1875, Loss: 0.4514769458770752\nEpoch 3/3, Batch 100/1875, Loss: 0.4637337350845337\nEpoch 3/3, Batch 200/1875, Loss: 0.4725750160217285\nEpoch 3/3, Batch 300/1875, Loss: 0.4390309047698975\nEpoch 3/3, Batch 400/1875, Loss: 0.4431319189071655\nEpoch 3/3, Batch 500/1875, Loss: 0.44276777029037473\nEpoch 3/3, Batch 600/1875, Loss: 0.4567181968688965\nEpoch 3/3, Batch 700/1875, Loss: 0.4231038761138916\nEpoch 3/3, Batch 800/1875, Loss: 0.43161486148834227\nEpoch 3/3, Batch 900/1875, Loss: 0.427634220123291\nEpoch 3/3, Batch 1000/1875, Loss: 0.428695821762085\nEpoch 3/3, Batch 1100/1875, Loss: 0.4082297658920288\nEpoch 3/3, Batch 1200/1875, Loss: 0.42948600292205813\nEpoch 3/3, Batch 1300/1875, Loss: 0.42375107765197756\nEpoch 3/3, Batch 1400/1875, Loss: 0.43669298648834226\nEpoch 3/3, Batch 1500/1875, Loss: 0.3974007177352905\nEpoch 3/3, Batch 1600/1875, Loss: 0.3981157207489014\nEpoch 3/3, Batch 1700/1875, Loss: 0.39326844453811644\nEpoch 3/3, Batch 1800/1875, Loss: 0.3862108492851257\nAccuracy: 0.7854\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, RobertaTokenizer, RobertaForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Load training dataset\ntrain_df = pd.read_csv(\"/kaggle/input/plmdataset/train.csv/train.csv\")  # Replace \"train_dataset.csv\" with the actual path to your training dataset file\n\n# Load testing dataset\ntest_df = pd.read_csv(\"/kaggle/input/plmdataset/test.csv/test.csv\")  # Replace \"test_dataset.csv\" with the actual path to your testing dataset file\n\n# Extract texts and labels from training dataset\ntrain_texts = train_df['review']\ntrain_labels = train_df['sentiment']\n\n# Extract texts and labels from testing dataset\ntest_texts = test_df['review']\ntest_labels = test_df['sentiment']\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to tokenize and encode the text\ndef tokenize_and_encode(tokenizer, texts, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n                            text,\n                            add_special_tokens=True,\n                            max_length=max_length,\n                            padding='max_length',\n                            truncation=True,\n                            return_attention_mask=True,\n                            return_tensors='pt'\n                       )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\n# Function to train the model with mixed precision\ndef train_model_with_mixed_precision(model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, batch in enumerate(train_dataloader):\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            optimizer.zero_grad()\n\n            with autocast():  # Use mixed precision\n                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n                loss = outputs.loss\n\n            scaler.scale(loss).backward()  # Scale loss to prevent overflow\n            scaler.unscale_(optimizer)  # Unscales the gradients of optimizer's assigned params in-place\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Clip gradients to prevent explosion\n            scaler.step(optimizer)  # Take a step using the optimizer\n            scaler.update()  # Updates the scale for next iteration\n\n            scheduler.step()  # Update learning rate scheduler\n\n            running_loss += loss.item()\n\n            if i % 100 == 99:  # Print every 100 mini-batches\n                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {running_loss/100}\")\n                running_loss = 0.0\n\n# Tokenize and encode the texts\nmax_length = 128\n\n# DistilBERT\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(distilbert_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(distilbert_tokenizer, test_texts, max_length)\n\n# Assuming the labels are strings ('positive' and 'negative'), you can convert them to integers\nlabel_map = {'positive': 1, 'negative': 0}\ntrain_labels = train_labels.map(label_map)\ntest_labels = test_labels.map(label_map)\n\n# Convert labels to tensor\ntrain_labels = torch.tensor(train_labels.values, dtype=torch.long)\ntest_labels = torch.tensor(test_labels.values, dtype=torch.long)\n\n# Create dataloaders\nbatch_size = 16\ntrain_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\ntest_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n# Define hyperparameters\nnum_epochs = 3\nmax_grad_norm = 1.0\nwarmup_steps = 0.1 * len(train_dataloader)  # 10% of total training steps\n\n# Create scheduler\noptimizer = AdamW(distilbert_model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\n\n# Define scaler for mixed precision training\nscaler = GradScaler()\n\n# Fine-tune DistilBERT with mixed precision\ntrain_model_with_mixed_precision(distilbert_model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs)\nevaluate_model(distilbert_model, test_dataloader)\n\n# Repeat the same process for RoBERTa and GPT-2\n# For RoBERTa\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(roberta_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(roberta_tokenizer, test_texts, max_length)\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\nroberta_model.to(device)\noptimizer = AdamW(roberta_model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\ntrain_model_with_mixed_precision(roberta_model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs)\nevaluate_model(roberta_model, test_dataloader)\n\n# # For GPT-2\n# gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n# train_input_ids, train_attention_masks = tokenize_and_encode(gpt2_tokenizer, train_texts, max_length)\n# test_input_ids, test_attention_masks = tokenize_and_encode(gpt2_tokenizer, test_texts, max_length)\n# gpt2_model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n# gpt2_model.to(device)\n# optimizer = AdamW(gpt2_model.parameters(), lr=2e-5)\n# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\n# train_model_with_mixed_precision(gpt2_model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs)\n# evaluate_model(gpt2_model, test_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T14:36:13.176660Z","iopub.execute_input":"2024-02-18T14:36:13.177059Z","iopub.status.idle":"2024-02-18T15:09:32.030515Z","shell.execute_reply.started":"2024-02-18T14:36:13.177027Z","shell.execute_reply":"2024-02-18T15:09:32.029347Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.038977860137820244\nEpoch 1/3, Batch 200/1875, Loss: 0.018567416667938232\nEpoch 1/3, Batch 300/1875, Loss: 0.03453112371265888\nEpoch 1/3, Batch 400/1875, Loss: 0.0348983533680439\nEpoch 1/3, Batch 500/1875, Loss: 0.036452372744679454\nEpoch 1/3, Batch 600/1875, Loss: 0.0335359850525856\nEpoch 1/3, Batch 700/1875, Loss: 0.0476680451631546\nEpoch 1/3, Batch 800/1875, Loss: 0.030840462371706963\nEpoch 1/3, Batch 900/1875, Loss: 0.04182672463357449\nEpoch 1/3, Batch 1000/1875, Loss: 0.03341382563114166\nEpoch 1/3, Batch 1100/1875, Loss: 0.03000076524913311\nEpoch 1/3, Batch 1200/1875, Loss: 0.05138696871697903\nEpoch 1/3, Batch 1300/1875, Loss: 0.03074098937213421\nEpoch 1/3, Batch 1400/1875, Loss: 0.05903112329542637\nEpoch 1/3, Batch 1500/1875, Loss: 0.0674786216020584\nEpoch 1/3, Batch 1600/1875, Loss: 0.03453420706093311\nEpoch 1/3, Batch 1700/1875, Loss: 0.04805536761879921\nEpoch 1/3, Batch 1800/1875, Loss: 0.050163709595799445\nEpoch 2/3, Batch 100/1875, Loss: 0.05087951570749283\nEpoch 2/3, Batch 200/1875, Loss: 0.018963614627718926\nEpoch 2/3, Batch 300/1875, Loss: 0.032024386450648305\nEpoch 2/3, Batch 400/1875, Loss: 0.028769012838602066\nEpoch 2/3, Batch 500/1875, Loss: 0.034850781857967375\nEpoch 2/3, Batch 600/1875, Loss: 0.027376727685332298\nEpoch 2/3, Batch 700/1875, Loss: 0.03806659646332264\nEpoch 2/3, Batch 800/1875, Loss: 0.02438078887760639\nEpoch 2/3, Batch 900/1875, Loss: 0.015044594630599022\nEpoch 2/3, Batch 1000/1875, Loss: 0.016338389292359354\nEpoch 2/3, Batch 1100/1875, Loss: 0.023700745850801466\nEpoch 2/3, Batch 1200/1875, Loss: 0.024964946210384368\nEpoch 2/3, Batch 1300/1875, Loss: 0.02956727407872677\nEpoch 2/3, Batch 1400/1875, Loss: 0.0244949921220541\nEpoch 2/3, Batch 1500/1875, Loss: 0.03376739338040352\nEpoch 2/3, Batch 1600/1875, Loss: 0.0202851053327322\nEpoch 2/3, Batch 1700/1875, Loss: 0.026334463953971862\nEpoch 2/3, Batch 1800/1875, Loss: 0.020118688866496086\nEpoch 3/3, Batch 100/1875, Loss: 0.01361479513347149\nEpoch 3/3, Batch 200/1875, Loss: 0.004864135757088661\nEpoch 3/3, Batch 300/1875, Loss: 0.009817417114973068\nEpoch 3/3, Batch 400/1875, Loss: 0.010628629997372628\nEpoch 3/3, Batch 500/1875, Loss: 0.01804939180612564\nEpoch 3/3, Batch 600/1875, Loss: 0.0076069141924381255\nEpoch 3/3, Batch 700/1875, Loss: 0.007217779830098152\nEpoch 3/3, Batch 800/1875, Loss: 0.002310696691274643\nEpoch 3/3, Batch 900/1875, Loss: 0.0073244827985763546\nEpoch 3/3, Batch 1000/1875, Loss: 0.011395799964666366\nEpoch 3/3, Batch 1100/1875, Loss: 0.019949740990996362\nEpoch 3/3, Batch 1200/1875, Loss: 0.008566016927361489\nEpoch 3/3, Batch 1300/1875, Loss: 0.0038940203189849853\nEpoch 3/3, Batch 1400/1875, Loss: 0.01918076790869236\nEpoch 3/3, Batch 1500/1875, Loss: 0.011083162426948547\nEpoch 3/3, Batch 1600/1875, Loss: 0.014606440737843514\nEpoch 3/3, Batch 1700/1875, Loss: 0.012294552698731422\nEpoch 3/3, Batch 1800/1875, Loss: 0.027351216301321982\nAccuracy: 0.884\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.6971086120605469\nEpoch 1/3, Batch 200/1875, Loss: 0.6948506164550782\nEpoch 1/3, Batch 300/1875, Loss: 0.6959030151367187\nEpoch 1/3, Batch 400/1875, Loss: 0.69581298828125\nEpoch 1/3, Batch 500/1875, Loss: 0.6953398132324219\nEpoch 1/3, Batch 600/1875, Loss: 0.6969720458984375\nEpoch 1/3, Batch 700/1875, Loss: 0.6973811340332031\nEpoch 1/3, Batch 800/1875, Loss: 0.6933233642578125\nEpoch 1/3, Batch 900/1875, Loss: 0.6957020568847656\nEpoch 1/3, Batch 1000/1875, Loss: 0.6940127563476562\nEpoch 1/3, Batch 1100/1875, Loss: 0.6964111328125\nEpoch 1/3, Batch 1200/1875, Loss: 0.6905145645141602\nEpoch 1/3, Batch 1300/1875, Loss: 0.6935882568359375\nEpoch 1/3, Batch 1400/1875, Loss: 0.6881367492675782\nEpoch 1/3, Batch 1500/1875, Loss: 0.6878263854980469\nEpoch 1/3, Batch 1600/1875, Loss: 0.6957080078125\nEpoch 1/3, Batch 1700/1875, Loss: 0.6936155700683594\nEpoch 1/3, Batch 1800/1875, Loss: 0.6919789123535156\nEpoch 2/3, Batch 100/1875, Loss: 0.6870713806152344\nEpoch 2/3, Batch 200/1875, Loss: 0.6868045806884766\nEpoch 2/3, Batch 300/1875, Loss: 0.6718344116210937\nEpoch 2/3, Batch 400/1875, Loss: 0.6687701416015625\nEpoch 2/3, Batch 500/1875, Loss: 0.6648538970947265\nEpoch 2/3, Batch 600/1875, Loss: 0.6351055526733398\nEpoch 2/3, Batch 700/1875, Loss: 0.6519227981567383\nEpoch 2/3, Batch 800/1875, Loss: 0.6401300048828125\nEpoch 2/3, Batch 900/1875, Loss: 0.6347154235839844\nEpoch 2/3, Batch 1000/1875, Loss: 0.5992267990112304\nEpoch 2/3, Batch 1100/1875, Loss: 0.5832063674926757\nEpoch 2/3, Batch 1200/1875, Loss: 0.5886997222900391\nEpoch 2/3, Batch 1300/1875, Loss: 0.5936423492431641\nEpoch 2/3, Batch 1400/1875, Loss: 0.5790802383422852\nEpoch 2/3, Batch 1500/1875, Loss: 0.585690803527832\nEpoch 2/3, Batch 1600/1875, Loss: 0.5831288719177246\nEpoch 2/3, Batch 1700/1875, Loss: 0.5795828056335449\nEpoch 2/3, Batch 1800/1875, Loss: 0.5604137420654297\nEpoch 3/3, Batch 100/1875, Loss: 0.5547257995605469\nEpoch 3/3, Batch 200/1875, Loss: 0.57234619140625\nEpoch 3/3, Batch 300/1875, Loss: 0.5500665855407715\nEpoch 3/3, Batch 400/1875, Loss: 0.5579892921447754\nEpoch 3/3, Batch 500/1875, Loss: 0.5468659782409668\nEpoch 3/3, Batch 600/1875, Loss: 0.5296306610107422\nEpoch 3/3, Batch 700/1875, Loss: 0.5217404365539551\nEpoch 3/3, Batch 800/1875, Loss: 0.5217997550964355\nEpoch 3/3, Batch 900/1875, Loss: 0.5338098907470703\nEpoch 3/3, Batch 1000/1875, Loss: 0.5157862472534179\nEpoch 3/3, Batch 1100/1875, Loss: 0.4867878532409668\nEpoch 3/3, Batch 1200/1875, Loss: 0.5195725727081298\nEpoch 3/3, Batch 1300/1875, Loss: 0.5076051616668701\nEpoch 3/3, Batch 1400/1875, Loss: 0.5228401184082031\nEpoch 3/3, Batch 1500/1875, Loss: 0.5031242179870605\nEpoch 3/3, Batch 1600/1875, Loss: 0.5088554763793945\nEpoch 3/3, Batch 1700/1875, Loss: 0.5078655338287353\nEpoch 3/3, Batch 1800/1875, Loss: 0.4978698921203613\nAccuracy: 0.74285\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim import SGD\n\n# Load training dataset\ntrain_df = pd.read_csv(\"/kaggle/input/plmdataset/train.csv/train.csv\")  # Replace \"train_dataset.csv\" with the actual path to your training dataset file\n\n# Load testing dataset\ntest_df = pd.read_csv(\"/kaggle/input/plmdataset/test.csv/test.csv\")  # Replace \"test_dataset.csv\" with the actual path to your testing dataset file\n\n# Extract texts and labels from training dataset\ntrain_texts = train_df['review']\ntrain_labels = train_df['sentiment']\n\n# Extract texts and labels from testing dataset\ntest_texts = test_df['review']\ntest_labels = test_df['sentiment']\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to tokenize and encode the text\ndef tokenize_and_encode(tokenizer, texts, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n                            text,\n                            add_special_tokens=True,\n                            max_length=max_length,\n                            padding='max_length',\n                            truncation=True,\n                            return_attention_mask=True,\n                            return_tensors='pt'\n                       )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\n# Function to train the model with mixed precision\ndef train_model_with_mixed_precision(model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, batch in enumerate(train_dataloader):\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            optimizer.zero_grad()\n\n            with autocast():  # Use mixed precision\n                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n                loss = outputs.loss\n\n            scaler.scale(loss).backward()  # Scale loss to prevent overflow\n            scaler.unscale_(optimizer)  # Unscales the gradients of optimizer's assigned params in-place\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Clip gradients to prevent explosion\n            scaler.step(optimizer)  # Take a step using the optimizer\n            scaler.update()  # Updates the scale for next iteration\n\n            scheduler.step()  # Update learning rate scheduler\n\n            running_loss += loss.item()\n\n            if i % 100 == 99:  # Print every 100 mini-batches\n                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {running_loss/100}\")\n                running_loss = 0.0\n\n# Tokenize and encode the texts\nmax_length = 128\n\n# DistilBERT\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(distilbert_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(distilbert_tokenizer, test_texts, max_length)\n\n# Assuming the labels are strings ('positive' and 'negative'), you can convert them to integers\nlabel_map = {'positive': 1, 'negative': 0}\ntrain_labels = train_labels.map(label_map)\ntest_labels = test_labels.map(label_map)\n\n# Convert labels to tensor\ntrain_labels = torch.tensor(train_labels.values, dtype=torch.long)\ntest_labels = torch.tensor(test_labels.values, dtype=torch.long)\n\n# Create dataloaders\nbatch_size = 16\ntrain_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\ntest_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n# Define hyperparameters\nnum_epochs = 3\nmax_grad_norm = 1.0\nwarmup_steps = 0.1 * len(train_dataloader)  # 10% of total training steps\n\n# Create DistilBERT model\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(device)\n\n# Create optimizer and scheduler\noptimizer_distilbert = SGD(distilbert_model.parameters(), lr=0.01, momentum=0.9)\nscheduler_distilbert = get_linear_schedule_with_warmup(optimizer_distilbert, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\n\n# Define scaler for mixed precision training\nscaler = GradScaler()\n\n# Fine-tune DistilBERT with mixed precision\ntrain_model_with_mixed_precision(distilbert_model, train_dataloader, optimizer_distilbert, scheduler_distilbert, nn.CrossEntropyLoss(), scaler, num_epochs)\nevaluate_model(distilbert_model, test_dataloader)\n\n# Repeat the same process for RoBERTa\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(roberta_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(roberta_tokenizer, test_texts, max_length)\n\n# Create RoBERTa model\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2).to(device)\n\n# Create optimizer and scheduler\noptimizer_roberta = SGD(roberta_model.parameters(), lr=0.01, momentum=0.9)\nscheduler_roberta = get_linear_schedule_with_warmup(optimizer_roberta, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\n\n# Fine-tune RoBERTa with mixed precision\ntrain_model_with_mixed_precision(roberta_model, train_dataloader, optimizer_roberta, scheduler_roberta, nn.CrossEntropyLoss(), scaler, num_epochs)\nevaluate_model(roberta_model, test_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:56:35.903617Z","iopub.execute_input":"2024-02-18T16:56:35.904013Z","iopub.status.idle":"2024-02-18T17:27:30.444167Z","shell.execute_reply.started":"2024-02-18T16:56:35.903985Z","shell.execute_reply":"2024-02-18T17:27:30.443124Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.638399543762207\nEpoch 1/3, Batch 200/1875, Loss: 0.494405632019043\nEpoch 1/3, Batch 300/1875, Loss: 0.422783145904541\nEpoch 1/3, Batch 400/1875, Loss: 0.4123725938796997\nEpoch 1/3, Batch 500/1875, Loss: 0.3828009366989136\nEpoch 1/3, Batch 600/1875, Loss: 0.38030768394470216\nEpoch 1/3, Batch 700/1875, Loss: 0.37837531328201296\nEpoch 1/3, Batch 800/1875, Loss: 0.4078387808799744\nEpoch 1/3, Batch 900/1875, Loss: 0.3658698034286499\nEpoch 1/3, Batch 1000/1875, Loss: 0.3521576488018036\nEpoch 1/3, Batch 1100/1875, Loss: 0.344450261592865\nEpoch 1/3, Batch 1200/1875, Loss: 0.3409921288490295\nEpoch 1/3, Batch 1300/1875, Loss: 0.3452486753463745\nEpoch 1/3, Batch 1400/1875, Loss: 0.36082704782485964\nEpoch 1/3, Batch 1500/1875, Loss: 0.32019894003868105\nEpoch 1/3, Batch 1600/1875, Loss: 0.33297763466835023\nEpoch 1/3, Batch 1700/1875, Loss: 0.36296723008155823\nEpoch 1/3, Batch 1800/1875, Loss: 0.3260021793842316\nEpoch 2/3, Batch 100/1875, Loss: 0.32985604763031007\nEpoch 2/3, Batch 200/1875, Loss: 0.31528124570846555\nEpoch 2/3, Batch 300/1875, Loss: 0.2817630678415298\nEpoch 2/3, Batch 400/1875, Loss: 0.3011503005027771\nEpoch 2/3, Batch 500/1875, Loss: 0.280997711122036\nEpoch 2/3, Batch 600/1875, Loss: 0.28678333163261416\nEpoch 2/3, Batch 700/1875, Loss: 0.2576856642961502\nEpoch 2/3, Batch 800/1875, Loss: 0.26707930088043214\nEpoch 2/3, Batch 900/1875, Loss: 0.2569731563329697\nEpoch 2/3, Batch 1000/1875, Loss: 0.2364058029651642\nEpoch 2/3, Batch 1100/1875, Loss: 0.23307652086019515\nEpoch 2/3, Batch 1200/1875, Loss: 0.23338297843933106\nEpoch 2/3, Batch 1300/1875, Loss: 0.270063219666481\nEpoch 2/3, Batch 1400/1875, Loss: 0.2729021620750427\nEpoch 2/3, Batch 1500/1875, Loss: 0.22152749300003052\nEpoch 2/3, Batch 1600/1875, Loss: 0.27228757441043855\nEpoch 2/3, Batch 1700/1875, Loss: 0.25026678562164306\nEpoch 2/3, Batch 1800/1875, Loss: 0.22098086655139923\nEpoch 3/3, Batch 100/1875, Loss: 0.24657639414072036\nEpoch 3/3, Batch 200/1875, Loss: 0.2323412564396858\nEpoch 3/3, Batch 300/1875, Loss: 0.19998804360628128\nEpoch 3/3, Batch 400/1875, Loss: 0.21420256346464156\nEpoch 3/3, Batch 500/1875, Loss: 0.18530170395970344\nEpoch 3/3, Batch 600/1875, Loss: 0.19853367209434508\nEpoch 3/3, Batch 700/1875, Loss: 0.1722753517329693\nEpoch 3/3, Batch 800/1875, Loss: 0.19756732687354087\nEpoch 3/3, Batch 900/1875, Loss: 0.18920622900128364\nEpoch 3/3, Batch 1000/1875, Loss: 0.16169812321662902\nEpoch 3/3, Batch 1100/1875, Loss: 0.1534866251051426\nEpoch 3/3, Batch 1200/1875, Loss: 0.15827347204089165\nEpoch 3/3, Batch 1300/1875, Loss: 0.16716860368847847\nEpoch 3/3, Batch 1400/1875, Loss: 0.2186095404624939\nEpoch 3/3, Batch 1500/1875, Loss: 0.1814175758510828\nEpoch 3/3, Batch 1600/1875, Loss: 0.2003120781481266\nEpoch 3/3, Batch 1700/1875, Loss: 0.18933923915028572\nEpoch 3/3, Batch 1800/1875, Loss: 0.17760580018162728\nAccuracy: 0.87725\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.7025511932373046\nEpoch 1/3, Batch 200/1875, Loss: 0.7114691925048828\nEpoch 1/3, Batch 300/1875, Loss: 0.7070757293701172\nEpoch 1/3, Batch 400/1875, Loss: 0.7067838287353516\nEpoch 1/3, Batch 500/1875, Loss: 0.6982167053222657\nEpoch 1/3, Batch 600/1875, Loss: 0.6987605285644531\nEpoch 1/3, Batch 700/1875, Loss: 0.6948300170898437\nEpoch 1/3, Batch 800/1875, Loss: 0.69453857421875\nEpoch 1/3, Batch 900/1875, Loss: 0.69548095703125\nEpoch 1/3, Batch 1000/1875, Loss: 0.6946369934082032\nEpoch 1/3, Batch 1100/1875, Loss: 0.6942010498046876\nEpoch 1/3, Batch 1200/1875, Loss: 0.6946176147460937\nEpoch 1/3, Batch 1300/1875, Loss: 0.6954307556152344\nEpoch 1/3, Batch 1400/1875, Loss: 0.694906005859375\nEpoch 1/3, Batch 1500/1875, Loss: 0.6949349975585938\nEpoch 1/3, Batch 1600/1875, Loss: 0.6954974365234375\nEpoch 1/3, Batch 1700/1875, Loss: 0.69441162109375\nEpoch 1/3, Batch 1800/1875, Loss: 0.69405029296875\nEpoch 2/3, Batch 100/1875, Loss: 0.694393310546875\nEpoch 2/3, Batch 200/1875, Loss: 0.6943914794921875\nEpoch 2/3, Batch 300/1875, Loss: 0.69385498046875\nEpoch 2/3, Batch 400/1875, Loss: 0.6942315673828126\nEpoch 2/3, Batch 500/1875, Loss: 0.6952951049804688\nEpoch 2/3, Batch 600/1875, Loss: 0.6945474243164063\nEpoch 2/3, Batch 700/1875, Loss: 0.6922427368164062\nEpoch 2/3, Batch 800/1875, Loss: 0.6928944396972656\nEpoch 2/3, Batch 900/1875, Loss: 0.6942579650878906\nEpoch 2/3, Batch 1000/1875, Loss: 0.6932888793945312\nEpoch 2/3, Batch 1100/1875, Loss: 0.6932589721679687\nEpoch 2/3, Batch 1200/1875, Loss: 0.6944659423828125\nEpoch 2/3, Batch 1300/1875, Loss: 0.6943551635742188\nEpoch 2/3, Batch 1400/1875, Loss: 0.6934896850585938\nEpoch 2/3, Batch 1500/1875, Loss: 0.6932945251464844\nEpoch 2/3, Batch 1600/1875, Loss: 0.693909912109375\nEpoch 2/3, Batch 1700/1875, Loss: 0.6935284423828125\nEpoch 2/3, Batch 1800/1875, Loss: 0.6941763305664063\nEpoch 3/3, Batch 100/1875, Loss: 0.6933859252929687\nEpoch 3/3, Batch 200/1875, Loss: 0.6931289672851563\nEpoch 3/3, Batch 300/1875, Loss: 0.693084716796875\nEpoch 3/3, Batch 400/1875, Loss: 0.6935757446289063\nEpoch 3/3, Batch 500/1875, Loss: 0.6938406372070313\nEpoch 3/3, Batch 600/1875, Loss: 0.6936569213867188\nEpoch 3/3, Batch 700/1875, Loss: 0.6927560424804687\nEpoch 3/3, Batch 800/1875, Loss: 0.6925009155273437\nEpoch 3/3, Batch 900/1875, Loss: 0.6931149291992188\nEpoch 3/3, Batch 1000/1875, Loss: 0.692882080078125\nEpoch 3/3, Batch 1100/1875, Loss: 0.6899261474609375\nEpoch 3/3, Batch 1200/1875, Loss: 0.68900390625\nEpoch 3/3, Batch 1300/1875, Loss: 0.694307861328125\nEpoch 3/3, Batch 1400/1875, Loss: 0.6934947204589844\nEpoch 3/3, Batch 1500/1875, Loss: 0.6876557922363281\nEpoch 3/3, Batch 1600/1875, Loss: 0.6826687622070312\nEpoch 3/3, Batch 1700/1875, Loss: 0.6789578247070313\nEpoch 3/3, Batch 1800/1875, Loss: 0.678226318359375\nAccuracy: 0.5969\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import ExponentialLR\n\n# Load training dataset\ntrain_df = pd.read_csv(\"/kaggle/input/plmdataset/train.csv/train.csv\")  # Replace \"train_dataset.csv\" with the actual path to your training dataset file\n\n# Load testing dataset\ntest_df = pd.read_csv(\"/kaggle/input/plmdataset/test.csv/test.csv\")  # Replace \"test_dataset.csv\" with the actual path to your testing dataset file\n\n# Extract texts and labels from training dataset\ntrain_texts = train_df['review']\ntrain_labels = train_df['sentiment']\n\n# Extract texts and labels from testing dataset\ntest_texts = test_df['review']\ntest_labels = test_df['sentiment']\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to tokenize and encode the text\ndef tokenize_and_encode(tokenizer, texts, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n                            text,\n                            add_special_tokens=True,\n                            max_length=max_length,\n                            padding='max_length',\n                            truncation=True,\n                            return_attention_mask=True,\n                            return_tensors='pt'\n                       )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\n# Function to train the model with mixed precision\ndef train_model_with_mixed_precision(model, train_dataloader, optimizer, scheduler, criterion, scaler, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, batch in enumerate(train_dataloader):\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            optimizer.zero_grad()\n\n            with autocast():  # Use mixed precision\n                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n                loss = outputs.loss\n\n            scaler.scale(loss).backward()  # Scale loss to prevent overflow\n            scaler.unscale_(optimizer)  # Unscales the gradients of optimizer's assigned params in-place\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Clip gradients to prevent explosion\n            scaler.step(optimizer)  # Take a step using the optimizer\n            scaler.update()  # Updates the scale for next iteration\n\n            scheduler.step()  # Update learning rate scheduler\n\n            running_loss += loss.item()\n\n            if i % 100 == 99:  # Print every 100 mini-batches\n                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {running_loss/100}\")\n                running_loss = 0.0\n\n# Tokenize and encode the texts\nmax_length = 128\n\n# DistilBERT\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(distilbert_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(distilbert_tokenizer, test_texts, max_length)\n\n# RoBERTa\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntrain_input_ids_roberta, train_attention_masks_roberta = tokenize_and_encode(roberta_tokenizer, train_texts, max_length)\ntest_input_ids_roberta, test_attention_masks_roberta = tokenize_and_encode(roberta_tokenizer, test_texts, max_length)\n\n# Assuming the labels are strings ('positive' and 'negative'), you can convert them to integers\nlabel_map = {'positive': 1, 'negative': 0}\ntrain_labels = train_labels.map(label_map)\ntest_labels = test_labels.map(label_map)\n\n# Convert labels to tensor\ntrain_labels = torch.tensor(train_labels.values, dtype=torch.long)\ntest_labels = torch.tensor(test_labels.values, dtype=torch.long)\n\n# Create dataloaders\nbatch_size = 16\ntrain_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\ntest_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n# Create dataloaders for RoBERTa\ntrain_data_roberta = TensorDataset(train_input_ids_roberta, train_attention_masks_roberta, train_labels)\ntrain_dataloader_roberta = DataLoader(train_data_roberta, batch_size=batch_size)\ntest_data_roberta = TensorDataset(test_input_ids_roberta, test_attention_masks_roberta, test_labels)\ntest_dataloader_roberta = DataLoader(test_data_roberta, batch_size=batch_size)\n\n# Define hyperparameters\nnum_epochs = 3\nmax_grad_norm = 1.0\nwarmup_steps = 0.1 * len(train_dataloader)  # 10% of total training steps\n\n# Define learning rate and schedule for DistilBERT\ninitial_learning_rate_distilbert = 2e-5\n# Create optimizer with adjustable learning rate for DistilBERT\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\ndistilbert_model.to(device)\noptimizer_distilbert = AdamW(distilbert_model.parameters(), lr=initial_learning_rate_distilbert)\n\n# Create scheduler with exponential decay for DistilBERT\nscheduler_distilbert = get_linear_schedule_with_warmup(optimizer_distilbert, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * num_epochs)\nscheduler_distilbert = ExponentialLR(optimizer_distilbert, gamma=0.9) # ExponentialLR for exponential decay\n\n# Define scaler for mixed precision training\nscaler = GradScaler()\n\n# Fine-tune DistilBERT with mixed precision\ntrain_model_with_mixed_precision(distilbert_model, train_dataloader, optimizer_distilbert, scheduler_distilbert, nn.CrossEntropyLoss(), scaler, num_epochs)\n\n# Define learning rate and schedule for RoBERTa\ninitial_learning_rate_roberta = 2e-5\n# Create optimizer with adjustable learning rate for RoBERTa\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\nroberta_model.to(device)\noptimizer_roberta = AdamW(roberta_model.parameters(), lr=initial_learning_rate_roberta)\n\n# Create scheduler with exponential decay for RoBERTa\nscheduler_roberta = get_linear_schedule_with_warmup(optimizer_roberta, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader_roberta) * num_epochs)\nscheduler_roberta = ExponentialLR(optimizer_roberta, gamma=0.9) # ExponentialLR for exponential decay\n\n# Fine-tune RoBERTa with mixed precision\ntrain_model_with_mixed_precision(roberta_model, train_dataloader_roberta, optimizer_roberta, scheduler_roberta, nn.CrossEntropyLoss(), scaler, num_epochs)\n\n# Evaluate DistilBERT\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, labels = batch\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs.logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Accuracy: {accuracy}\")\n\nevaluate_model(distilbert_model, test_dataloader)\n\n# Evaluate RoBERTa\nevaluate_model(roberta_model, test_dataloader_roberta)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:07:45.828799Z","iopub.execute_input":"2024-02-18T16:07:45.829170Z","iopub.status.idle":"2024-02-18T16:41:06.231400Z","shell.execute_reply.started":"2024-02-18T16:07:45.829141Z","shell.execute_reply":"2024-02-18T16:41:06.230376Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.6952365112304687\nEpoch 1/3, Batch 200/1875, Loss: 0.696234130859375\nEpoch 1/3, Batch 300/1875, Loss: 0.6951004028320312\nEpoch 1/3, Batch 400/1875, Loss: 0.6953091430664062\nEpoch 1/3, Batch 500/1875, Loss: 0.6928515625\nEpoch 1/3, Batch 600/1875, Loss: 0.694931640625\nEpoch 1/3, Batch 700/1875, Loss: 0.696068115234375\nEpoch 1/3, Batch 800/1875, Loss: 0.695142822265625\nEpoch 1/3, Batch 900/1875, Loss: 0.6942910766601562\nEpoch 1/3, Batch 1000/1875, Loss: 0.69353759765625\nEpoch 1/3, Batch 1100/1875, Loss: 0.6954696655273438\nEpoch 1/3, Batch 1200/1875, Loss: 0.6924240112304687\nEpoch 1/3, Batch 1300/1875, Loss: 0.6942092895507812\nEpoch 1/3, Batch 1400/1875, Loss: 0.69363525390625\nEpoch 1/3, Batch 1500/1875, Loss: 0.6938516235351563\nEpoch 1/3, Batch 1600/1875, Loss: 0.694998779296875\nEpoch 1/3, Batch 1700/1875, Loss: 0.6942047119140625\nEpoch 1/3, Batch 1800/1875, Loss: 0.6917852783203124\nEpoch 2/3, Batch 100/1875, Loss: 0.695845947265625\nEpoch 2/3, Batch 200/1875, Loss: 0.6955203247070313\nEpoch 2/3, Batch 300/1875, Loss: 0.6943658447265625\nEpoch 2/3, Batch 400/1875, Loss: 0.6950830078125\nEpoch 2/3, Batch 500/1875, Loss: 0.692049560546875\nEpoch 2/3, Batch 600/1875, Loss: 0.6933535766601563\nEpoch 2/3, Batch 700/1875, Loss: 0.6917349243164063\nEpoch 2/3, Batch 800/1875, Loss: 0.6946011352539062\nEpoch 2/3, Batch 900/1875, Loss: 0.69296875\nEpoch 2/3, Batch 1000/1875, Loss: 0.6924227905273438\nEpoch 2/3, Batch 1100/1875, Loss: 0.69147705078125\nEpoch 2/3, Batch 1200/1875, Loss: 0.6934478759765625\nEpoch 2/3, Batch 1300/1875, Loss: 0.6943014526367187\nEpoch 2/3, Batch 1400/1875, Loss: 0.695115966796875\nEpoch 2/3, Batch 1500/1875, Loss: 0.6943798828125\nEpoch 2/3, Batch 1600/1875, Loss: 0.6947116088867188\nEpoch 2/3, Batch 1700/1875, Loss: 0.693489990234375\nEpoch 2/3, Batch 1800/1875, Loss: 0.6922604370117188\nEpoch 3/3, Batch 100/1875, Loss: 0.6951278686523438\nEpoch 3/3, Batch 200/1875, Loss: 0.6953207397460938\nEpoch 3/3, Batch 300/1875, Loss: 0.69247802734375\nEpoch 3/3, Batch 400/1875, Loss: 0.695355224609375\nEpoch 3/3, Batch 500/1875, Loss: 0.6948406982421875\nEpoch 3/3, Batch 600/1875, Loss: 0.6944290161132812\nEpoch 3/3, Batch 700/1875, Loss: 0.6951596069335938\nEpoch 3/3, Batch 800/1875, Loss: 0.6949166870117187\nEpoch 3/3, Batch 900/1875, Loss: 0.6925833129882812\nEpoch 3/3, Batch 1000/1875, Loss: 0.6930105590820312\nEpoch 3/3, Batch 1100/1875, Loss: 0.6935324096679687\nEpoch 3/3, Batch 1200/1875, Loss: 0.6939352416992187\nEpoch 3/3, Batch 1300/1875, Loss: 0.6931344604492188\nEpoch 3/3, Batch 1400/1875, Loss: 0.6958328247070312\nEpoch 3/3, Batch 1500/1875, Loss: 0.6924664306640625\nEpoch 3/3, Batch 1600/1875, Loss: 0.6954547119140625\nEpoch 3/3, Batch 1700/1875, Loss: 0.6957962036132812\nEpoch 3/3, Batch 1800/1875, Loss: 0.6926739501953125\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch 100/1875, Loss: 0.7008372497558594\nEpoch 1/3, Batch 200/1875, Loss: 0.6993682861328125\nEpoch 1/3, Batch 300/1875, Loss: 0.6985882568359375\nEpoch 1/3, Batch 400/1875, Loss: 0.7031089782714843\nEpoch 1/3, Batch 500/1875, Loss: 0.6975228881835938\nEpoch 1/3, Batch 600/1875, Loss: 0.6984939575195312\nEpoch 1/3, Batch 700/1875, Loss: 0.7010128784179688\nEpoch 1/3, Batch 800/1875, Loss: 0.6990983581542969\nEpoch 1/3, Batch 900/1875, Loss: 0.692613525390625\nEpoch 1/3, Batch 1000/1875, Loss: 0.6938954162597656\nEpoch 1/3, Batch 1100/1875, Loss: 0.6946134948730469\nEpoch 1/3, Batch 1200/1875, Loss: 0.6980857849121094\nEpoch 1/3, Batch 1300/1875, Loss: 0.6969267272949219\nEpoch 1/3, Batch 1400/1875, Loss: 0.6992776489257813\nEpoch 1/3, Batch 1500/1875, Loss: 0.6999372863769531\nEpoch 1/3, Batch 1600/1875, Loss: 0.6993745422363281\nEpoch 1/3, Batch 1700/1875, Loss: 0.6991278076171875\nEpoch 1/3, Batch 1800/1875, Loss: 0.6999179077148437\nEpoch 2/3, Batch 100/1875, Loss: 0.7030986022949218\nEpoch 2/3, Batch 200/1875, Loss: 0.7003971862792969\nEpoch 2/3, Batch 300/1875, Loss: 0.698895263671875\nEpoch 2/3, Batch 400/1875, Loss: 0.7002685546875\nEpoch 2/3, Batch 500/1875, Loss: 0.6960035705566406\nEpoch 2/3, Batch 600/1875, Loss: 0.7009284973144532\nEpoch 2/3, Batch 700/1875, Loss: 0.6981956481933593\nEpoch 2/3, Batch 800/1875, Loss: 0.6996267700195312\nEpoch 2/3, Batch 900/1875, Loss: 0.6964289855957031\nEpoch 2/3, Batch 1000/1875, Loss: 0.6953314208984375\nEpoch 2/3, Batch 1100/1875, Loss: 0.6953016662597656\nEpoch 2/3, Batch 1200/1875, Loss: 0.6975892639160156\nEpoch 2/3, Batch 1300/1875, Loss: 0.6971302795410156\nEpoch 2/3, Batch 1400/1875, Loss: 0.6975001525878907\nEpoch 2/3, Batch 1500/1875, Loss: 0.6958174133300781\nEpoch 2/3, Batch 1600/1875, Loss: 0.6976278686523437\nEpoch 2/3, Batch 1700/1875, Loss: 0.6980998229980468\nEpoch 2/3, Batch 1800/1875, Loss: 0.6967044067382813\nEpoch 3/3, Batch 100/1875, Loss: 0.7012713623046875\nEpoch 3/3, Batch 200/1875, Loss: 0.6991055297851563\nEpoch 3/3, Batch 300/1875, Loss: 0.6986656188964844\nEpoch 3/3, Batch 400/1875, Loss: 0.6993418884277344\nEpoch 3/3, Batch 500/1875, Loss: 0.6976358032226563\nEpoch 3/3, Batch 600/1875, Loss: 0.6989553833007812\nEpoch 3/3, Batch 700/1875, Loss: 0.7009678649902343\nEpoch 3/3, Batch 800/1875, Loss: 0.6979411315917968\nEpoch 3/3, Batch 900/1875, Loss: 0.6952255249023438\nEpoch 3/3, Batch 1000/1875, Loss: 0.6954566955566406\nEpoch 3/3, Batch 1100/1875, Loss: 0.6937945556640625\nEpoch 3/3, Batch 1200/1875, Loss: 0.695462646484375\nEpoch 3/3, Batch 1300/1875, Loss: 0.6949417114257812\nEpoch 3/3, Batch 1400/1875, Loss: 0.6965745544433594\nEpoch 3/3, Batch 1500/1875, Loss: 0.6979829406738282\nEpoch 3/3, Batch 1600/1875, Loss: 0.6994242858886719\nEpoch 3/3, Batch 1700/1875, Loss: 0.6975149536132812\nEpoch 3/3, Batch 1800/1875, Loss: 0.6957183837890625\nAccuracy: 0.5028\nAccuracy: 0.50325\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Hyperparameter tuning of roberta and distilbert ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.model_selection import GridSearchCV\n\n# Load training dataset\ntrain_df = pd.read_csv(\"/kaggle/input/plmdataset/train.csv/train.csv\")  # Replace \"train_dataset.csv\" with the actual path to your training dataset file\n\n# Load testing dataset\ntest_df = pd.read_csv(\"/kaggle/input/plmdataset/test.csv/test.csv\")  # Replace \"test_dataset.csv\" with the actual path to your testing dataset file\n\n# Extract texts and labels from training dataset\ntrain_texts = train_df['review']\ntrain_labels = train_df['sentiment']\n\n# Extract texts and labels from testing dataset\ntest_texts = test_df['review']\ntest_labels = test_df['sentiment']\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to tokenize and encode the text\ndef tokenize_and_encode(tokenizer, texts, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n                            text,\n                            add_special_tokens=True,\n                            max_length=max_length,\n                            padding='max_length',\n                            truncation=True,\n                            return_attention_mask=True,\n                            return_tensors='pt'\n                       )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\n# Tokenize and encode the texts\nmax_length = 128\n\n# Define function to create dataloaders\ndef create_data_loader(tokenizer, texts, labels, max_length, batch_size):\n    input_ids, attention_masks = tokenize_and_encode(tokenizer, texts, max_length)\n    labels = torch.tensor(labels.map(label_map).values, dtype=torch.long)\n\n    dataset = TensorDataset(input_ids, attention_masks, labels)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    return dataloader\n\n# Define hyperparameters to search\nparam_grid = {\n    'learning_rate': [5e-5, 3e-5, 2e-5],\n    'batch_size': [16, 32],\n    'num_epochs': [2, 3, 4]\n}\n\n# DistilBERT\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_input_ids, train_attention_masks = tokenize_and_encode(distilbert_tokenizer, train_texts, max_length)\ntest_input_ids, test_attention_masks = tokenize_and_encode(distilbert_tokenizer, test_texts, max_length)\n\n# Define label mapping\nlabel_map = {'positive': 1, 'negative': 0}\n\n# Assuming the labels are strings ('positive' and 'negative'), you can convert them to integers\ntrain_labels = train_labels.map(label_map)\ntest_labels = test_labels.map(label_map)\n\n# Convert labels to tensor\ntrain_labels = torch.tensor(train_labels.values, dtype=torch.long)\ntest_labels = torch.tensor(test_labels.values, dtype=torch.long)\n\n# Initialize DistilBERT model\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\n# Create dataloaders for DistilBERT\ntrain_dataloader = create_data_loader(distilbert_tokenizer, train_texts, train_labels, max_length, batch_size=param_grid['batch_size'][0])\ntest_dataloader = create_data_loader(distilbert_tokenizer, test_texts, test_labels, max_length, batch_size=param_grid['batch_size'][0])\n\n# Initialize GridSearchCV for DistilBERT\ngrid_search_distilbert = GridSearchCV(estimator=None, param_grid=param_grid, scoring='accuracy', cv=3)\n\n# Run GridSearchCV for DistilBERT\ngrid_search_distilbert.fit(train_dataloader, test_dataloader)\n\n# Get the best parameters and accuracy for DistilBERT\nbest_params_distilbert = grid_search_distilbert.best_params_\nbest_accuracy_distilbert = grid_search_distilbert.best_score_\n\nprint(\"Best Parameters (DistilBERT):\", best_params_distilbert)\nprint(\"Best Accuracy (DistilBERT):\", best_accuracy_distilbert)\n\n# Instantiate RoBERTa tokenizer\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Initialize RoBERTa model\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n\n# Create dataloaders for RoBERTa\ntrain_dataloader = create_data_loader(roberta_tokenizer, train_texts, train_labels, max_length, batch_size=param_grid['batch_size'][0])\ntest_dataloader = create_data_loader(roberta_tokenizer, test_texts, test_labels, max_length, batch_size=param_grid['batch_size'][0])\n\n# Initialize GridSearchCV for RoBERTa\ngrid_search_roberta = GridSearchCV(estimator=None, param_grid=param_grid, scoring='accuracy', cv=3)\n\n# Run GridSearchCV for RoBERTa\ngrid_search_roberta.fit(train_dataloader, test_dataloader)\n\n# Get the best parameters and accuracy for RoBERTa\nbest_params_roberta = grid_search_roberta.best_params_\nbest_accuracy_roberta = grid_search_roberta.best_score_\n\nprint(\"Best Parameters (RoBERTa):\", best_params_roberta)\nprint(\"Best Accuracy (RoBERTa):\", best_accuracy_roberta)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T14:17:58.449183Z","iopub.execute_input":"2024-02-21T14:17:58.449631Z","iopub.status.idle":"2024-02-21T14:20:45.702585Z","shell.execute_reply.started":"2024-02-21T14:17:58.449601Z","shell.execute_reply":"2024-02-21T14:20:45.701096Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# DistilBERT\u001b[39;00m\n\u001b[1;32m     73\u001b[0m distilbert_tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m train_input_ids, train_attention_masks \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistilbert_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m test_input_ids, test_attention_masks \u001b[38;5;241m=\u001b[39m tokenize_and_encode(distilbert_tokenizer, test_texts, max_length)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Define label mapping\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 35\u001b[0m, in \u001b[0;36mtokenize_and_encode\u001b[0;34m(tokenizer, texts, max_length)\u001b[0m\n\u001b[1;32m     32\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m---> 35\u001b[0m     encoded_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(encoded_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     45\u001b[0m     attention_masks\u001b[38;5;241m.\u001b[39mappend(encoded_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2982\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2974\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2975\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2979\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2980\u001b[0m )\n\u001b[0;32m-> 2982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:201\u001b[0m, in \u001b[0;36mDistilBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    199\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    206\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:387\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[0;32m--> 387\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:489\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    488\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_is_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:290\u001b[0m, in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    289\u001b[0m cat \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mcategory(char)\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification#, AdamW\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom torch.optim import AdamW\n\nimport time\n# Record start time\nstart_time = time.time()\n\n\n# Load data\ndata = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ndata['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\nreviews = data['review'].tolist()\nlabels = data['sentiment'].tolist()  # assuming sentiment is encoded as 0 (negative) and 1 (positive)\n\n# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(reviews, labels, test_size=0.2)\n\n# Initialize tokenizer\ntokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n\n# Tokenize data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create dataloaders\ntrain_dataset = ReviewDataset(train_encodings, train_labels)\nval_dataset = ReviewDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize model\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\nmodel = model.to('cuda')  # if GPU is available\n\n# Initialize optimizer\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n# Training loop\nfor epoch in range(3):  # number of epochs\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Save the model\nmodel.save_pretrained('sentiment_model_RoBERTa')\n\n# Record end time\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T14:22:29.044793Z","iopub.execute_input":"2024-02-21T14:22:29.045771Z","iopub.status.idle":"2024-02-21T16:14:24.224947Z","shell.execute_reply.started":"2024-02-21T14:22:29.045737Z","shell.execute_reply":"2024-02-21T16:14:24.223824Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ffdaba5b71444f49313bcace94982cd"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Time required to fine-tune:  6715.162095785141\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport torch\n\n# Load the model\nmodel = RobertaForSequenceClassification.from_pretrained('sentiment_model_RoBERTa')\nmodel = model.to('cuda')  # if GPU is available\n\n# Load validation data\nval_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\nval_texts = val_data['review'].tolist()\nval_labels = val_data['sentiment'].map({'positive': 1, 'negative': 0}).tolist()  # convert sentiment to numeric\n\n# Initialize tokenizer\ntokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n\n# Tokenize data\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset for validation\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\nval_dataset = ReviewDataset(val_encodings, val_labels)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Evaluate the model\nmodel.eval()\npredictions = []\ntrue_labels = []\nfor batch in val_loader:\n    input_ids = batch['input_ids'].to('cuda')\n    attention_mask = batch['attention_mask'].to('cuda')\n    labels = batch['labels'].to('cuda')\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions.extend(predicted_labels)\n    true_labels.extend(labels.cpu().numpy())\n\n# Calculate metrics\naccuracy = accuracy_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\nconf_matrix = confusion_matrix(true_labels, predictions)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'F1-score: {f1}')\nprint(f'Confusion matrix:\\n {conf_matrix}')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T16:16:38.333238Z","iopub.execute_input":"2024-02-21T16:16:38.334245Z","iopub.status.idle":"2024-02-21T16:27:36.330666Z","shell.execute_reply.started":"2024-02-21T16:16:38.334204Z","shell.execute_reply":"2024-02-21T16:27:36.329477Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Accuracy: 0.95255\nF1-score: 0.9531149646756584\nConfusion matrix:\n [[9405  530]\n [ 419 9646]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n# Print classification report\nprint(classification_report(true_labels, predictions, target_names=['negative', 'positive']))","metadata":{"execution":{"iopub.status.busy":"2024-02-21T16:47:31.284438Z","iopub.execute_input":"2024-02-21T16:47:31.285405Z","iopub.status.idle":"2024-02-21T16:47:31.319383Z","shell.execute_reply.started":"2024-02-21T16:47:31.285349Z","shell.execute_reply":"2024-02-21T16:47:31.318405Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    negative       0.96      0.95      0.95      9935\n    positive       0.95      0.96      0.95     10065\n\n    accuracy                           0.95     20000\n   macro avg       0.95      0.95      0.95     20000\nweighted avg       0.95      0.95      0.95     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification#, AdamW\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom torch.optim import AdamW\n\n\nimport time\n\n# Record start time\nstart_time = time.time()\n\n# Load data\ndata = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ndata['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\nreviews = data['review'].tolist()\nlabels = data['sentiment'].tolist()  # assuming sentiment is encoded as 0 (negative) and 1 (positive)\n\n# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(reviews, labels, test_size=0.2)\n\n# Initialize tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n    \n# Create torch dataset\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create dataloaders\ntrain_dataset = ReviewDataset(train_encodings, train_labels)\nval_dataset = ReviewDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\nmodel = model.to('cuda')  # if GPU is available\n\n# Initialize optimizer\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n# Training loop\nfor epoch in range(3):  # number of epochs\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Save the model\nmodel.save_pretrained('sentiment_model_DistilBERT')\n\n# Record end time\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T16:50:10.870607Z","iopub.execute_input":"2024-02-21T16:50:10.871367Z","iopub.status.idle":"2024-02-21T17:51:23.183277Z","shell.execute_reply.started":"2024-02-21T16:50:10.871329Z","shell.execute_reply":"2024-02-21T17:51:23.182166Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Time required to fine-tune:  3672.2959537506104\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport torch\n\n# Load the model\nmodel = DistilBertForSequenceClassification.from_pretrained('sentiment_model_DistilBERT')\nmodel = model.to('cuda')  # if GPU is available\n\n# Load validation data\nval_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\nval_texts = val_data['review'].tolist()\nval_labels = val_data['sentiment'].map({'positive': 1, 'negative': 0}).tolist()  # convert sentiment to numeric\n\n# Initialize tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Tokenize data\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset for validation\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\nval_dataset = ReviewDataset(val_encodings, val_labels)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Evaluate the model\nmodel.eval()\npredictions = []\ntrue_labels = []\nfor batch in val_loader:\n    input_ids = batch['input_ids'].to('cuda')\n    attention_mask = batch['attention_mask'].to('cuda')\n    labels = batch['labels'].to('cuda')\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions.extend(predicted_labels)\n    true_labels.extend(labels.cpu().numpy())\n\n# Calculate metrics\naccuracy = accuracy_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\nconf_matrix = confusion_matrix(true_labels, predictions)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'F1-score: {f1}')\nprint(f'Confusion matrix:\\n {conf_matrix}')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T17:55:37.744482Z","iopub.execute_input":"2024-02-21T17:55:37.745128Z","iopub.status.idle":"2024-02-21T18:01:36.202203Z","shell.execute_reply.started":"2024-02-21T17:55:37.745095Z","shell.execute_reply":"2024-02-21T18:01:36.200990Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Accuracy: 0.93105\nF1-score: 0.9313349599163471\nConfusion matrix:\n [[9269  666]\n [ 713 9352]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n# Print classification report\nprint(classification_report(true_labels, predictions, target_names=['negative', 'positive']))","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:10:44.749749Z","iopub.execute_input":"2024-02-21T18:10:44.750222Z","iopub.status.idle":"2024-02-21T18:10:44.784867Z","shell.execute_reply.started":"2024-02-21T18:10:44.750179Z","shell.execute_reply":"2024-02-21T18:10:44.783848Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    negative       0.93      0.93      0.93      9935\n    positive       0.93      0.93      0.93     10065\n\n    accuracy                           0.93     20000\n   macro avg       0.93      0.93      0.93     20000\nweighted avg       0.93      0.93      0.93     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"glove","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 100  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.100d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T17:28:00.410030Z","iopub.execute_input":"2024-02-22T17:28:00.410401Z","iopub.status.idle":"2024-02-22T17:44:19.483530Z","shell.execute_reply.started":"2024-02-22T17:28:00.410373Z","shell.execute_reply":"2024-02-22T17:44:19.482540Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1708622911.969708      96 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"750/750 [==============================] - 179s 233ms/step - loss: 0.4654 - accuracy: 0.7723 - val_loss: 0.3492 - val_accuracy: 0.8497\nEpoch 2/5\n750/750 [==============================] - 174s 232ms/step - loss: 0.3226 - accuracy: 0.8614 - val_loss: 0.3088 - val_accuracy: 0.8693\nEpoch 3/5\n750/750 [==============================] - 174s 232ms/step - loss: 0.2644 - accuracy: 0.8935 - val_loss: 0.2949 - val_accuracy: 0.8740\nEpoch 4/5\n750/750 [==============================] - 176s 234ms/step - loss: 0.2195 - accuracy: 0.9137 - val_loss: 0.3627 - val_accuracy: 0.8417\nEpoch 5/5\n750/750 [==============================] - 176s 234ms/step - loss: 0.1825 - accuracy: 0.9315 - val_loss: 0.2910 - val_accuracy: 0.8797\n625/625 [==============================] - 27s 44ms/step - loss: 0.2954 - accuracy: 0.8752\nTest accuracy: 0.8751999735832214\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 200  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.200d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T17:48:10.135945Z","iopub.execute_input":"2024-02-22T17:48:10.136330Z","iopub.status.idle":"2024-02-22T18:04:20.952916Z","shell.execute_reply.started":"2024-02-22T17:48:10.136297Z","shell.execute_reply":"2024-02-22T18:04:20.951868Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 183s 241ms/step - loss: 0.4169 - accuracy: 0.8020 - val_loss: 0.3150 - val_accuracy: 0.8663\nEpoch 2/5\n750/750 [==============================] - 180s 239ms/step - loss: 0.2768 - accuracy: 0.8855 - val_loss: 0.2903 - val_accuracy: 0.8813\nEpoch 3/5\n750/750 [==============================] - 183s 244ms/step - loss: 0.2171 - accuracy: 0.9140 - val_loss: 0.2871 - val_accuracy: 0.8822\nEpoch 4/5\n750/750 [==============================] - 179s 239ms/step - loss: 0.1607 - accuracy: 0.9428 - val_loss: 0.2775 - val_accuracy: 0.8890\nEpoch 5/5\n750/750 [==============================] - 179s 239ms/step - loss: 0.1165 - accuracy: 0.9625 - val_loss: 0.2981 - val_accuracy: 0.8847\n625/625 [==============================] - 28s 44ms/step - loss: 0.2972 - accuracy: 0.8842\nTest accuracy: 0.8841500282287598\nTime required to fine-tune:  970.7984492778778\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:06:55.141536Z","iopub.execute_input":"2024-02-22T18:06:55.141919Z","iopub.status.idle":"2024-02-22T18:23:38.023283Z","shell.execute_reply.started":"2024-02-22T18:06:55.141886Z","shell.execute_reply":"2024-02-22T18:23:38.022328Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 185s 243ms/step - loss: 0.3877 - accuracy: 0.8185 - val_loss: 0.3397 - val_accuracy: 0.8553\nEpoch 2/5\n750/750 [==============================] - 182s 243ms/step - loss: 0.2515 - accuracy: 0.8976 - val_loss: 0.2833 - val_accuracy: 0.8800\nEpoch 3/5\n750/750 [==============================] - 182s 242ms/step - loss: 0.1832 - accuracy: 0.9306 - val_loss: 0.2603 - val_accuracy: 0.8935\nEpoch 4/5\n750/750 [==============================] - 181s 242ms/step - loss: 0.1227 - accuracy: 0.9598 - val_loss: 0.2691 - val_accuracy: 0.8897\nEpoch 5/5\n750/750 [==============================] - 181s 241ms/step - loss: 0.0716 - accuracy: 0.9820 - val_loss: 0.2883 - val_accuracy: 0.8905\n625/625 [==============================] - 27s 44ms/step - loss: 0.2957 - accuracy: 0.8881\nTest accuracy: 0.8880500197410583\nTime required to fine-tune:  1002.8629622459412\n","output_type":"stream"}]},{"cell_type":"markdown","source":"10 epochs","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 10\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:26:00.293901Z","iopub.execute_input":"2024-02-22T18:26:00.294758Z","iopub.status.idle":"2024-02-22T18:57:25.908322Z","shell.execute_reply.started":"2024-02-22T18:26:00.294726Z","shell.execute_reply":"2024-02-22T18:57:25.907403Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10\n750/750 [==============================] - 184s 242ms/step - loss: 0.3811 - accuracy: 0.8263 - val_loss: 0.2921 - val_accuracy: 0.8765\nEpoch 2/10\n750/750 [==============================] - 181s 241ms/step - loss: 0.2476 - accuracy: 0.8982 - val_loss: 0.2723 - val_accuracy: 0.8875\nEpoch 3/10\n750/750 [==============================] - 181s 241ms/step - loss: 0.1776 - accuracy: 0.9330 - val_loss: 0.2701 - val_accuracy: 0.8857\nEpoch 4/10\n750/750 [==============================] - 181s 241ms/step - loss: 0.1194 - accuracy: 0.9615 - val_loss: 0.2787 - val_accuracy: 0.8865\nEpoch 5/10\n750/750 [==============================] - 181s 241ms/step - loss: 0.0684 - accuracy: 0.9845 - val_loss: 0.2993 - val_accuracy: 0.8888\nEpoch 6/10\n750/750 [==============================] - 180s 240ms/step - loss: 0.0369 - accuracy: 0.9957 - val_loss: 0.3236 - val_accuracy: 0.8868\nEpoch 7/10\n750/750 [==============================] - 181s 241ms/step - loss: 0.0194 - accuracy: 0.9986 - val_loss: 0.3459 - val_accuracy: 0.8870\nEpoch 8/10\n750/750 [==============================] - 181s 242ms/step - loss: 0.0108 - accuracy: 0.9996 - val_loss: 0.3839 - val_accuracy: 0.8858\nEpoch 9/10\n750/750 [==============================] - 180s 240ms/step - loss: 0.0065 - accuracy: 0.9999 - val_loss: 0.3927 - val_accuracy: 0.8893\nEpoch 10/10\n750/750 [==============================] - 181s 241ms/step - loss: 0.0070 - accuracy: 0.9996 - val_loss: 0.4282 - val_accuracy: 0.8850\n625/625 [==============================] - 27s 43ms/step - loss: 0.4351 - accuracy: 0.8854\nTest accuracy: 0.8853999972343445\nTime required to fine-tune:  1885.5962204933167\n","output_type":"stream"}]},{"cell_type":"markdown","source":"20 epochs","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\nimport tensorflow as tf\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 20  # Increase the number of epochs\nearly_stopping_patience = 3  # Early stopping patience\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model with early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True\n)\n\n# Train the model with modified epochs and callbacks\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n          validation_split=0.2, callbacks=[early_stopping])\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T19:01:31.528819Z","iopub.execute_input":"2024-02-22T19:01:31.529182Z","iopub.status.idle":"2024-02-22T19:17:57.114827Z","shell.execute_reply.started":"2024-02-22T19:01:31.529155Z","shell.execute_reply":"2024-02-22T19:17:57.113990Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/20\n750/750 [==============================] - 184s 241ms/step - loss: 0.3756 - accuracy: 0.8256 - val_loss: 0.3077 - val_accuracy: 0.8633\nEpoch 2/20\n750/750 [==============================] - 182s 242ms/step - loss: 0.2502 - accuracy: 0.8969 - val_loss: 0.2704 - val_accuracy: 0.8865\nEpoch 3/20\n750/750 [==============================] - 183s 243ms/step - loss: 0.1788 - accuracy: 0.9312 - val_loss: 0.2936 - val_accuracy: 0.8750\nEpoch 4/20\n750/750 [==============================] - 181s 241ms/step - loss: 0.1165 - accuracy: 0.9632 - val_loss: 0.2932 - val_accuracy: 0.8865\nEpoch 5/20\n750/750 [==============================] - 181s 241ms/step - loss: 0.0676 - accuracy: 0.9847 - val_loss: 0.3050 - val_accuracy: 0.8860\n625/625 [==============================] - 27s 44ms/step - loss: 0.2685 - accuracy: 0.8874\nTest accuracy: 0.8873999714851379\nTime required to fine-tune:  985.5678837299347\n","output_type":"stream"}]},{"cell_type":"markdown","source":"l2 regularizatiom","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import regularizers\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5  # Increase the number of epochs\nearly_stopping_patience = 3  # Early stopping patience\ndropout_rate = 0.2  # Dropout rate for regularization\n\n# Load train and test datasets from CSV files\ntrain_data = pd.read_csv('/kaggle/input/plmdataset/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmdataset/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch with dropout\nlstm_branch = LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate,\n                   kernel_regularizer=regularizers.l2(0.001))(embedding_layer)\n\n# CNN branch with dropout\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\ncnn_branch = Dropout(rate=dropout_rate)(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model with early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True\n)\n\n# Train the model with modified epochs and callbacks\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n          validation_split=0.2, callbacks=[early_stopping])\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T19:20:25.209121Z","iopub.execute_input":"2024-02-22T19:20:25.209527Z","iopub.status.idle":"2024-02-22T19:37:05.275145Z","shell.execute_reply.started":"2024-02-22T19:20:25.209495Z","shell.execute_reply":"2024-02-22T19:37:05.274283Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 185s 243ms/step - loss: 0.5175 - accuracy: 0.7972 - val_loss: 0.4044 - val_accuracy: 0.8317\nEpoch 2/5\n750/750 [==============================] - 182s 243ms/step - loss: 0.3347 - accuracy: 0.8702 - val_loss: 0.3362 - val_accuracy: 0.8693\nEpoch 3/5\n750/750 [==============================] - 182s 242ms/step - loss: 0.2695 - accuracy: 0.8957 - val_loss: 0.3168 - val_accuracy: 0.8723\nEpoch 4/5\n750/750 [==============================] - 183s 244ms/step - loss: 0.2221 - accuracy: 0.9148 - val_loss: 0.3154 - val_accuracy: 0.8743\nEpoch 5/5\n750/750 [==============================] - 182s 242ms/step - loss: 0.1788 - accuracy: 0.9343 - val_loss: 0.3246 - val_accuracy: 0.8735\n625/625 [==============================] - 27s 44ms/step - loss: 0.3184 - accuracy: 0.8726\nTest accuracy: 0.8726000189781189\nTime required to fine-tune:  1000.0464890003204\n","output_type":"stream"}]}]}