{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7671775,"sourceType":"datasetVersion","datasetId":4474781},{"sourceId":7686077,"sourceType":"datasetVersion","datasetId":4485032},{"sourceId":7687223,"sourceType":"datasetVersion","datasetId":4485868}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-23T14:36:22.344206Z","iopub.execute_input":"2024-02-23T14:36:22.344668Z","iopub.status.idle":"2024-02-23T14:36:23.675097Z","shell.execute_reply.started":"2024-02-23T14:36:22.344633Z","shell.execute_reply":"2024-02-23T14:36:23.673064Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"chagig the ymer of epochs from 3 to 5","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\nfrom tensorflow.keras.initializers import Constant\n\n# Function to load GloVe embeddings\ndef load_glove_embeddings(embedding_dim):\n    embeddings_index = {}\n    embedding_file = f\"/kaggle/input/glove6b/glove.6B.{embedding_dim}d.txt\"  # Assuming files are named as glove.6B.{dim}d.txt\n    with open(embedding_file, encoding=\"utf-8\") as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    return embeddings_index\n\n# Load IMDb train and test datasets\ntrain_data = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train = np.where(y_train == 'negative', 0, 1)\ny_test = np.where(y_test == 'negative', 0, 1)\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nbatch_size = 32\nepochs = 5\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load GloVe embeddings of different dimensions\nembedding_dims = [50, 100, 200, 300]\nglove_embeddings = {}\n\nfor dim in embedding_dims:\n    glove_embeddings[dim] = load_glove_embeddings(dim)\n\n# List to store individual model predictions\nmodel_predictions = []\n\n# Function to create model with GloVe embeddings\ndef create_model(embedding_dim):\n    embedding_matrix = np.zeros((max_features, embedding_dim))\n    for word, i in tokenizer.word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = glove_embeddings[embedding_dim].get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n    inputs = Input(shape=(maxlen,))\n    embedding_layer = Embedding(max_features, embedding_dim,\n                                embeddings_initializer=Constant(embedding_matrix),\n                                input_length=maxlen,\n                                trainable=False)(inputs)\n\n    lstm_branch = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n    cnn_branch = Conv1D(64, 5, activation='relu')(embedding_layer)\n    cnn_branch = GlobalMaxPooling1D()(cnn_branch)\n    merged = Concatenate()([lstm_branch, cnn_branch])\n    merged = Dropout(0.5)(merged)\n    output = Dense(1, activation='sigmoid')(merged)\n\n    model = Model(inputs=inputs, outputs=output)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# Train models and collect predictions\nfor dim in embedding_dims:\n    print(f\"Training model with {dim}d GloVe embeddings\")\n    model = create_model(dim)\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1)\n    predictions = model.predict(X_test)\n    model_predictions.append(predictions)\n\n# Ensemble predictions by averaging\nensemble_predictions = np.mean(model_predictions, axis=0)\n\n# Calculate ensemble accuracy\nensemble_accuracy = np.mean((ensemble_predictions > 0.5) == y_test)\nprint(f\"Ensemble accuracy: {ensemble_accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T14:43:16.885183Z","iopub.execute_input":"2024-02-23T14:43:16.885584Z","iopub.status.idle":"2024-02-23T15:58:39.555600Z","shell.execute_reply.started":"2024-02-23T14:43:16.885552Z","shell.execute_reply":"2024-02-23T15:58:39.554531Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Training model with 50d GloVe embeddings\nEpoch 1/5\n750/750 [==============================] - 170s 223ms/step - loss: 0.5773 - accuracy: 0.6907 - val_loss: 0.4500 - val_accuracy: 0.7845\nEpoch 2/5\n750/750 [==============================] - 166s 222ms/step - loss: 0.4601 - accuracy: 0.7815 - val_loss: 0.4091 - val_accuracy: 0.8142\nEpoch 3/5\n750/750 [==============================] - 167s 222ms/step - loss: 0.4137 - accuracy: 0.8103 - val_loss: 0.3812 - val_accuracy: 0.8355\nEpoch 4/5\n750/750 [==============================] - 166s 222ms/step - loss: 0.3751 - accuracy: 0.8332 - val_loss: 0.3571 - val_accuracy: 0.8433\nEpoch 5/5\n750/750 [==============================] - 167s 222ms/step - loss: 0.3513 - accuracy: 0.8463 - val_loss: 0.3443 - val_accuracy: 0.8493\n625/625 [==============================] - 27s 42ms/step\nTraining model with 100d GloVe embeddings\nEpoch 1/5\n750/750 [==============================] - 185s 243ms/step - loss: 0.5378 - accuracy: 0.7239 - val_loss: 0.3867 - val_accuracy: 0.8357\nEpoch 2/5\n750/750 [==============================] - 182s 242ms/step - loss: 0.3905 - accuracy: 0.8235 - val_loss: 0.3919 - val_accuracy: 0.8230\nEpoch 3/5\n750/750 [==============================] - 182s 243ms/step - loss: 0.3489 - accuracy: 0.8464 - val_loss: 0.3149 - val_accuracy: 0.8683\nEpoch 4/5\n750/750 [==============================] - 182s 242ms/step - loss: 0.3179 - accuracy: 0.8650 - val_loss: 0.3152 - val_accuracy: 0.8705\nEpoch 5/5\n750/750 [==============================] - 181s 242ms/step - loss: 0.2908 - accuracy: 0.8768 - val_loss: 0.2943 - val_accuracy: 0.8790\n625/625 [==============================] - 29s 46ms/step\nTraining model with 200d GloVe embeddings\nEpoch 1/5\n750/750 [==============================] - 227s 299ms/step - loss: 0.4937 - accuracy: 0.7556 - val_loss: 0.3814 - val_accuracy: 0.8347\nEpoch 2/5\n750/750 [==============================] - 226s 301ms/step - loss: 0.3722 - accuracy: 0.8360 - val_loss: 0.3309 - val_accuracy: 0.8635\nEpoch 3/5\n750/750 [==============================] - 226s 302ms/step - loss: 0.3231 - accuracy: 0.8605 - val_loss: 0.3291 - val_accuracy: 0.8560\nEpoch 4/5\n750/750 [==============================] - 225s 301ms/step - loss: 0.2835 - accuracy: 0.8823 - val_loss: 0.3012 - val_accuracy: 0.8720\nEpoch 5/5\n750/750 [==============================] - 226s 301ms/step - loss: 0.2550 - accuracy: 0.8950 - val_loss: 0.2968 - val_accuracy: 0.8768\n625/625 [==============================] - 38s 60ms/step\nTraining model with 300d GloVe embeddings\nEpoch 1/5\n750/750 [==============================] - 256s 338ms/step - loss: 0.4498 - accuracy: 0.7843 - val_loss: 0.3242 - val_accuracy: 0.8630\nEpoch 2/5\n750/750 [==============================] - 253s 337ms/step - loss: 0.3319 - accuracy: 0.8565 - val_loss: 0.3006 - val_accuracy: 0.8708\nEpoch 3/5\n750/750 [==============================] - 252s 337ms/step - loss: 0.2843 - accuracy: 0.8825 - val_loss: 0.2851 - val_accuracy: 0.8788\nEpoch 4/5\n750/750 [==============================] - 252s 336ms/step - loss: 0.2433 - accuracy: 0.8990 - val_loss: 0.3120 - val_accuracy: 0.8717\nEpoch 5/5\n750/750 [==============================] - 252s 336ms/step - loss: 0.2129 - accuracy: 0.9162 - val_loss: 0.2753 - val_accuracy: 0.8915\n625/625 [==============================] - 44s 70ms/step\nEnsemble accuracy: 0.499917125\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport gensim \nimport logging\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nmodel_W2V = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin',binary=True, limit=100000)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T17:51:53.776031Z","iopub.execute_input":"2024-02-23T17:51:53.776520Z","iopub.status.idle":"2024-02-23T17:51:56.311159Z","shell.execute_reply.started":"2024-02-23T17:51:53.776490Z","shell.execute_reply":"2024-02-23T17:51:56.309744Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_W2V.vector_size","metadata":{"execution":{"iopub.status.busy":"2024-02-23T17:52:05.281300Z","iopub.execute_input":"2024-02-23T17:52:05.281701Z","iopub.status.idle":"2024-02-23T17:52:05.290021Z","shell.execute_reply.started":"2024-02-23T17:52:05.281674Z","shell.execute_reply":"2024-02-23T17:52:05.288270Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"300"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load IMDb train and test datasets\ntrain_data = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load Word2Vec embeddings\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n\n# Record end time\nend_time = time.time()\nprint(\"Time required to fine-tune:\", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T17:53:09.773676Z","iopub.execute_input":"2024-02-23T17:53:09.774015Z","iopub.status.idle":"2024-02-23T18:10:36.697470Z","shell.execute_reply.started":"2024-02-23T17:53:09.773989Z","shell.execute_reply":"2024-02-23T18:10:36.695673Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-02-23 17:53:11.925471: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-23 17:53:11.925659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-23 17:53:12.086586: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 187s 247ms/step - loss: 0.3793 - accuracy: 0.8282 - val_loss: 0.3028 - val_accuracy: 0.8757\nEpoch 2/5\n750/750 [==============================] - 183s 244ms/step - loss: 0.2624 - accuracy: 0.8917 - val_loss: 0.2874 - val_accuracy: 0.8827\nEpoch 3/5\n750/750 [==============================] - 184s 246ms/step - loss: 0.2004 - accuracy: 0.9241 - val_loss: 0.2600 - val_accuracy: 0.8970\nEpoch 4/5\n750/750 [==============================] - 184s 245ms/step - loss: 0.1470 - accuracy: 0.9506 - val_loss: 0.2584 - val_accuracy: 0.8968\nEpoch 5/5\n750/750 [==============================] - 183s 244ms/step - loss: 0.1005 - accuracy: 0.9722 - val_loss: 0.2737 - val_accuracy: 0.8958\n625/625 [==============================] - 32s 51ms/step - loss: 0.2810 - accuracy: 0.8896\nTest accuracy: 0.8895999789237976\nTime required to fine-tune: 1033.9831807613373\n","output_type":"stream"}]},{"cell_type":"markdown","source":"adding dropout to merged output ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load IMDb train and test datasets\ntrain_data = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load Word2Vec embeddings\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\nmerged = Dropout(0.5)(merged)  # Dropout layer for regularization\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n\n# Record end time\nend_time = time.time()\nprint(\"Time required to fine-tune:\", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T18:25:42.733609Z","iopub.execute_input":"2024-02-23T18:25:42.733974Z","iopub.status.idle":"2024-02-23T18:42:40.733518Z","shell.execute_reply.started":"2024-02-23T18:25:42.733947Z","shell.execute_reply":"2024-02-23T18:42:40.732301Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/5\n750/750 [==============================] - 193s 254ms/step - loss: 0.4272 - accuracy: 0.7999 - val_loss: 0.3195 - val_accuracy: 0.8647\nEpoch 2/5\n750/750 [==============================] - 189s 252ms/step - loss: 0.3324 - accuracy: 0.8567 - val_loss: 0.3146 - val_accuracy: 0.8707\nEpoch 3/5\n750/750 [==============================] - 187s 249ms/step - loss: 0.2883 - accuracy: 0.8793 - val_loss: 0.2803 - val_accuracy: 0.8852\nEpoch 4/5\n750/750 [==============================] - 186s 248ms/step - loss: 0.2568 - accuracy: 0.8934 - val_loss: 0.3061 - val_accuracy: 0.8720\nEpoch 5/5\n750/750 [==============================] - 187s 249ms/step - loss: 0.2285 - accuracy: 0.9068 - val_loss: 0.2695 - val_accuracy: 0.8873\n625/625 [==============================] - 33s 52ms/step - loss: 0.2660 - accuracy: 0.8881\nTest accuracy: 0.8881000280380249\nTime required to fine-tune: 1017.9823062419891\n","output_type":"stream"}]},{"cell_type":"markdown","source":"icreasimg the epochs to 10","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 10\n\n# Load IMDb train and test datasets\ntrain_data = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load Word2Vec embeddings\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n\n# Record end time\nend_time = time.time()\nprint(\"Time required to fine-tune:\", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T18:47:12.520878Z","iopub.execute_input":"2024-02-23T18:47:12.521798Z","iopub.status.idle":"2024-02-23T19:20:57.311100Z","shell.execute_reply.started":"2024-02-23T18:47:12.521738Z","shell.execute_reply":"2024-02-23T19:20:57.310369Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/10\n750/750 [==============================] - 192s 253ms/step - loss: 0.3904 - accuracy: 0.8227 - val_loss: 0.3287 - val_accuracy: 0.8582\nEpoch 2/10\n750/750 [==============================] - 190s 254ms/step - loss: 0.2673 - accuracy: 0.8880 - val_loss: 0.2803 - val_accuracy: 0.8843\nEpoch 3/10\n750/750 [==============================] - 189s 252ms/step - loss: 0.2069 - accuracy: 0.9198 - val_loss: 0.2781 - val_accuracy: 0.8878\nEpoch 4/10\n750/750 [==============================] - 188s 250ms/step - loss: 0.1532 - accuracy: 0.9469 - val_loss: 0.2635 - val_accuracy: 0.8962\nEpoch 5/10\n750/750 [==============================] - 187s 249ms/step - loss: 0.1071 - accuracy: 0.9674 - val_loss: 0.2753 - val_accuracy: 0.8920\nEpoch 6/10\n750/750 [==============================] - 188s 250ms/step - loss: 0.0709 - accuracy: 0.9846 - val_loss: 0.2793 - val_accuracy: 0.8992\nEpoch 7/10\n750/750 [==============================] - 188s 251ms/step - loss: 0.0429 - accuracy: 0.9951 - val_loss: 0.2943 - val_accuracy: 0.8980\nEpoch 8/10\n750/750 [==============================] - 189s 252ms/step - loss: 0.0256 - accuracy: 0.9984 - val_loss: 0.3196 - val_accuracy: 0.8977\nEpoch 9/10\n750/750 [==============================] - 188s 250ms/step - loss: 0.0155 - accuracy: 0.9996 - val_loss: 0.3314 - val_accuracy: 0.8982\nEpoch 10/10\n750/750 [==============================] - 189s 253ms/step - loss: 0.0092 - accuracy: 0.9999 - val_loss: 0.3569 - val_accuracy: 0.8975\n625/625 [==============================] - 33s 53ms/step - loss: 0.3658 - accuracy: 0.8899\nTest accuracy: 0.8899499773979187\nTime required to fine-tune: 2024.7742836475372\n","output_type":"stream"}]},{"cell_type":"markdown","source":"stemmig usimg porter stemmer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nnltk.download('punkt')\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load IMDb train and test datasets\ntrain_data = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Stemming function\nstemmer = PorterStemmer()\ndef stem_text(text):\n    return ' '.join([stemmer.stem(word) for word in word_tokenize(text.lower())])\n\n# Apply stemming to train and test data\nX_train = [stem_text(text) for text in X_train]\nX_test = [stem_text(text) for text in X_test]\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load Word2Vec embeddings\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n\n# Record end time\nend_time = time.time()\nprint(\"Time required to fine-tune:\", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T19:26:17.469230Z","iopub.execute_input":"2024-02-23T19:26:17.469694Z","iopub.status.idle":"2024-02-23T19:49:13.443578Z","shell.execute_reply.started":"2024-02-23T19:26:17.469664Z","shell.execute_reply":"2024-02-23T19:49:13.442148Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nEpoch 1/5\n750/750 [==============================] - 201s 265ms/step - loss: 0.4524 - accuracy: 0.7826 - val_loss: 0.3905 - val_accuracy: 0.8123\nEpoch 2/5\n750/750 [==============================] - 191s 254ms/step - loss: 0.3149 - accuracy: 0.8660 - val_loss: 0.3301 - val_accuracy: 0.8513\nEpoch 3/5\n750/750 [==============================] - 193s 257ms/step - loss: 0.2432 - accuracy: 0.9041 - val_loss: 0.3113 - val_accuracy: 0.8597\nEpoch 4/5\n750/750 [==============================] - 200s 266ms/step - loss: 0.1839 - accuracy: 0.9350 - val_loss: 0.3109 - val_accuracy: 0.8652\nEpoch 5/5\n750/750 [==============================] - 190s 254ms/step - loss: 0.1325 - accuracy: 0.9599 - val_loss: 0.3415 - val_accuracy: 0.8590\n625/625 [==============================] - 33s 53ms/step - loss: 0.3468 - accuracy: 0.8576\nTest accuracy: 0.8576499819755554\nTime required to fine-tune: 1375.0074853897095\n","output_type":"stream"}]},{"cell_type":"markdown","source":"usig Lancaster stemmer ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\nfrom nltk.stem import LancasterStemmer\nimport nltk\nnltk.download('punkt')\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load IMDb train and test datasets\ntrain_data = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Initialize Lancaster Stemmer\nstemmer = LancasterStemmer()\n\n# Apply stemming to train and test data\nX_train_stemmed = [' '.join([stemmer.stem(word) for word in text.split()]) for text in X_train]\nX_test_stemmed = [' '.join([stemmer.stem(word) for word in text.split()]) for text in X_test]\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train_stemmed)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train_stemmed)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test_stemmed)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load Word2Vec embeddings\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n\n# Record end time\nend_time = time.time()\nprint(\"Time required to fine-tune:\", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T07:47:20.578838Z","iopub.execute_input":"2024-02-24T07:47:20.579347Z","iopub.status.idle":"2024-02-24T08:16:38.190100Z","shell.execute_reply.started":"2024-02-24T07:47:20.579313Z","shell.execute_reply":"2024-02-24T08:16:38.189143Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-24 07:47:35.117325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 07:47:35.117469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 07:47:35.291569: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nEpoch 1/5\n750/750 [==============================] - 260s 343ms/step - loss: 0.4463 - accuracy: 0.7897 - val_loss: 0.3717 - val_accuracy: 0.8320\nEpoch 2/5\n750/750 [==============================] - 258s 343ms/step - loss: 0.3102 - accuracy: 0.8677 - val_loss: 0.3265 - val_accuracy: 0.8570\nEpoch 3/5\n750/750 [==============================] - 257s 343ms/step - loss: 0.2358 - accuracy: 0.9081 - val_loss: 0.3074 - val_accuracy: 0.8690\nEpoch 4/5\n750/750 [==============================] - 260s 346ms/step - loss: 0.1791 - accuracy: 0.9364 - val_loss: 0.3025 - val_accuracy: 0.8743\nEpoch 5/5\n750/750 [==============================] - 262s 349ms/step - loss: 0.1274 - accuracy: 0.9612 - val_loss: 0.3239 - val_accuracy: 0.8687\n625/625 [==============================] - 47s 75ms/step - loss: 0.3248 - accuracy: 0.8696\nTest accuracy: 0.8695999979972839\nTime required to fine-tune: 1729.7143523693085\n","output_type":"stream"}]},{"cell_type":"markdown","source":"using SnowballStemmer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\nfrom nltk.stem import SnowballStemmer\nimport nltk\nnltk.download('punkt')\n\n# Record start time\nstart_time = time.time()\n\n# Parameters for the model and training\nmax_features = 20000  # Number of words to consider as features\nmaxlen = 200  # Max length of individual reviews\nembedding_dim = 300  # Dimension of word embeddings\nlstm_units = 128  # Number of LSTM units\nfilters = 64  # Number of filters for CNN\nkernel_size = 5  # Kernel size for CNN\nbatch_size = 32\nepochs = 5\n\n# Load IMDb train and test datasets\ntrain_data = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Preprocess train data\nX_train = train_data['review'].values\ny_train = train_data['sentiment'].values\n\n# Preprocess test data\nX_test = test_data['review'].values\ny_test = test_data['sentiment'].values\n\n# Convert labels to numerical format\ny_train[y_train == 'negative'] = 0\ny_train[y_train == 'positive'] = 1\ny_train = y_train.astype(int)\n\ny_test[y_test == 'negative'] = 0\ny_test[y_test == 'positive'] = 1\ny_test = y_test.astype(int)\n\n# Initialize Snowball Stemmer\nstemmer = SnowballStemmer(\"english\")\n\n# Apply stemming to train and test data\nX_train_stemmed = [' '.join([stemmer.stem(word) for word in text.split()]) for text in X_train]\nX_test_stemmed = [' '.join([stemmer.stem(word) for word in text.split()]) for text in X_test]\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train_stemmed)\n\nX_train_tokenized = tokenizer.texts_to_sequences(X_train_stemmed)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test_stemmed)\nX_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\nX_test = pad_sequences(X_test_tokenized, maxlen=maxlen)\n\n# Load Word2Vec embeddings\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Create embedding matrix\nnum_words = min(max_features, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i >= max_features:\n        continue\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\n# Define the model using functional API\ninputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)(inputs)\n\n# LSTM branch\nlstm_branch = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n\n# CNN branch\ncnn_branch = Conv1D(filters, kernel_size, activation='relu')(embedding_layer)\ncnn_branch = GlobalMaxPooling1D()(cnn_branch)\n\n# Concatenate both branches\nmerged = Concatenate()([lstm_branch, cnn_branch])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Create model\nmodel = Model(inputs=inputs, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy:', accuracy)\n\n# Record end time\nend_time = time.time()\nprint(\"Time required to fine-tune:\", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T08:18:30.519292Z","iopub.execute_input":"2024-02-24T08:18:30.519684Z","iopub.status.idle":"2024-02-24T08:47:04.943424Z","shell.execute_reply.started":"2024-02-24T08:18:30.519655Z","shell.execute_reply":"2024-02-24T08:47:04.941764Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nEpoch 1/5\n750/750 [==============================] - 266s 351ms/step - loss: 0.4267 - accuracy: 0.7985 - val_loss: 0.3460 - val_accuracy: 0.8493\nEpoch 2/5\n750/750 [==============================] - 257s 343ms/step - loss: 0.1673 - accuracy: 0.9397 - val_loss: 0.3104 - val_accuracy: 0.8695\nEpoch 5/5\n750/750 [==============================] - 258s 344ms/step - loss: 0.1167 - accuracy: 0.9657 - val_loss: 0.3037 - val_accuracy: 0.8790\n625/625 [==============================] - 45s 73ms/step - loss: 0.2969 - accuracy: 0.8781\nTest accuracy: 0.8781499862670898\nTime required to fine-tune: 1714.3936071395874\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Ensemble of hlove and word2vec","metadata":{}},{"cell_type":"markdown","source":"This code combines lexicon-based sentiment analysis using VADER, embedding-based sentiment analysis using Word2Vec, and PLM-based sentiment analysis using BERT. The final sentiment prediction is made through a voting mechanism, where each approach gets one vote, and the final sentiment label is determined by the majority vote.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense, concatenate\nfrom keras.models import Model\n\n# Load IMDb dataset\nimdb_train = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\nimdb_test = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Convert sentiment labels to integer type\nlabel_mapping = {'positive': 1, 'negative': 0}\nimdb_train['sentiment'] = imdb_train['sentiment'].map(label_mapping)\nimdb_test['sentiment'] = imdb_test['sentiment'].map(label_mapping)\n\n\n# Load GloVe embeddings\nglove_model = KeyedVectors.load_word2vec_format('/kaggle/input/glove6b/glove.6B.100d.txt', binary=False, no_header=True)\n\n\n# Lexicon-based approach using VADER\nvader = SentimentIntensityAnalyzer()\n\ndef vader_sentiment_analysis(text):\n    compound_score = vader.polarity_scores(text)['compound']\n    return 1 if compound_score >= 0 else 0\n\n# Embedding-based approach using GloVe\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(imdb_train['review'])\nX_train = tokenizer.texts_to_sequences(imdb_train['review'])\nX_test = tokenizer.texts_to_sequences(imdb_test['review'])\nX_train = pad_sequences(X_train, maxlen=100)\nX_test = pad_sequences(X_test, maxlen=100)\n\ndef glove_sentiment_analysis(text):\n    tokens = text.split()\n    embedding = np.zeros((100,))\n    for token in tokens:\n        if token in glove_model:\n            embedding += glove_model[token]\n    embedding /= len(tokens)\n    prediction = model.predict(np.array([embedding]))\n    return int(round(prediction[0][0]))\n\n# Combine both approaches in an ensemble model\ninput_layer = Input(shape=(100,))\nembedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\nlstm_layer = LSTM(128)(embedding_layer)\noutput_layer_glove = Dense(1, activation='sigmoid')(lstm_layer)\noutput_layer_vader = Dense(1, activation='sigmoid')(lstm_layer)\n\n# Concatenate the output layers\nconcatenated_output = concatenate([output_layer_glove, output_layer_vader])\nensemble_output = Dense(1, activation='sigmoid')(concatenated_output)\n\n# Compile the model\nmodel = Model(inputs=input_layer, outputs=ensemble_output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, imdb_train['sentiment'], batch_size=128, epochs=5, validation_split=0.2)\n\n# Evaluate the model\n_, accuracy = model.evaluate(X_test, imdb_test['sentiment'])\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n#5 mi","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:02:02.685708Z","iopub.execute_input":"2024-02-24T10:02:02.686243Z","iopub.status.idle":"2024-02-24T10:08:16.273480Z","shell.execute_reply.started":"2024-02-24T10:02:02.686201Z","shell.execute_reply":"2024-02-24T10:08:16.272133Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/5\n188/188 [==============================] - 47s 236ms/step - loss: 0.6955 - accuracy: 0.5067 - val_loss: 0.6923 - val_accuracy: 0.5022\nEpoch 2/5\n188/188 [==============================] - 44s 233ms/step - loss: 0.6352 - accuracy: 0.6535 - val_loss: 0.5569 - val_accuracy: 0.7772\nEpoch 3/5\n188/188 [==============================] - 44s 234ms/step - loss: 0.5011 - accuracy: 0.8375 - val_loss: 0.5121 - val_accuracy: 0.8185\nEpoch 4/5\n188/188 [==============================] - 44s 232ms/step - loss: 0.4294 - accuracy: 0.8794 - val_loss: 0.4765 - val_accuracy: 0.8325\nEpoch 5/5\n188/188 [==============================] - 44s 235ms/step - loss: 0.3782 - accuracy: 0.8998 - val_loss: 0.4447 - val_accuracy: 0.8370\n625/625 [==============================] - 27s 44ms/step - loss: 0.4419 - accuracy: 0.8395\nAccuracy: 83.95%\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\n\n# Load IMDb dataset\nimdb_train = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\nimdb_test = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Convert sentiment labels to integer type\nlabel_mapping = {'positive': 1, 'negative': 0}\nimdb_train['sentiment'] = imdb_train['sentiment'].map(label_mapping)\nimdb_test['sentiment'] = imdb_test['sentiment'].map(label_mapping)\n\n# Load GloVe embeddings\nglove_model = KeyedVectors.load_word2vec_format('/kaggle/input/glove6b/glove.6B.100d.txt', binary=False, no_header=True)\n\n# Lexicon-based approach using VADER\nvader = SentimentIntensityAnalyzer()\n\ndef vader_sentiment_analysis(text):\n    compound_score = vader.polarity_scores(text)['compound']\n    return 1 if compound_score >= 0 else 0\n\n# Embedding-based approach using GloVe\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(imdb_train['review'])\nX_train = tokenizer.texts_to_sequences(imdb_train['review'])\nX_test = tokenizer.texts_to_sequences(imdb_test['review'])\nX_train = pad_sequences(X_train, maxlen=100)\nX_test = pad_sequences(X_test, maxlen=100)\n\n# Combine both approaches in an ensemble model\ninput_layer = Input(shape=(100,))\nembedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\nlstm_layer = LSTM(256, dropout=0.2)(embedding_layer)\noutput_layer_glove = Dense(1, activation='relu')(lstm_layer)\noutput_layer_vader = Dense(1, activation='relu')(lstm_layer)\n\n# Concatenate the output layers\nconcatenated_output = concatenate([output_layer_glove, output_layer_vader])\nensemble_output = Dense(1, activation='relu')(concatenated_output)\n\n# Compile the model\noptimizer = RMSprop(learning_rate=0.001)\nmodel = Model(inputs=input_layer, outputs=ensemble_output)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, imdb_train['sentiment'], batch_size=128, epochs=5, validation_split=0.2)\n\n# Evaluate the model\n_, accuracy = model.evaluate(X_test, imdb_test['sentiment'])\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:21:17.619781Z","iopub.execute_input":"2024-02-24T10:21:17.620270Z","iopub.status.idle":"2024-02-24T10:33:10.507129Z","shell.execute_reply.started":"2024-02-24T10:21:17.620235Z","shell.execute_reply":"2024-02-24T10:33:10.505769Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/5\n188/188 [==============================] - 103s 535ms/step - loss: 7.5507 - accuracy: 0.4985 - val_loss: 7.6577 - val_accuracy: 0.4978\nEpoch 2/5\n188/188 [==============================] - 103s 549ms/step - loss: 7.6577 - accuracy: 0.4978 - val_loss: 7.6577 - val_accuracy: 0.4978\nEpoch 3/5\n188/188 [==============================] - 101s 535ms/step - loss: 7.6577 - accuracy: 0.4978 - val_loss: 7.6577 - val_accuracy: 0.4978\nEpoch 4/5\n188/188 [==============================] - 99s 526ms/step - loss: 7.6577 - accuracy: 0.4978 - val_loss: 7.6577 - val_accuracy: 0.4978\nEpoch 5/5\n188/188 [==============================] - 99s 528ms/step - loss: 7.6577 - accuracy: 0.4978 - val_loss: 7.6577 - val_accuracy: 0.4978\n625/625 [==============================] - 50s 80ms/step - loss: 7.5750 - accuracy: 0.5033\nAccuracy: 50.33%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"vader + glove100d","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, Bidirectional\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\n\n# Load IMDb dataset\nimdb_train = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\nimdb_test = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Convert sentiment labels to integer type\nlabel_mapping = {'positive': 1, 'negative': 0}\nimdb_train['sentiment'] = imdb_train['sentiment'].map(label_mapping)\nimdb_test['sentiment'] = imdb_test['sentiment'].map(label_mapping)\n\n# Load GloVe embeddings\nglove_model = KeyedVectors.load_word2vec_format('/kaggle/input/glove6b/glove.6B.100d.txt', binary=False, no_header=True)\n\n# Lexicon-based approach using VADER\nvader = SentimentIntensityAnalyzer()\n\ndef vader_sentiment_analysis(text):\n    compound_score = vader.polarity_scores(text)['compound']\n    return 1 if compound_score >= 0 else 0\n\n# Embedding-based approach using GloVe\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(imdb_train['review'])\nX_train = tokenizer.texts_to_sequences(imdb_train['review'])\nX_test = tokenizer.texts_to_sequences(imdb_test['review'])\nX_train = pad_sequences(X_train, maxlen=100)\nX_test = pad_sequences(X_test, maxlen=100)\n\n# Combine both approaches in an ensemble model\ninput_layer = Input(shape=(100,))\nembedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\nlstm_layer = Bidirectional(LSTM(128, dropout=0.2))(embedding_layer)\noutput_layer_glove = Dense(64, activation='relu')(lstm_layer)\noutput_layer_vader = Dense(64, activation='relu')(lstm_layer)\n\n# Concatenate the output layers\nconcatenated_output = concatenate([output_layer_glove, output_layer_vader])\nensemble_output = Dense(1, activation='sigmoid')(concatenated_output)\n\n# Compile the model\noptimizer = RMSprop(learning_rate=0.001)\nmodel = Model(inputs=input_layer, outputs=ensemble_output)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, imdb_train['sentiment'], batch_size=128, epochs=5, validation_split=0.2)\n\n# Evaluate the model\n_, accuracy = model.evaluate(X_test, imdb_test['sentiment'])\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:53:30.499597Z","iopub.execute_input":"2024-02-24T10:53:30.500115Z","iopub.status.idle":"2024-02-24T11:01:40.463921Z","shell.execute_reply.started":"2024-02-24T10:53:30.500078Z","shell.execute_reply":"2024-02-24T11:01:40.463030Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/5\n188/188 [==============================] - 72s 360ms/step - loss: 0.5750 - accuracy: 0.6756 - val_loss: 0.4333 - val_accuracy: 0.7975\nEpoch 2/5\n188/188 [==============================] - 66s 353ms/step - loss: 0.3555 - accuracy: 0.8497 - val_loss: 0.4162 - val_accuracy: 0.8127\nEpoch 3/5\n188/188 [==============================] - 66s 352ms/step - loss: 0.2940 - accuracy: 0.8798 - val_loss: 0.3496 - val_accuracy: 0.8513\nEpoch 4/5\n188/188 [==============================] - 66s 351ms/step - loss: 0.2606 - accuracy: 0.8960 - val_loss: 0.3808 - val_accuracy: 0.8420\nEpoch 5/5\n188/188 [==============================] - 66s 352ms/step - loss: 0.2338 - accuracy: 0.9108 - val_loss: 0.3473 - val_accuracy: 0.8555\n625/625 [==============================] - 35s 56ms/step - loss: 0.3349 - accuracy: 0.8558\nAccuracy: 85.58%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"vader + glove200d","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, Bidirectional\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\n\n# Load IMDb dataset\nimdb_train = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\nimdb_test = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Convert sentiment labels to integer type\nlabel_mapping = {'positive': 1, 'negative': 0}\nimdb_train['sentiment'] = imdb_train['sentiment'].map(label_mapping)\nimdb_test['sentiment'] = imdb_test['sentiment'].map(label_mapping)\n\n# Load GloVe embeddings\nglove_model = KeyedVectors.load_word2vec_format('/kaggle/input/glove6b/glove.6B.200d.txt', binary=False, no_header=True)\n\n# Lexicon-based approach using VADER\nvader = SentimentIntensityAnalyzer()\n\ndef vader_sentiment_analysis(text):\n    compound_score = vader.polarity_scores(text)['compound']\n    return 1 if compound_score >= 0 else 0\n\n# Embedding-based approach using GloVe\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(imdb_train['review'])\nX_train = tokenizer.texts_to_sequences(imdb_train['review'])\nX_test = tokenizer.texts_to_sequences(imdb_test['review'])\nX_train = pad_sequences(X_train, maxlen=100)\nX_test = pad_sequences(X_test, maxlen=100)\n\n# Combine both approaches in an ensemble model\ninput_layer = Input(shape=(100,))\nembedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\nlstm_layer = Bidirectional(LSTM(128, dropout=0.2))(embedding_layer)\noutput_layer_glove = Dense(64, activation='relu')(lstm_layer)\noutput_layer_vader = Dense(64, activation='relu')(lstm_layer)\n\n# Concatenate the output layers\nconcatenated_output = concatenate([output_layer_glove, output_layer_vader])\nensemble_output = Dense(1, activation='sigmoid')(concatenated_output)\n\n# Compile the model\noptimizer = RMSprop(learning_rate=0.001)\nmodel = Model(inputs=input_layer, outputs=ensemble_output)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, imdb_train['sentiment'], batch_size=128, epochs=5, validation_split=0.2)\n\n# Evaluate the model\n_, accuracy = model.evaluate(X_test, imdb_test['sentiment'])\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T16:49:06.971724Z","iopub.execute_input":"2024-02-25T16:49:06.972486Z","iopub.status.idle":"2024-02-25T16:57:33.278895Z","shell.execute_reply.started":"2024-02-25T16:49:06.972446Z","shell.execute_reply":"2024-02-25T16:57:33.277716Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n2024-02-25 16:49:22.928073: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 16:49:22.928219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 16:49:23.077807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n188/188 [==============================] - 67s 337ms/step - loss: 0.5512 - accuracy: 0.6990 - val_loss: 0.3853 - val_accuracy: 0.8308\nEpoch 2/5\n188/188 [==============================] - 61s 327ms/step - loss: 0.3508 - accuracy: 0.8508 - val_loss: 0.5723 - val_accuracy: 0.7973\nEpoch 3/5\n188/188 [==============================] - 62s 332ms/step - loss: 0.2947 - accuracy: 0.8784 - val_loss: 0.3388 - val_accuracy: 0.8552\nEpoch 4/5\n188/188 [==============================] - 61s 327ms/step - loss: 0.2569 - accuracy: 0.8999 - val_loss: 0.3649 - val_accuracy: 0.8533\nEpoch 5/5\n188/188 [==============================] - 63s 334ms/step - loss: 0.2304 - accuracy: 0.9115 - val_loss: 0.3473 - val_accuracy: 0.8520\n625/625 [==============================] - 33s 53ms/step - loss: 0.3356 - accuracy: 0.8541\nAccuracy: 85.41%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"vader + glove300d","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, Bidirectional\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\n\n# Load IMDb dataset\nimdb_train = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\nimdb_test = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Convert sentiment labels to integer type\nlabel_mapping = {'positive': 1, 'negative': 0}\nimdb_train['sentiment'] = imdb_train['sentiment'].map(label_mapping)\nimdb_test['sentiment'] = imdb_test['sentiment'].map(label_mapping)\n\n# Load GloVe embeddings\nglove_model = KeyedVectors.load_word2vec_format('/kaggle/input/glove6b/glove.6B.300d.txt', binary=False, no_header=True)\n\n# Lexicon-based approach using VADER\nvader = SentimentIntensityAnalyzer()\n\ndef vader_sentiment_analysis(text):\n    compound_score = vader.polarity_scores(text)['compound']\n    return 1 if compound_score >= 0 else 0\n\n# Embedding-based approach using GloVe\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(imdb_train['review'])\nX_train = tokenizer.texts_to_sequences(imdb_train['review'])\nX_test = tokenizer.texts_to_sequences(imdb_test['review'])\nX_train = pad_sequences(X_train, maxlen=100)\nX_test = pad_sequences(X_test, maxlen=100)\n\n# Combine both approaches in an ensemble model\ninput_layer = Input(shape=(100,))\nembedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\nlstm_layer = Bidirectional(LSTM(128, dropout=0.2))(embedding_layer)\noutput_layer_glove = Dense(64, activation='relu')(lstm_layer)\noutput_layer_vader = Dense(64, activation='relu')(lstm_layer)\n\n# Concatenate the output layers\nconcatenated_output = concatenate([output_layer_glove, output_layer_vader])\nensemble_output = Dense(1, activation='sigmoid')(concatenated_output)\n\n# Compile the model\noptimizer = RMSprop(learning_rate=0.001)\nmodel = Model(inputs=input_layer, outputs=ensemble_output)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, imdb_train['sentiment'], batch_size=128, epochs=5, validation_split=0.2)\n\n# Evaluate the model\n_, accuracy = model.evaluate(X_test, imdb_test['sentiment'])\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T16:57:52.784047Z","iopub.execute_input":"2024-02-25T16:57:52.784492Z","iopub.status.idle":"2024-02-25T17:06:23.706046Z","shell.execute_reply.started":"2024-02-25T16:57:52.784460Z","shell.execute_reply":"2024-02-25T17:06:23.705166Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Epoch 1/5\n188/188 [==============================] - 67s 338ms/step - loss: 0.5814 - accuracy: 0.6700 - val_loss: 0.3913 - val_accuracy: 0.8300\nEpoch 2/5\n188/188 [==============================] - 62s 332ms/step - loss: 0.3591 - accuracy: 0.8457 - val_loss: 0.3781 - val_accuracy: 0.8305\nEpoch 3/5\n188/188 [==============================] - 62s 332ms/step - loss: 0.2954 - accuracy: 0.8785 - val_loss: 0.4211 - val_accuracy: 0.8042\nEpoch 4/5\n188/188 [==============================] - 63s 333ms/step - loss: 0.2598 - accuracy: 0.8963 - val_loss: 0.3564 - val_accuracy: 0.8538\nEpoch 5/5\n188/188 [==============================] - 63s 333ms/step - loss: 0.2309 - accuracy: 0.9104 - val_loss: 0.3470 - val_accuracy: 0.8532\n625/625 [==============================] - 33s 52ms/step - loss: 0.3347 - accuracy: 0.8566\nAccuracy: 85.66%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"glove100d with afinn","metadata":{}},{"cell_type":"code","source":"!pip install afinn\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom afinn import Afinn  # Import Afinn\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense, concatenate, Bidirectional\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\n\n# Load IMDb dataset\nimdb_train = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\nimdb_test = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\n# Convert sentiment labels to integer type\nlabel_mapping = {'positive': 1, 'negative': 0}\nimdb_train['sentiment'] = imdb_train['sentiment'].map(label_mapping)\nimdb_test['sentiment'] = imdb_test['sentiment'].map(label_mapping)\n\n# Load GloVe embeddings\nglove_model = KeyedVectors.load_word2vec_format('/kaggle/input/glove6b/glove.6B.100d.txt', binary=False, no_header=True)\n\n# Lexicon-based approach using AFINN\nafinn = Afinn()\n\ndef afinn_sentiment_analysis(text):\n    sentiment_score = afinn.score(text)\n    return 1 if sentiment_score >= 0 else 0\n\n# Embedding-based approach using GloVe\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(imdb_train['review'])\nX_train = tokenizer.texts_to_sequences(imdb_train['review'])\nX_test = tokenizer.texts_to_sequences(imdb_test['review'])\nX_train = pad_sequences(X_train, maxlen=100)\nX_test = pad_sequences(X_test, maxlen=100)\n\n# Combine both approaches in an ensemble model\ninput_layer = Input(shape=(100,))\nembedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\nlstm_layer = Bidirectional(LSTM(128, dropout=0.2))(embedding_layer)\noutput_layer_glove = Dense(64, activation='relu')(lstm_layer)\noutput_layer_afinn = Dense(64, activation='relu')(lstm_layer)  # Afinn-based output layer\n\n# Concatenate the output layers\nconcatenated_output = concatenate([output_layer_glove, output_layer_afinn])  # Afinn output added here\nensemble_output = Dense(1, activation='sigmoid')(concatenated_output)\n\n# Compile the model\noptimizer = RMSprop(learning_rate=0.001)\nmodel = Model(inputs=input_layer, outputs=ensemble_output)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, imdb_train['sentiment'], batch_size=128, epochs=5, validation_split=0.2)\n\n# Evaluate the model\n_, accuracy = model.evaluate(X_test, imdb_test['sentiment'])\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:34:29.755777Z","iopub.execute_input":"2024-02-24T14:34:29.756176Z","iopub.status.idle":"2024-02-24T14:43:15.776530Z","shell.execute_reply.started":"2024-02-24T14:34:29.756147Z","shell.execute_reply":"2024-02-24T14:43:15.775585Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting afinn\n  Downloading afinn-0.1.tar.gz (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: afinn\n  Building wheel for afinn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=fa9e38e9688802de24c96c6e5cb7faf3e648c7b0a3ed3bcb7ee64d7876078845\n  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\nSuccessfully built afinn\nInstalling collected packages: afinn\nSuccessfully installed afinn-0.1\n","output_type":"stream"},{"name":"stderr","text":"2024-02-24 14:34:51.681062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 14:34:51.681208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 14:34:51.860704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n188/188 [==============================] - 72s 360ms/step - loss: 0.5705 - accuracy: 0.6835 - val_loss: 0.3759 - val_accuracy: 0.8360\nEpoch 2/5\n188/188 [==============================] - 68s 361ms/step - loss: 0.3568 - accuracy: 0.8473 - val_loss: 0.5320 - val_accuracy: 0.8112\nEpoch 3/5\n188/188 [==============================] - 67s 357ms/step - loss: 0.2948 - accuracy: 0.8795 - val_loss: 0.3460 - val_accuracy: 0.8498\nEpoch 4/5\n188/188 [==============================] - 67s 357ms/step - loss: 0.2547 - accuracy: 0.9004 - val_loss: 0.3391 - val_accuracy: 0.8550\nEpoch 5/5\n188/188 [==============================] - 67s 354ms/step - loss: 0.2305 - accuracy: 0.9090 - val_loss: 0.4364 - val_accuracy: 0.8397\n625/625 [==============================] - 34s 55ms/step - loss: 0.4242 - accuracy: 0.8413\nAccuracy: 84.13%\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom torch.optim import AdamW\nimport numpy as np\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.initializers import Constant\nfrom gensim.models import KeyedVectors\nimport time\n\n# Record start time\nstart_time = time.time()\n\n# Load data for RoBERTa model\ndata_roberta = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ndata_roberta['sentiment'] = data_roberta['sentiment'].map({'positive': 1, 'negative': 0})\nreviews_roberta = data_roberta['review'].tolist()\nlabels_roberta = data_roberta['sentiment'].tolist()\n\n# Split data for RoBERTa model into training and validation sets\ntrain_texts_roberta, val_texts_roberta, train_labels_roberta, val_labels_roberta = train_test_split(reviews_roberta, labels_roberta, test_size=0.2)\n\n# Initialize tokenizer for RoBERTa\ntokenizer_roberta = RobertaTokenizerFast.from_pretrained('roberta-base')\n\n# Tokenize data for RoBERTa\ntrain_encodings_roberta = tokenizer_roberta(train_texts_roberta, truncation=True, padding=True, max_length=512)\nval_encodings_roberta = tokenizer_roberta(val_texts_roberta, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset for RoBERTa\nclass ReviewDatasetRoBERTa(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create dataloaders for RoBERTa\ntrain_dataset_roberta = ReviewDatasetRoBERTa(train_encodings_roberta, train_labels_roberta)\nval_dataset_roberta = ReviewDatasetRoBERTa(val_encodings_roberta, val_labels_roberta)\n\ntrain_loader_roberta = DataLoader(train_dataset_roberta, batch_size=16, shuffle=True)\nval_loader_roberta = DataLoader(val_dataset_roberta, batch_size=16, shuffle=False)\n\n# Initialize RoBERTa model\nmodel_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\nmodel_roberta = model_roberta.to('cuda')\n\n# Initialize optimizer for RoBERTa\noptimizer_roberta = AdamW(model_roberta.parameters(), lr=1e-5)\n\n# Training loop for RoBERTa model\nfor epoch in range(3):  \n    model_roberta.train()\n    for batch in train_loader_roberta:\n        optimizer_roberta.zero_grad()\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        outputs = model_roberta(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_roberta.step()\n\n# Save the RoBERTa model\nmodel_roberta.save_pretrained('sentiment_model_RoBERTa')\n\n# Record end time for RoBERTa model training\nend_time_roberta = time.time()\n\nprint(\"Time required to fine-tune RoBERTa: \", end_time_roberta - start_time)\n\n# Record start time for Word2Vec model\nstart_time_word2vec = time.time()\n\n# Parameters for the Word2Vec model\nmax_features = 20000  \nmaxlen = 200  \nembedding_dim = 300  \nlstm_units = 128  \nfilters = 64  \nkernel_size = 5  \nbatch_size = 32\nepochs = 10\n\n# Load data for Word2Vec model\ntrain_data_word2vec = pd.read_csv('/kaggle/input/plmsentiment/train.csv/train.csv')\ntest_data_word2vec = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\n\nX_train_word2vec = train_data_word2vec['review'].values\ny_train_word2vec = train_data_word2vec['sentiment'].values\n\nX_test_word2vec = test_data_word2vec['review'].values\ny_test_word2vec = test_data_word2vec['sentiment'].values\n\ny_train_word2vec[y_train_word2vec == 'negative'] = 0\ny_train_word2vec[y_train_word2vec == 'positive'] = 1\ny_train_word2vec = y_train_word2vec.astype(int)\n\ny_test_word2vec[y_test_word2vec == 'negative'] = 0\ny_test_word2vec[y_test_word2vec == 'positive'] = 1\ny_test_word2vec = y_test_word2vec.astype(int)\n\ntokenizer_word2vec = Tokenizer(num_words=max_features)\ntokenizer_word2vec.fit_on_texts(X_train_word2vec)\n\nX_train_tokenized_word2vec = tokenizer_word2vec.texts_to_sequences(X_train_word2vec)\nX_test_tokenized_word2vec = tokenizer_word2vec.texts_to_sequences(X_test_word2vec)\nX_train_word2vec = pad_sequences(X_train_tokenized_word2vec, maxlen=maxlen)\nX_test_word2vec = pad_sequences(X_test_tokenized_word2vec, maxlen=maxlen)\n\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n\nnum_words = min(max_features, len(tokenizer_word2vec.word_index) + 1)\nembedding_matrix_word2vec = np.zeros((num_words, embedding_dim))\nfor word, i in tokenizer_word2vec.word_index.items():\n    if i >= max_features:\n        continue\n    if word in word2vec_model:\n        embedding_matrix_word2vec[i] = word2vec_model[word]\n\ninputs_word2vec = Input(shape=(maxlen,))\nembedding_layer_word2vec = Embedding(num_words, embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix_word2vec),\n                            input_length=maxlen,\n                            trainable=False)(inputs_word2vec)\n\nlstm_branch_word2vec = LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)(embedding_layer_word2vec)\ncnn_branch_word2vec = Conv1D(filters, kernel_size, activation='relu')(embedding_layer_word2vec)\ncnn_branch_word2vec = GlobalMaxPooling1D()(cnn_branch_word2vec)\n\nmerged_word2vec = Concatenate()([lstm_branch_word2vec, cnn_branch_word2vec])\noutput_word2vec = Dense(1, activation='sigmoid')(merged_word2vec)\n\nmodel_word2vec = Model(inputs=inputs_word2vec, outputs=output_word2vec)\n\nmodel_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_word2vec.fit(X_train_word2vec, y_train_word2vec, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\nloss_word2vec, accuracy_word2vec = model_word2vec.evaluate(X_test_word2vec, y_test_word2vec)\nprint('Test accuracy for Word2Vec model:', accuracy_word2vec)\n\n# Record end time for Word2Vec model\nend_time_word2vec = time.time()\nprint(\"Time required to fine-tune Word2Vec:\", end_time_word2vec - start_time_word2vec)\n\n# Load validation data for RoBERTa model\nval_data_roberta = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\nval_texts_roberta = val_data_roberta['review'].tolist()\nval_labels_roberta = val_data_roberta['sentiment'].map({'positive': 1, 'negative': 0}).tolist()\n\n# Tokenize data for RoBERTa model\nval_encodings_roberta = tokenizer_roberta(val_texts_roberta, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset for validation data for RoBERTa\nval_dataset_roberta = ReviewDatasetRoBERTa(val_encodings_roberta, val_labels_roberta)\nval_loader_roberta = DataLoader(val_dataset_roberta, batch_size=16, shuffle=False)\n\n# Predict on validation data using RoBERTa model\nmodel_roberta.eval()\npredictions_roberta = []\ntrue_labels_roberta = []\nfor batch in val_loader_roberta:\n    input_ids = batch['input_ids'].to('cuda')\n    attention_mask = batch['attention_mask'].to('cuda')\n    labels = batch['labels'].to('cuda')\n\n    with torch.no_grad():\n        outputs = model_roberta(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions_roberta.extend(predicted_labels)\n    true_labels_roberta.extend(labels.cpu().numpy())\n\n# Convert RoBERTa predictions to numpy array\npredictions_roberta = np.array(predictions_roberta)\n\n# Load test data for Word2Vec model\ntest_data_word2vec = pd.read_csv('/kaggle/input/plmsentiment/test.csv/test.csv')\ntest_texts_word2vec = test_data_word2vec['review'].tolist()\ntest_labels_word2vec = test_data_word2vec['sentiment'].map({'positive': 1, 'negative': 0}).tolist()\n\n# Tokenize test data for Word2Vec model\ntest_encodings_word2vec = tokenizer_word2vec(test_texts_word2vec, truncation=True, padding=True, max_length=maxlen)\n\n# Predict on test data using Word2Vec model\ntest_predictions_word2vec = model_word2vec.predict(test_encodings_word2vec)\ntest_predictions_word2vec = np.round(test_predictions_word2vec).flatten()\n\n# Create ensemble predictions\nensemble_predictions = np.column_stack((predictions_roberta, test_predictions_word2vec))\n\n# Train meta-classifier (Logistic Regression) on ensemble predictions\nmeta_classifier = LogisticRegression()\nmeta_classifier.fit(ensemble_predictions, test_labels_word2vec)\n\n# Predict on test data using ensemble model\nfinal_predictions = meta_classifier.predict(ensemble_predictions)\n\n# Calculate accuracy\nensemble_accuracy = accuracy_score(test_labels_word2vec, final_predictions)\nprint(f'Ensemble Model Accuracy: {ensemble_accuracy}')\n\n# Record end time\nend_time = time.time()\nprint(\"Total time required:\", end_time - start_time)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T05:08:05.808636Z","iopub.execute_input":"2024-02-25T05:08:05.808909Z","iopub.status.idle":"2024-02-25T07:46:29.131805Z","shell.execute_reply.started":"2024-02-25T05:08:05.808885Z","shell.execute_reply":"2024-02-25T07:46:29.130524Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-25 05:08:22.198392: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 05:08:22.198506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 05:08:22.492807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01aa14bea68a4483a3df4fecbc40a578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faa2ae7422e9410588dc30f6a9d05feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de89464bb06b40f1845ffc6ccc209417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e52e1c8b97e84a2ea1aede93fb7a4dd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c1996b433e8464090669b5e94ddf14d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"554f9fc7f17749369214cfd5175e4a6e"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Time required to fine-tune RoBERTa:  6750.19261097908\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1708844586.672850     123 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"750/750 [==============================] - 197s 256ms/step - loss: 0.3830 - accuracy: 0.8268 - val_loss: 0.3083 - val_accuracy: 0.8730\nEpoch 2/10\n750/750 [==============================] - 191s 255ms/step - loss: 0.2699 - accuracy: 0.8885 - val_loss: 0.2962 - val_accuracy: 0.8777\nEpoch 3/10\n750/750 [==============================] - 190s 254ms/step - loss: 0.2052 - accuracy: 0.9208 - val_loss: 0.3288 - val_accuracy: 0.8622\nEpoch 4/10\n750/750 [==============================] - 191s 255ms/step - loss: 0.1522 - accuracy: 0.9482 - val_loss: 0.2760 - val_accuracy: 0.8842\nEpoch 5/10\n750/750 [==============================] - 191s 255ms/step - loss: 0.1057 - accuracy: 0.9711 - val_loss: 0.2784 - val_accuracy: 0.8933\nEpoch 6/10\n750/750 [==============================] - 191s 254ms/step - loss: 0.0683 - accuracy: 0.9857 - val_loss: 0.2884 - val_accuracy: 0.8897\nEpoch 7/10\n750/750 [==============================] - 190s 254ms/step - loss: 0.0417 - accuracy: 0.9948 - val_loss: 0.3053 - val_accuracy: 0.8912\nEpoch 8/10\n750/750 [==============================] - 191s 255ms/step - loss: 0.0253 - accuracy: 0.9986 - val_loss: 0.3282 - val_accuracy: 0.8913\nEpoch 9/10\n750/750 [==============================] - 190s 254ms/step - loss: 0.0154 - accuracy: 0.9995 - val_loss: 0.3545 - val_accuracy: 0.8907\nEpoch 10/10\n750/750 [==============================] - 191s 255ms/step - loss: 0.0097 - accuracy: 0.9998 - val_loss: 0.3602 - val_accuracy: 0.8902\n625/625 [==============================] - 32s 51ms/step - loss: 0.3599 - accuracy: 0.8920\nTest accuracy for Word2Vec model: 0.8920000195503235\nTime required to fine-tune Word2Vec: 2040.541413784027\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 200\u001b[0m\n\u001b[1;32m    197\u001b[0m test_labels_word2vec \u001b[38;5;241m=\u001b[39m test_data_word2vec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Tokenize test data for Word2Vec model\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m test_encodings_word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_word2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_texts_word2vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Predict on test data using Word2Vec model\u001b[39;00m\n\u001b[1;32m    203\u001b[0m test_predictions_word2vec \u001b[38;5;241m=\u001b[39m model_word2vec\u001b[38;5;241m.\u001b[39mpredict(test_encodings_word2vec)\n","\u001b[0;31mTypeError\u001b[0m: 'Tokenizer' object is not callable"],"ename":"TypeError","evalue":"'Tokenizer' object is not callable","output_type":"error"}]}]}