{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7657000,"sourceType":"datasetVersion","datasetId":4464349},{"sourceId":7657010,"sourceType":"datasetVersion","datasetId":4464357}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Load and preprocess the text data\ntrain_df = pd.read_csv(\"/kaggle/input/embedding/train.csv/train.csv\")  # Replace \"/path/to/training/dataset.csv\" with the actual path to your training dataset file\ntest_df = pd.read_csv(\"/kaggle/input/embedding/test.csv/test.csv\")  # Replace \"/path/to/testing/dataset.csv\" with the actual path to your testing dataset file\n\n# Tokenize the text data\ntrain_texts = train_df['review'].apply(lambda x: x.split())\ntest_texts = test_df['review'].apply(lambda x: x.split())\n\n# Train Word2Vec model\nword2vec_model = Word2Vec(sentences=train_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n# Transform text data into numerical vectors using Word2Vec embeddings\ndef word_embeddings(text, model):\n    embeddings = []\n    for word in text:\n        if word in model.wv:\n            embeddings.append(model.wv[word])\n    if len(embeddings) == 0:\n        embeddings = [np.zeros(model.vector_size)]\n    return np.mean(embeddings, axis=0)\n\nX_train = np.array([word_embeddings(text, word2vec_model) for text in train_texts])\nX_test = np.array([word_embeddings(text, word2vec_model) for text in test_texts])\n\n# Encode labels\nlabel_map = {'positive': 1, 'negative': 0}\ntrain_labels = train_df['sentiment'].map(label_map)\ntest_labels = test_df['sentiment'].map(label_map)\n\n# Initialize classifiers\nclassifiers = {\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"Logistic Regression\": LogisticRegression(),\n    \"LightGBM\": lgb.LGBMClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),\n    \"k-NN\": KNeighborsClassifier(),\n    \"XGBoost\": xgb.XGBClassifier()\n}\n\n# Train and evaluate classifiers\nfor name, classifier in classifiers.items():\n    classifier.fit(X_train, train_labels)\n    predictions = classifier.predict(X_test)\n    accuracy = accuracy_score(test_labels, predictions)\n    report = classification_report(test_labels, predictions, target_names=label_map.keys())\n    print(f\"{name} Accuracy:\", accuracy)\n    print(f\"{name} Classification Report:\\n{report}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-19T15:00:19.487364Z","iopub.execute_input":"2024-02-19T15:00:19.487766Z","iopub.status.idle":"2024-02-19T15:06:16.619783Z","shell.execute_reply.started":"2024-02-19T15:00:19.487737Z","shell.execute_reply":"2024-02-19T15:06:16.618123Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Decision Tree Accuracy: 0.6451\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.65      0.63      0.64      9935\n    negative       0.64      0.66      0.65     10065\n\n    accuracy                           0.65     20000\n   macro avg       0.65      0.65      0.64     20000\nweighted avg       0.65      0.65      0.65     20000\n\nRandom Forest Accuracy: 0.75375\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.76      0.74      0.75      9935\n    negative       0.75      0.77      0.76     10065\n\n    accuracy                           0.75     20000\n   macro avg       0.75      0.75      0.75     20000\nweighted avg       0.75      0.75      0.75     20000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.80365\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.81      0.79      0.80      9935\n    negative       0.80      0.82      0.81     10065\n\n    accuracy                           0.80     20000\n   macro avg       0.80      0.80      0.80     20000\nweighted avg       0.80      0.80      0.80     20000\n\n[LightGBM] [Info] Number of positive: 14935, number of negative: 15065\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035625 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 30000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497833 -> initscore=-0.008667\n[LightGBM] [Info] Start training from score -0.008667\nLightGBM Accuracy: 0.78095\nLightGBM Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.79      0.77      0.78      9935\n    negative       0.77      0.80      0.79     10065\n\n    accuracy                           0.78     20000\n   macro avg       0.78      0.78      0.78     20000\nweighted avg       0.78      0.78      0.78     20000\n\nGradient Boosting Accuracy: 0.76545\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.77      0.75      0.76      9935\n    negative       0.76      0.78      0.77     10065\n\n    accuracy                           0.77     20000\n   macro avg       0.77      0.77      0.77     20000\nweighted avg       0.77      0.77      0.77     20000\n\nNeural Network Accuracy: 0.80475\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.82      0.78      0.80      9935\n    negative       0.79      0.83      0.81     10065\n\n    accuracy                           0.80     20000\n   macro avg       0.81      0.80      0.80     20000\nweighted avg       0.81      0.80      0.80     20000\n\nk-NN Accuracy: 0.7025\nk-NN Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.69      0.73      0.71      9935\n    negative       0.72      0.68      0.70     10065\n\n    accuracy                           0.70     20000\n   macro avg       0.70      0.70      0.70     20000\nweighted avg       0.70      0.70      0.70     20000\n\nXGBoost Accuracy: 0.77965\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.79      0.77      0.78      9935\n    negative       0.77      0.79      0.78     10065\n\n    accuracy                           0.78     20000\n   macro avg       0.78      0.78      0.78     20000\nweighted avg       0.78      0.78      0.78     20000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Add the necessary imports for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pandas as pd\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Load and preprocess the text data\ntrain_df = pd.read_csv(\"/kaggle/input/embedding/train.csv/train.csv\")  # Replace \"/path/to/training/dataset.csv\" with the actual path to your training dataset file\ntest_df = pd.read_csv(\"/kaggle/input/embedding/test.csv/test.csv\")  # Replace \"/path/to/testing/dataset.csv\" with the actual path to your testing dataset file\n\n# Tokenize the text data\ntrain_texts = train_df['review'].apply(lambda x: x.split())\ntest_texts = test_df['review'].apply(lambda x: x.split())\n\n# Train Word2Vec model\nword2vec_model = Word2Vec(sentences=train_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n# Transform text data into numerical vectors using Word2Vec embeddings\ndef word_embeddings(text, model):\n    embeddings = []\n    for word in text:\n        if word in model.wv:\n            embeddings.append(model.wv[word])\n    if len(embeddings) == 0:\n        embeddings = [np.zeros(model.vector_size)]\n    return np.mean(embeddings, axis=0)\n\nX_train = np.array([word_embeddings(text, word2vec_model) for text in train_texts])\nX_test = np.array([word_embeddings(text, word2vec_model) for text in test_texts])\n\n# Encode labels\nlabel_map = {'positive': 1, 'negative': 0}\ntrain_labels = train_df['sentiment'].map(label_map)\ntest_labels = test_df['sentiment'].map(label_map)\n# Define the classifiers with default hyperparameters\nclassifiers = {\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"LightGBM\": lgb.LGBMClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),\n    \"k-NN\": KNeighborsClassifier(),\n    \"XGBoost\": xgb.XGBClassifier()\n}\n\n# Define hyperparameter grids for hyperparameter tuning\nparam_grids = {\n    \"Decision Tree\": {\"max_depth\": [None, 10, 20, 30]},\n    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20, 30]},\n    \"Logistic Regression\": {\"C\": [0.1, 1, 10]},\n    \"LightGBM\": {\"num_leaves\": [31, 50, 100], \"max_depth\": [-1, 10, 20, 30]},\n    \"Gradient Boosting\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [3, 5, 7]},\n    \"Neural Network\": {\"hidden_layer_sizes\": [(50,), (100,), (200,)], \"alpha\": [0.0001, 0.001, 0.01]},\n    \"k-NN\": {\"n_neighbors\": [3, 5, 7, 9]},\n    \"XGBoost\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [3, 5, 7]}\n}\n\n# Perform hyperparameter tuning and train classifiers\nfor name, classifier in classifiers.items():\n    param_grid = param_grids.get(name, {})  # Get the corresponding hyperparameter grid\n    grid_search = GridSearchCV(classifier, param_grid, cv=3, scoring=\"accuracy\")\n    grid_search.fit(X_train, train_labels)\n    \n    # Train the classifier with the best hyperparameters\n    best_classifier = grid_search.best_estimator_\n    best_classifier.fit(X_train, train_labels)\n    \n    # Evaluate the classifier\n    predictions = best_classifier.predict(X_test)\n    accuracy = accuracy_score(test_labels, predictions)\n    report = classification_report(test_labels, predictions, target_names=label_map.keys())\n    print(f\"{name} Accuracy:\", accuracy)\n    print(f\"{name} Classification Report:\\n{report}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-19T15:30:29.654318Z","iopub.execute_input":"2024-02-19T15:30:29.654743Z","iopub.status.idle":"2024-02-19T19:46:40.755894Z","shell.execute_reply.started":"2024-02-19T15:30:29.654712Z","shell.execute_reply":"2024-02-19T19:46:40.754953Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Decision Tree Accuracy: 0.66285\nDecision Tree Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.66      0.67      0.66      9935\n    negative       0.67      0.66      0.66     10065\n\n    accuracy                           0.66     20000\n   macro avg       0.66      0.66      0.66     20000\nweighted avg       0.66      0.66      0.66     20000\n\nRandom Forest Accuracy: 0.76105\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.77      0.73      0.75      9935\n    negative       0.75      0.79      0.77     10065\n\n    accuracy                           0.76     20000\n   macro avg       0.76      0.76      0.76     20000\nweighted avg       0.76      0.76      0.76     20000\n\nLogistic Regression Accuracy: 0.80655\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.81      0.79      0.80      9935\n    negative       0.80      0.82      0.81     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048699 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044381 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044778 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045617 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044874 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044595 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044580 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044566 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044871 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044765 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044315 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045783 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044929 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044862 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044918 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045046 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045681 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045845 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044754 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044815 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044833 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045156 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044858 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045090 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044801 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044779 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044816 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045165 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044852 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9957, number of negative: 10043\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044757 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497850 -> initscore=-0.008600\n[LightGBM] [Info] Start training from score -0.008600\n[LightGBM] [Info] Number of positive: 9956, number of negative: 10044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045503 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497800 -> initscore=-0.008800\n[LightGBM] [Info] Start training from score -0.008800\n[LightGBM] [Info] Number of positive: 14935, number of negative: 15065\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 30000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497833 -> initscore=-0.008667\n[LightGBM] [Info] Start training from score -0.008667\n[LightGBM] [Info] Number of positive: 14935, number of negative: 15065\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067880 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 30000, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497833 -> initscore=-0.008667\n[LightGBM] [Info] Start training from score -0.008667\nLightGBM Accuracy: 0.7816\nLightGBM Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.79      0.76      0.77      9935\n    negative       0.77      0.81      0.79     10065\n\n    accuracy                           0.78     20000\n   macro avg       0.78      0.78      0.78     20000\nweighted avg       0.78      0.78      0.78     20000\n\nGradient Boosting Accuracy: 0.7912\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.80      0.77      0.79      9935\n    negative       0.78      0.81      0.80     10065\n\n    accuracy                           0.79     20000\n   macro avg       0.79      0.79      0.79     20000\nweighted avg       0.79      0.79      0.79     20000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Neural Network Accuracy: 0.80535\nNeural Network Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.79      0.83      0.81      9935\n    negative       0.83      0.78      0.80     10065\n\n    accuracy                           0.81     20000\n   macro avg       0.81      0.81      0.81     20000\nweighted avg       0.81      0.81      0.81     20000\n\nk-NN Accuracy: 0.7159\nk-NN Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.70      0.75      0.72      9935\n    negative       0.73      0.68      0.71     10065\n\n    accuracy                           0.72     20000\n   macro avg       0.72      0.72      0.72     20000\nweighted avg       0.72      0.72      0.72     20000\n\nXGBoost Accuracy: 0.7923\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.80      0.77      0.79      9935\n    negative       0.78      0.81      0.80     10065\n\n    accuracy                           0.79     20000\n   macro avg       0.79      0.79      0.79     20000\nweighted avg       0.79      0.79      0.79     20000\n\n","output_type":"stream"}]}]}